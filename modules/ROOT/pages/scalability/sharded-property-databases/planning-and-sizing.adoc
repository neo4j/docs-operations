:page-role: new-2025.10 enterprise-edition not-on-aura
:description: This page describes the planning and sizing of sharded property databases.
= Planning and sizing

== Planning the topology of a sharded property database

The sharded property database is deployed into an autonomous cluster with the graph shard being a regular Raft group.
This means that you should deploy the graph shard cluster with a topology consisting of at least three primary servers for high availability.
Additional primaries may be added to support a higher fault tolerance.
If high availability is not required, you can create a database with a single primary for minimum write latency and cost efficiency.

Secondary servers can be added to the graph shard to scale out read workloads.
Secondaries act like caches for the graph data and are fully capable of executing read-only queries and procedures.

Replicas contain the property data.
The property data is replicated from the primary servers via transaction log shipping.
Replicas periodically poll an upstream server for new transactions and have these shipped over.
They are not in a Raft group and do not have the same high availability features as the graph shards.
To achieve high availability of the replicas containing the property shards, it is recommended to have multiple replicas per property shard.
The fault tolerance for a property shard replica is calculated with the formula M = F + 1, where M is the number of replicas required to tolerate F faults.

image::scalability/property-shard-architecture.svg[title="Sample architecture with high availability graph shard, graph shard secondaries, and 4 property shards with 2 replicas for each property shard.", role="middle"]

== Planning the sizing of a sharded property database

Property sharding relies on the capabilities provided by autonomous clustering for managing and sizing the infrastructure.
More specifically:

* Some servers can be associated to the graph shard databases, they can also be separated between primary and secondary members of the cluster.

* Other servers can be associated to the property shard databases.
It is important to consider the number of servers available in conjunction with the number of shards and the number of replicas (i.e. copies of the same shard for availability and read scalability).

* Data in sharded property databases will be evenly distributed.
It is recommended to consider that each database does not exceed a size the is suitable for the available hardware and that allows a relatively smooth set of admin operations. For example, in commodity virtual or physical hardware, the size of the database should not exceed 1 to 3 TBytes in size.

* Should property sharding start relatively small and increase in size overtime, it is recommended to create more property shards that initially may be co-located on the same server.

* When the size of the database grows, more servers may be added to allow hardware resharding of the database.
This administrative change happens during normal online operations.

* Database resharding, i.e. the change of the number of property shards, can be executed offline using the `neo4j-admin database copy` command.
See xref:scalability/sharded-property-databases/data-ingestion.adoc#splitting-existing-db-into-shards[Splitting an existing database into shards].

[NOTE]
====
Online resharding (adding new shards, removing old ones, relocating data to accommodate the new topology) is currently not supported.
====

The block format (see xref:database-internals/store-formats.adoc[Store formats]) is required for both the graph shard and the property shard.
For accurate sizing estimation, contact your Neo4j representative for assistance.

