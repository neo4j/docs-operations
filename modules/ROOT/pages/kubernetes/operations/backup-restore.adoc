[role=enterprise-edition]
[[kubernetes-neo4j-backup-restore]]
= Back up and restore (online)

[NOTE]
====
For performing backups, Neo4j uses the _Admin Service_, which is only available inside the Kubernetes cluster and access to it should be guarded.
For more information, see xref:kubernetes/accessing-neo4j.adoc[Accessing Neo4j].
====

[[kubernetes-neo4j-backup-cloud]]
== Back up a database(s) to a cloud provider (AWS, GCP, and Azure) bucket

You can perform a backup of a Neo4j database(s) to any cloud provider (AWS, GCP, and Azure) bucket using the _neo4j/neo4j-admin_ Helm chart.
From version 4.4.22, the _neo4j/neo4j-admin_ Helm chart also supports performing a backup of multiple databases.

=== Prerequisites

Before you can back up a database and upload it to your bucket, verify that you have the following:

* A cloud provider bucket (AWS, GCP, or Azure) with read and write access to be able to upload the backup.
* Credentials to access the cloud provider bucket, such as a service account JSON key file for GCP, a credentials file for AWS, or storage account credentials for Azure.
* A Kubernetes cluster running on one of the cloud providers with the Neo4j Helm chart installed.
For more information, see xref:kubernetes/quickstart-standalone/index.adoc[Quickstart: Deploy a standalone instance] or xref:kubernetes/quickstart-cluster/index.adoc[Quickstart: Deploy a cluster].

=== Steps

To perform a backup of a Neo4j database to any cloud provider (AWS, GCP, and Azure) bucket, follow these steps:

. Update the repository to get the latest charts:
+
[source, shell, role='noheader']
----
helm repo update
----

. Create a Kubernetes secret with the credentials to access the cloud provider bucket using one of the following options:
+
[.tabbed-example]
=====
[.include-with-gke]
======
Create the secret named `gcpcreds` using your GCP service account JSON key file.
The JSON key file contains all the details of the service account that has access to the bucket.

[source, shell, role='noheader']
----
kubectl create secret generic gcpcreds --from-file=credentials=/path/to/gcpcreds.json
----
======

[.include-with-aws]
======
. Create a credentials file in the following format:
+
[source, properties, role='noheader']
----
[ default ]
region = us-east-1
aws_access_key_id = <your-aws_access_key_id>
aws_secret_access_key = <your-aws_secret_access_key>
----
. Create the secret named `awscreds` via the credentials file:
+
[source, shell, role='noheader']
----
kubectl create secret generic awscreds --from-file=credentials=/path/to/your/credentials
----
======

[.include-with-azure]
======
. Create a credentials file in the following format:
+
[source, properties, role='noheader']
----
AZURE_STORAGE_ACCOUNT_NAME=<your-azure-storage-account-name>
AZURE_STORAGE_ACCOUNT_KEY=<your-azure-storage-account-key>
----
. Create the secret named `azurecred` via the credentials file:
+
[source, shell, role='noheader']
----
kubectl create secret generic azurecred --from-file=credentials=/path/to/your/credentials
----
======
=====

. Configure the backup parameters in the _backup-values.yaml_ file using one of the following options:
+
[NOTE]
====
The following examples show the minimum configuration required to perform a backup to a cloud provider bucket.
For more information about the available backup parameters, see <<kubernetes-neo4j-backup-parameters, Backup parameters>>.
====
+
[.tabbed-example]
=====
[.include-with-gke]
======
[source, yaml, role='noheader']
----
neo4j:
  image: "neo4j/helm-charts-backup"
  imageTag: "4.4.22"
  jobSchedule: "* * * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  backoffLimit: 3

backup:
  bucketName: "my-bucket"
  databaseAdminServiceName:  "standalone-admin" #This is the Neo4j Admin Service name.
  database: "neo4j,system"
  cloudProvider: "gcp"
  secretName: "gcpcreds"
  secretKeyName: "credentials"
  checkConsistency: true
----
======

[.include-with-aws]
======
[source, yaml, role='noheader']
----
neo4j:
  image: "neo4j/helm-charts-backup"
  imageTag: "4.4.22"
  jobSchedule: "* * * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  backoffLimit: 3

backup:
  bucketName: "my-bucket"
  databaseAdminServiceName:  "standalone-admin"
  database: "neo4j,system"
  cloudProvider: "aws"
  secretName: "awscreds"
  secretKeyName: "credentials"
  checkConsistency: true
----
======

[.include-with-azure]
======
[source, yaml, role='noheader']
----
neo4j:
  image: "neo4j/helm-charts-backup"
  imageTag: "4.4.22"
  jobSchedule: "* * * * *"
  successfulJobsHistoryLimit: 3
  failedJobsHistoryLimit: 1
  backoffLimit: 3

backup:
  bucketName: "my-bucket"
  databaseAdminServiceName:  "standalone-admin"
  database: "neo4j,system"
  cloudProvider: "azure"
  secretName: "azurecreds"
  secretKeyName: "credentials"
  checkConsistency: true
----
======
=====
+
The _/backups_ mount created by default is an _emptyDir_ type volume.
This means that the data stored in this volume is not persistent and will be lost when the pod is deleted.
To use a persistent volume for backups add the following section to the _backup-values.yaml_ file:
+
[source, yaml, role='noheader']
----
tempVolume:
  persistentVolumeClaim:
    claimName: backup-pvc
----
+
[NOTE]
====
You need to create the persistent volume and persistent volume claim before installing the _neo4j-admin_ Helm chart.
For more information, see xref:kubernetes/persistent-volumes.adoc[Volume mounts and persistent volumes].
====

. Install _neo4j-admin_ Helm chart using the _backup-values.yaml_ file:
+
[source, shell, role='noheader']
----
helm install backup-name neo4j-admin -f /path/to/your/backup-values.yaml
----
+
The _neo4j/neo4j-admin_ Helm chart installs a cronjob that launches a pod based on the job schedule. This pod performs a backup of one or multiple databases, a consistency check of the backup file(s),  and uploads them to the cloud provider bucket.

. Monitor the backup pod logs using `kubectl logs pod/<neo4j-backup-pod-name>` to check the progress of the backup.
. Check that the backup files and the consistency check reports have been uploaded to the cloud provider bucket.

[[kubernetes-neo4j-backup-parameters]]
=== Backup parameters

To see what options are configurable on the Helm chart use `helm show values` and the Helm chart _neo4j/neo4j-admin_. +
From version 4.4.22, the _neo4j/neo4j-admin_ Helm chart also supports assigning your Neo4j pods to specific nodes using `nodeSelector` labels, and from Neo4j 4.4.23, using affinity/anti-affinity rules or tolerations.
For more information, see xref:kubernetes/operations/assign-neo4j-pods.adoc[Assigning backup pods to specific nodes] and the Kubernetes official documentation on link:https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#affinity-and-anti-affinity[Affinity and anti-affinity] rules and https://kubernetes.io/docs/concepts/scheduling-eviction/taint-and-toleration/[Taints and Tolerations].

For example:

[source, shell, role='noheader']
----
helm show values neo4j/neo4j-admin
----

[source, yaml, role='noheader']
----
## @param nameOverride String to partially override common.names.fullname
nameOverride: ""
## @param fullnameOverride String to fully override common.names.fullname
fullnameOverride: ""
# disableLookups will disable all the lookups done in the helm charts
# You can enable this when executing helm commands with --dry-run command
disableLookups: false

neo4j:
  image: "neo4j/helm-charts-backup"
  imageTag: "4.4.23"
  podLabels: {}
#    app: "demo"
#    acac: "dcdddc"
  podAnnotations: {}
#    ssdvvs: "svvvsvs"
#    vfsvswef: "vcfvgb"
  # define the backup job schedule . default is * * * * *
  jobSchedule: ""
  # default is 3
  successfulJobsHistoryLimit:
  # default is 1
  failedJobsHistoryLimit:
  # default is 3
  backoffLimit:
  #add labels if required
  labels: {}

backup:
  # Ensure the bucket is already existing in the respective cloud provider
  # In case of azure the bucket is the container name in the storage account
  # bucket: azure-storage-container
  bucketName: ""

  #address details of the neo4j instance from which backup is to be done (serviceName or ip either one is required)

  #ex: standalone-admin.default.svc.cluster.local:6362
  # admin service name -  standalone-admin
  # namespace - default
  # cluster domain - cluster.local
  # port - 6362

  #ex: 10.3.3.2:6362
  # admin service ip - 10.3.3.2
  # port - 6362

  databaseAdminServiceName: ""
  databaseAdminServiceIP: ""
  #default name is 'default'
  databaseNamespace: ""
  #default port is 6362
  databaseBackupPort: ""
  #default value is cluster.local
  databaseClusterDomain: ""

  #name of the database to backup ex: neo4j or neo4j,system (You can provide command separated database names)
  # In case of comma separated databases failure of any single database will lead to failure of complete operation
  database: ""
  # cloudProvider can be either gcp, aws, or azure
  cloudProvider: ""

  # name of the kubernetes secret containing the respective cloud provider credentials
  # Ensure you have read,write access to the mentioned bucket
  # For AWS :
  # add the below in a file and create a secret via
  # 'kubectl create secret generic awscred --from-file=credentials=/demo/awscredentials'

  #  [ default ]
  #  region = us-east-1
  #  aws_access_key_id = XXXXX
  #  aws_secret_access_key = XXXX

  # For AZURE :
  # add the storage account name and key in below format in a file create a secret via
  # 'kubectl create secret generic azurecred --from-file=credentials=/demo/azurecredentials'

  #  AZURE_STORAGE_ACCOUNT_NAME=XXXX
  #  AZURE_STORAGE_ACCOUNT_KEY=XXXX

  # For GCP :
  # create the secret via the gcp service account json key file.
  # ex: 'kubectl create secret generic gcpcred --from-file=credentials=/demo/gcpcreds.json'
  secretName: ""
  # provide the keyname used in the above secret
  secretKeyName: ""

  #setting this to true will not delete the backup files generated at the /backup mount
  keepBackupFiles: true

  #Below are all neo4j-admin database backup flags / options
  #To know more about the flags read here : https://neo4j.com/docs/operations-manual/4.4/backup-restore/online-backup/
  pageCache: ""
  fallbackToFull: true
  includeMetadata: "all"
  parallelRecovery: false
  verbose: true
  heapSize: ""
  checkConsistency: true
  checkIndexes: true
  checkIndexStructure: true
  checkGraph: true
  prepareRestore: true



# Set to name of an existing Service Account to use if desired
serviceAccountName: ""

# Volume to use as temporary storage for files before they are uploaded to cloud. For large databases local storage may not have sufficient space.
# In that case set an ephemeral or persistent volume with sufficient space here
# The chart defaults to an emptyDir, use this to overwrite default behavior
#tempVolume:
#  persistentVolumeClaim:
#    claimName: backup-pvc

# securityContext defines privilege and access control settings for a Pod. Making sure that we don't run Neo4j as root user.
securityContext:
  runAsNonRoot: true
  runAsUser: 7474
  runAsGroup: 7474
  fsGroup: 7474
  fsGroupChangePolicy: "Always"

# default ephemeral storage of backup container
resources:
  requests:
    ephemeralStorage: "4Gi"
  limits:
    ephemeralStorage: "5Gi"

# nodeSelector labels
# please ensure the respective labels are present on one of nodes or else helm charts will throw an error
nodeSelector: {}
#  label1: "true"
#  label2: "value1"

# set backup pod affinity
affinity: {}
#  podAffinity:
#    requiredDuringSchedulingIgnoredDuringExecution:
#      - labelSelector:
#          matchExpressions:
#            - key: security
#              operator: In
#              values:
#                - S1
#        topologyKey: topology.kubernetes.io/zone
#  podAntiAffinity:
#    preferredDuringSchedulingIgnoredDuringExecution:
#      - weight: 100
#        podAffinityTerm:
#          labelSelector:
#            matchExpressions:
#              - key: security
#                operator: In
#                values:
#                  - S2
#          topologyKey: topology.kubernetes.io/zone

#Add tolerations to the backup pod
tolerations: []
#  - key: "key1"
#    operator: "Equal"
#    value: "value1"
#    effect: "NoSchedule"
#  - key: "key2"
#    operator: "Equal"
#    value: "value2"
#    effect: "NoSchedule"
----

[[kubernetes-neo4j-restore]]
== Restore a single database

To restore a single offline database or a database backup, you first need to delete the database that you want to replace unless you want to restore the backup as an additional database in your DBMS, then
use the restore command of `neo4j-admin` to restore the database backup, and finally, use the Cypher command `CREATE DATABASE name` to create the restored database in the `system` database.

=== Delete the database that you want to replace

Before you restore the database backup, you have to delete the database that you want to replace with that backup using the Cypher command `DROP DATABASE name` against the `system` database.
If you want to restore the backup as an additional database in your DBMS, then you can proceed to the next section.

[NOTE]
====
For Neo4j cluster deployments, you run the Cypher command `DROP DATABASE name` only on one of the cluster members.
The command is automatically routed to the leader and from there to the other cluster members.
====
. Connect to the Neo4j DBMS:
+
[source, shell, role='noheader']
----
kubectl exec -it <release-name>-0 -- bash
----
+
. Connect to the `system` database using `cypher-shell`:
+
[source, shell, role='noheader']
----
cypher-shell -u neo4j -p <password> -d system
----
+
. Drop the database you want to replace with the backup:
+
[source, cypher, role='noheader']
----
DROP DATABASE neo4j;
----
. Exit the Cypher Shell command-line console:
+
[source, shell, role='noheader']
----
:exit;
----

=== Restore the database backup

You use the `neo4j-admin restore` command to restore the database backup, and then the Cypher command `CREATE DATABASE name` to create the restored database in the `system` database.
For information about the command syntax, options, and usage, see xref:backup-restore/restore-backup.adoc[Restore a database backup].

. Restore the `neo4j` database backup.
+
[NOTE]
====
For Neo4j cluster deployments, restore the database backup on each cluster member.
====
. Run the `neo4j-admin restore` command:
+
[source, shell, role='noheader']
----
neo4j-admin restore --database=neo4j --from=/backups/neo4j --expand-commands
----
+
. Connect to the `system` database using `cypher-shell`:
+
[source, shell, role='noheader']
----
cypher-shell -u neo4j -p <password> -d system
----
+
. Create `neo4j` database.
+
[NOTE]
====
For Neo4j cluster deployments, you run the Cypher command `CREATE DATABASE name` only on one of the cluster members.
====
+
[source, cypher, role='noheader']
----
CREATE DATABASE neo4j;
----
. Open the browser at _http://<external-ip>:7474/browser/_ and check that all data has been successfully restored.
. Execute a Cypher command against the `neo4j` database, for example:
+
[source, cypher, role='noheader']
----
MATCH (n) RETURN n
----
+
[NOTE]
====
If you have backed up your database with the option `--include-metadata`, you can manually restore the users and roles metadata.
For more information, see xref:backup-restore/restore-backup.adoc#restore-backup-example[Restore a database backup -> Example].
====

[NOTE]
====
To restore the `system` database, follow the steps described in xref:kubernetes/operations/dump-load.adoc[Dump and load databases (offline)].
====
