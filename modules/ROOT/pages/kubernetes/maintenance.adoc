:description: The section describes some maintenance operations when running Neo4j in a Kubernetes cluster.
[[kubernetes-maintenance]]
= Operations
:description: The section describes some operations when running Neo4j in a Kubernetes cluster. 

[[online-maintenance]]
== Online Maintenance

Online maintenance does not require stopping the `neo4j` process.
It is performed using the command `kubectl exec`.

To directly run tasks:

[source, shell]
----
kubectl exec <release-name>-0 -- neo4j-admin store-info --all /var/lib/neo4j/data/databases --expand-commands
----

[NOTE]
====
All `neo4j-admin` commands need the `--expand-commands` flag to run in the Neo4j container.
This is because the Neo4j Helm Chart defines the Neo4j configuration using command expansion to dynamically resolve some configuration parameters at runtime.
====

To run a series of commands, use an interactive shell:

[source, shell]
----
kubectl exec -it <release-name>-0 -- bash
----

[NOTE]
====
Processes executed using `kubectl exec` count towards the Neo4j container’s memory allocation.
Therefore, running tasks that use a significant amount of memory or running Neo4j in an extremely memory-constrained configuration could cause the Neo4j container to be terminated by the underlying Operating System.
====

[[offline-maintenance]]
== Offline Maintenance

You use the Neo4j offline maintenance mode to perform maintenance tasks that require Neo4j to be offline.
In this mode, the `neo4j` process is not running.
However, the Neo4j Pod does run, but it never reaches the status `READY`.

[[put-offline-mode]]
=== Put the Neo4j instance in offline mode

. To put the Neo4j instance in offline maintenance mode, you set the `offlineMaintenanceModeEnabled: true` and upgrade the helm release.

* You can do that by using the _values.yaml_ file:
.. Open your _values.yaml_ file and add `offlineMaintenanceModeEnabled: true` to the `neo4j` object:
+
[source, yaml]
----
neo4j:
 offlineMaintenanceModeEnabled: true
----
+
.. Run `helm upgrade` to apply the changes:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----
* Alternatively, you can set `neo4j.offlineMaintenanceModeEnabled` to `true` as part of the `helm upgrade` command:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone --version={neo4j-version-exact} --set neo4j.offlineMaintenanceModeEnabled=true
----

. Poll `kubectl get pods` until the pod has restarted (`STATUS`=`Running`).
+
[source, shell]
----
kubectl get pod <release-name>-0
----
+
. Connect to the pod with an interactive shell:
+
[source, shell]
----
kubectl exec -it "<release-name>-0" -- bash
----
+
. View running java processes:
+
[source, shell]
----
jps
----
+
[queryresult]
----
19 Jps
----
+
The result shows no running java process other than `jps` itself.


[[offline-run-tasks]]
=== Run task in offline mode

Offline maintenance tasks are performed using the command `kubectl exec`.

* To directly run tasks:
+
[source, shell]
----
kubectl exec <release-name>-0 -- neo4j-admin store-info --all /var/lib/neo4j/data/databases --expand-commands
----

* To run a series of commands, use an interactive shell:
+
[source, shell]
----
kubectl exec -it <release-name>-0 -- bash
----

* For long-running commands, use a shell and run tasks using `nohup` so they continue if the `kubectl exec` connection is lost:
+
[source, shell]
----
kubectl exec -it <release-name>-0 -- bash
  $ nohup neo4j-admin check-consistency --database=neo4j --expand-commands &>job.out </dev/null &
  $ tail -f job.out
----

[[put-online-mode]]
=== Put the Neo4j DBMS in online mode

When you finish with the maintenance tasks, return the Neo4j instance to a normal operation:

* You can do that by using the _values.yaml_ file:
. Open your _values.yaml_ file and add `offlineMaintenanceModeEnabled: false` to the `neo4j` object:
+
[source, yaml]
----
neo4j:
 offlineMaintenanceModeEnabled: false
----
+
. Run `helm upgrade` to apply the changes:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----

* Alternatively, you can run `helm upgrade` with the flag set to `false`:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone --version={neo4j-version-exact} --set neo4j.offlineMaintenanceModeEnabled=false
----

[[reset-password]]
== Reset the `neo4j` user password

You reset the `neo4j` user password by disabling authentication and then re-enabling it.

. In the _values.yaml_ file, set `dbms.security.auth_enabled:` to `false` to disable the authentication:
+
[NOTE]
====
All Neo4j `config` values must be YAML strings, not YAML booleans.
Therefore, make sure you put quotes around values, such as `"true"` or `"false"`, so that they are handled correctly by Kubernetes.
====
+
[source, yaml]
----
# Neo4j Configuration (yaml format)
config:
  dbms.security.auth_enabled: "false"
----
+
. Run the following command to apply the changes:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----
+
Authentication is now disabled.
+
. Connect with `cypher-shell` and set the desired password:
+
[source, cypher]
----
ALTER USER neo4j SET PASSWORD '<new-password>'
----
+
. Update the Neo4j configuration to enable authentication:
+
[source, yaml]
----
# Neo4j Configuration (yaml format)
config:
  dbms.security.auth_enabled: "true"
----
+
. Run the following command to apply the update and re-enable authentication:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----
+
Authentication is now enabled, and the Neo4j user password has been reset to the desired password.


[[kubernetes-neo4j-dump-load]]
== Dump and load databases (offline)

You can use the `neo4j-admin dump` command to make a full backup (an archive) of an **offline** database(s) and `neo4j-admin load` to load it back into a Neo4j deployment.
These operations are performed in xref:kubernetes/maintenance.adoc#offline-maintenance[offline maintenance mode].

[[kubernetes-neo4j-dump]]
=== Dump the `neo4j` and `system` databases

. xref:kubernetes/maintenance.adoc#put-offline-mode[Put the Neo4j instance in offline mode].
. Dump `neo4j` and `system` databases:
+
[source, shell]
----
neo4j-admin dump --expand-commands --database=system --to /backups/system.dump && neo4j-admin dump --expand-commands --database=neo4j --to /backups/neo4j.dump
----
+
. xref:kubernetes/maintenance.adoc#put-online-mode[Put the Neo4j DBMS in online mode].
. Verify that Neo4j is working by refreshing Neo4j Browser.

[TIP]
====
For information about the command syntax, options, and usage, see xref:backup-restore/offline-backup.adoc[Back up an offline database].
====

[[kubernetes-neo4j-load]]
=== Load the `neo4j` and  `system` databases

. xref:kubernetes/maintenance.adoc#put-offline-mode[Put the Neo4j instance in offline mode].
. Run `neo4j-admin load` commands:
+
[source, shell]
----
neo4j-admin load --expand-commands --database=system --from /backups/system.dump && neo4j-admin load --expand-commands --database=neo4j --from /backups/neo4j.dump
----
+
[TIP]
====
For information about the command syntax, options, and usage, see xref:backup-restore/restore-dump.adoc[Restore a database dump].
====
+
. xref:kubernetes/maintenance.adoc#put-online-mode[Put the Neo4j DBMS in online mode].
. Verify that Neo4j is working by refreshing Neo4j Browser.

[role=enterprise-edition]
[[kubernetes-neo4j-backup-restore]]
== Back up and restore a single database (online)

You can use the `neo4j-admin backup` command to make a full or incremental backup of an **online** database(s) and `neo4j-admin restore` to restore it in a live Neo4j DBMS or cluster.
These operations are performed in xref:kubernetes/maintenance.adoc#online-maintenance[online maintenance mode].

[NOTE]
====
For performing backups, Neo4j uses the _Admin Service_, which is only available inside the Kubernetes cluster and access to it should be guarded.
For more information, see xref:kubernetes/quickstart-cluster/access-inside-k8s.adoc[Access the Neo4j cluster from inside Kubernetes]
and xref:kubernetes/quickstart-cluster/access-outside-k8s.adoc[Access the Neo4j cluster from outside Kubernetes].
====

[[kubernetes-neo4j-backup]]
=== Back up a single database

The `neo4j-admin backup` command can be run both from the same and a separate pod.
However, it uses resources (CPU, RAM) in the Neo4j container (competing with Neo4j itself), because it checks the database consistency at the end of every backup operation.
Therefore, it is recommended to run the operation in a separate pod.

[NOTE]
====
In the Neo4j Helm Charts, the backup configurations are set by default to `dbms.backup.enabled=true` and `dbms.backup.listen_address=0.0.0.0:6362`.

Note that the default for Neo4j on-site installations is to listen only on 127.0.0.1, which will not work from other containers, since they would not be able to access the backup port.
====

**Back up a database from a separate pod**

. Create a Neo4j instance pod to get access to the `neo4j-admin` command:
+
[source, shell, subs="attributes"]
----
kubectl run —rm -it —image “neo4j:{neo4j-version-exact}-enterprise” backup — bash
----

. Run the following command to back up the database you want.
In this example, this is the `neo4j` database.
The command is the same for standalone instances and Neo4j cluster members.
+
[source, shell]
----
bin/neo4j-admin backup --from=my-neo4j-release-admin.default.svc.cluster.local:6362 --database=neo4j --backup-dir=/backups --expand-commands
----

[[kubernetes-neo4j-restore]]
=== Restore a single database

To restore a single offline database or a database backup, you first need to delete the database that you want to replace unless you want to restore the backup as an additional database in your DBMS, then
use the restore command of `neo4j-admin` to restore the database backup, and finally, use the Cypher command `CREATE DATABASE name` to create the restored database in the `system` database.

==== Delete the database that you want to replace

Before you restore the database backup, you have to delete the database that you want to replace with that backup using the Cypher command `DROP DATABASE name` against the `system` database.
If you want to restore the backup as an additional database in your DBMS, then you can proceed to the next section.

[NOTE]
====
For Neo4j cluster deployments, you run the Cypher command `DROP DATABASE name` only on one of the cluster members.
The command is automatically routed to the leader and from there to the other cluster members.
====
. Connect to the Neo4j DBMS:
+
[source, shell]
----
kubectl exec -it <release-name>-0 -- bash
----
+
. Connect to the `system` database using `cypher-shell`:
+
[source, shell]
----
cypher-shell -u neo4j -p <password> -d system
----
+
. Drop the database you want to replace with the backup:
+
[source, cypher]
----
DROP DATABASE neo4j;
----
. Exit the Cypher Shell command-line console:
+
[source, shell]
----
:exit;
----

==== Restore the database backup

You use the `neo4j-admin restore` command to restore the database backup, and then the Cypher command `CREATE DATABASE name` to create the restored database in the `system` database.
For information about the command syntax, options, and usage, see xref:backup-restore/restore-backup.adoc[Restore a database backup].

. Restore the `neo4j` database backup.
+
[NOTE]
====
For Neo4j cluster deployments, restore the database backup on each cluster member.
====
. Run the `neo4j-admin restore` command:
+
[source, shell]
----
neo4j-admin restore --database=neo4j --from=/backups/neo4j --expand-commands
----
+
. Connect to the `system` database using `cypher-shell`:
+
[source, shell]
----
cypher-shell -u neo4j -p <password> -d system
----
+
. Create `neo4j` database.
+
[NOTE]
====
For Neo4j cluster deployments, you run the Cypher command `CREATE DATABASE name` only on one of the cluster members.
====
+
[source, cypher]
----
CREATE DATABASE neo4j;
----
. Open the browser at _http://<external-ip>:7474/browser/_ and check that all data has been successfully restored.
. Execute a Cypher command against the `neo4j` database, for example:
+
[source, cypher]
----
MATCH (n) RETURN n
----
+
[NOTE]
====
If you have backed up your database with the option `--include-metadata`, you can manually restore the users and roles metadata.
For more information, see xref:backup-restore/restore-backup.adoc#restore-backup-example[Restore a database backup -> Example].
====

[NOTE]
====
To restore the `system` database, follow the steps described in xref:kubernetes/maintenance.adoc#kubernetes-neo4j-dump-load[Dump and load databases (offline)].
====

[[kubernetes-upgrading]]
== Upgrade the Neo4j DBMS on Kubernetes

To upgrade from Neo4j Community to Enterprise edition, run:

[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone --set neo4j.edition=enterprise --set neo4j.acceptNeo4jLicenseAgreement=yes
----

To upgrade to the next patch release of Neo4j, update your Neo4j _values.yaml_ file and upgrade the helm release.

. Open the _values.yaml_ file, using the code editor of your choice, and add the following line to the `image` object:
+
[source, yaml, subs="attributes"]
----
image:
  customImage: neo4j:{neo4j-version-exact}
----
+
. Run `helm upgrade` to apply the changes:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----

[[_migrate_neo4j_from_the_labs_helm_charts_to_the_neo4j_helm_charts_offline]]
== Migrate Neo4j from the Labs Helm charts to the Neo4j Helm charts (offline)

To migrate your Neo4j deployment from the Labs Helm charts to the Neo4j Helm charts, back up your standalone instance or cluster created with the Labs Helm charts and restore it in a standalone instance or a cluster created using the Neo4j Helm charts.

Neo4j supports the following migration paths for a single instance and a cluster:

Single instance::
* From the Labs Helm charts 3.5 or earlier to either Neo4j Helm charts 4.3 or 4.4 -- upgrade your Neo4j deployment to whichever version you want to move to using the steps in the https://neo4j.com/labs/neo4j-helm/1.0.0/ and then migrate from the Labs Helm charts (4.3 or 4.4) to Neo4j Helm charts 4.3 or 4.4 using the steps described here.
* From the Labs Helm charts 4.3 to Neo4j Helm charts 4.3 -- follow the steps described here.
* From the Labs Helm charts 4.3 to Neo4j Helm charts 4.4 -- follow the steps described here.

Cluster::
From the Labs Helm charts 4.3 or 4.4 to Neo4j Helm charts 4.4 -- follow the steps described here.

=== Back up a Neo4j deployment created with the Labs Helm charts

To back up your Neo4j deployment created with the Labs Helm charts, follow the steps in the https://neo4j.com/labs/neo4j-helm/1.0.0/backup/[Neo4j-Helm User Guide -> Backing up Neo4j Containers].

=== Restore your backup into a standalone or a cluster created with the Neo4j Helm charts

If the backup exists on a cloud provider, you can take one of the following approaches:

Approach 1::
. Create a standalone or a cluster using the Neo4j Helm charts with a custom Neo4j image that has all the cloud provider utilities to download the backup from the respective cloud provider storage to your specific mount.
. Restore the backup following the steps described in xref:kubernetes/maintenance.adoc#kubernetes-neo4j-restore[Restore a single database].

Approach 2::
. Get the backup on your local machine.
. Copy the backup to the respective mount in your new cluster created using the Neo4j Helm charts, using the command `kubectl cp <local-path> <pod>:<path>`.
For example,
+
[source, shell]
----
kubectl cp /Users/username/Desktop/backup/4.3.3/neo4j standalone-0:/tmp/
----
where the _/tmp_ directory refers to the mount.
. Restore the back up following the steps described in xref:kubernetes/maintenance.adoc#kubernetes-neo4j-restore[Restore a single database].


[[scaling]]
== Scale a Neo4j deployment

Neo4j supports both vertical and horizontal scaling.

[[vertical-scaling]]
=== Vertical scaling

To increase or decrease the resources (CPU, memory) available to a Neo4j instance, change the `neo4j.resources` object in the _values.yaml_ file to set the desired resource usage, and then perform a helm upgrade.

[NOTE]
====
If you change the memory allocated to the Neo4j container, you should also change the Neo4j's memory configuration (`dbms.memory.heap.max_size` and `dbms.memory.pagecache.size` in particular).
See xref:configure-resources[Configure Resource Allocation] for more details.
====

For example, if your running Neo4j instance has the following allocated resources:

[source, role=noheader]
----
# values.yaml
neo4j:
  resources:
    cpu: "1"
    memory: "3Gi"

# Neo4j Configuration (yaml format)
config:
  dbms.memory.heap.initial_size: "2G"
  dbms.memory.heap.max_size: "2G"
  dbms.memory.pagecache.size: "500m"
----

And, you want to increase them to 2 CPUs and 4 GB of memory (allocating additional memory to the pagecache).

. Modify the _values.yaml_ file to set the desired resource usage:
+
[source, yaml]
----
# values.yaml
neo4j:
  resources:
    cpu: "2"
    memory: "4Gi"

# Neo4j Configuration (yaml format)
config:
  dbms.memory.heap.initial_size: "2G"
  dbms.memory.heap.max_size: "2G"
  dbms.memory.pagecache.size: "1G"
----
+
. Run `helm upgrade` with the modified deployment _values.yaml_ file and the respective Helm Chart (_neo4j/neo4j-standalone_, _neo4j/neo4j-cluster-core_, or _neo4j/neo4j-cluster-read-replica)_ to apply the changes.
For example:
+
[source, shell]
----
helm upgrade <release-name> neo4j/neo4j-standalone -f values.yaml
----

[role=enterprise-edition]
[[horizontal-scaling]]
=== Horizontal scaling

You can add a new core member or a read replica to the Neo4j cluster to scale out write or read workloads.

. In the Kubernetes cluster, verify that you have a node that you can use for the new Neo4j cluster member.
. Create a persistent disk for the new Neo4j cluster member to be used for its `data` volume mount.
For more information, see xref:kubernetes/quickstart-cluster/create-pv.adoc[Create a persistent volume for each cluster member] and xref:kubernetes/persistent-volumes.adoc[Volume mounts and persistent volumes].
. Create a Helm deployment YAML file for the new Neo4j cluster member with all the configuration settings and the disk you have created for it.
For more information, see xref:kubernetes/quickstart-cluster/create-value-file.adoc[Create Helm deployment values files] and xref:kubernetes/configuration.adoc[Configure a Neo4j Helm deployment].
. Install the new member using the command `helm install`, the deployment _values.yaml_ file, and the respective Helm Chart (_neo4j/neo4j-cluster-core_ or _neo4j/neo4j-cluster-read-replica)_.
For example:
+
[source, shell, subs="attributes"]
----
helm install rr-2 neo4j/neo4j-cluster-read-replica -f rr-2.values.yaml
----

[[imagepullsecrets]]
== Use custom images from private registries

Neo4j 4.4.4 introduces the support for using custom images from private registries by adding new or existing `imagePullSecrets`.

=== Add an existing `imagePullSecret`

You can use an existing `imagePullSecret` for your Neo4j deployment by specifying its name in the _values.yaml_ file.
Neo4j Helm charts will check if the provided `imagePullSecret` exists in the Kubernetes cluster and use it.
If a secret with the given name does not exist in the cluster, Helm Charts will throw an error.

.Using an already existing secret *mysecret*
[source, yaml]
----
# values.yaml
# Override image settings in Neo4j pod
image:
  imagePullPolicy: IfNotPresent
  # set a customImage if you want to use your own docker image
  customImage: demo_neo4j_image:v1

  #imagePullSecrets list
  imagePullSecrets:
      - "mysecret"
----

=== Create and add a new `imagePullSecret`

You can create a new `imagePullSecret` for your Neo4j deployment by defining an equivalent `imageCredential` in the _values.yaml_ file.

Neo4j Helm charts will create a secret with the given name and use it as an `imagePullSecret` to pull the custom image defined.
The following example shows how to define a private docker registry `mysecret` `imageCredential`.

.Creating and adding `mysecret` as the `imagePullSecret` to the cluster.
[source, yaml]
----
# values.yaml
# Override image settings in Neo4j pod
image:
  imagePullPolicy: IfNotPresent
  # set a customImage if you want to use your own docker image
  customImage: demo_neo4j_image:v1

  #imagePullSecrets list
  imagePullSecrets:
      - "mysecret"

  #imageCredentials list for which secret of type docker-registry will be created automatically using the details provided
  # registry, username, password, email are compulsory fields for an imageCredential, without any helm chart will throw an error
  # imageCredential name should be part of the imagePullSecrets list or else the respective imageCredential will be ignored and no secret creation will be done
  imageCredentials:
    - registry: "https://index.docker.io/v1/"
      username: "demouser"
      password: "demopass123"
      email: "demo@company1.com"
      name: "mysecret"
----

[[NodeSelector]]
== Assign Neo4j pods to specific nodes

From version 4.4.5, Neo4j provides support for assigning Neo4j pods to specific nodes using `nodeSelector` labels.

You specify the `nodeSelector` labels in the _values.yaml_ file.

[NOTE]
====
If there is no node with the given labels, the Helm chart will throw an error.
====

.nodeSelector labels in _values.yaml_
[source, yaml]
----
#nodeSelector labels
#Ensure the respective labels are present on one of the cluster nodes or else Helm charts will throw an error.
nodeSelector:
   nodeNumber: one
   name: node1
----
