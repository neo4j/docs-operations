:description: This section describes how to perform bulk offline imports of data into Neo4j using the command line tool `neo4j-admin database import`.
[[neo4j-admin-import]]
= Import

:rfc-4180: https://tools.ietf.org/html/rfc4180

`neo4j-admin database import` writes CSV data into Neo4j's native file format as fast as possible. +
Starting with version 5.26, Neo4j also provides support for the Parquet file format.

You should use this tool when:

* Import performance is important because you have a large amount of data (millions/billions of entities).
* The database can be taken offline and you have direct access to one of the servers hosting your Neo4j DBMS.
* The database is either empty or its content is unchanged since a previous incremental import.
* The CSV data is clean/fault-free (nodes are not duplicated and relationships' start and end nodes exist).
This tool can handle data faults but performance is not optimized.
If your data has a lot of faults, it is recommended to clean it using a dedicated tool before import.

Other methods of importing data into Neo4j might be better suited to non-admin users:

* Cypher(R) - CSV data can be bulk loaded via the Cypher command `LOAD CSV`.
See link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/load-csv/[Cypher Manual -> `LOAD CSV`].
* Graphical Tools - link:{neo4j-docs-base-uri}/aura/auradb/importing/importing-data/#_load_csv[Neo4j AuraDB -> Importing data].

[NOTE]
====
Change Data Capture does **not** capture any data changes resulting from the use of `neo4j-admin database import`.
See link:{neo4j-docs-base-uri}/cdc/current/get-started/self-managed/#non-tx-log-changes/[Change Data Capture -> Key considerations] for more information.
====

== Overview

The `neo4j-admin database import` command has two modes both used for initial data import:

* _full_ -- used to import data into a non-existent empty database.
* _incremental_ -- used when import cannot be completed in a single _full_ import, by allowing the import to be a series of smaller imports.

[NOTE]
====
The user running `neo4j-admin database import` must have `WRITE` capabilities into `server.directories.data` and `server.directories.log`.
====

This section describes the `neo4j-admin database import` option.


[TIP]
====
For information on `LOAD CSV`, see the link:{neo4j-docs-base-uri}/cypher-manual/current/clauses/load-csv[Cypher Manual -> `LOAD CSV`].
For in-depth examples of using the command `neo4j-admin database import`, refer to the xref:tutorial/neo4j-admin-import.adoc[Tutorials -> Neo4j Admin import].
====

These are some things you need to keep in mind when creating your input files:

* Fields are comma-separated by default but a different delimiter can be specified.
* All files must use the same delimiter.
* Multiple data sources can be used for both nodes and relationships.
* A data source can optionally be provided using multiple files.
* A separate file with a header that provides information on the data fields, must be the first specified file of each data source.
* Fields without corresponding information in the header are not read.
* UTF-8 encoding is used.
* By default, the importer trims extra whitespace at the beginning and end of strings.
  Quote your data to preserve leading and trailing whitespaces.


[NOTE]
.Indexes and constraints
====
Indexes and constraints are not created during the import.
Instead, you have to add these afterward (see link:{neo4j-docs-base-uri}/cypher-manual/current/indexes-for-full-text-search[Cypher Manual -> Indexes]).

You can use the `--schema` option to create and populate indexes and constraints during the import process.
The option is available in the Enterprise Edition and works only for the block format.
See <<indexes-constraints-import, Provide indexes and constraints during import>> for more information.
====

[[import-tool-full]]
== Full import

[[import-tool-syntax]]
=== Syntax

The syntax for importing a set of CSV files is:

[source, syntax, role="nocopy"]
----
neo4j-admin database import full [-h] [--expand-commands] [--verbose] [--auto-skip-subsequent-headers[=true|false]]
                                 [--ignore-empty-strings[=true|false]] [--ignore-extra-columns[=true|false]]
                                 [--legacy-style-quoting[=true|false]] [--normalize-types[=true|false]]
                                 [--overwrite-destination[=true|false]] [--skip-bad-entries-logging[=true|false]]
                                 [--skip-bad-relationships[=true|false]] [--skip-duplicate-nodes[=true|false]] [--strict
                                 [=true|false]] [--trim-strings[=true|false]] [--additional-config=<file>]
                                 [--array-delimiter=<char>] [--bad-tolerance=<num>] [--delimiter=<char>]
                                 [--format=<format>] [--high-parallel-io=on|off|auto] [--id-type=string|integer|actual]
                                 [--input-encoding=<character-set>] [--input-type=csv|parquet]
                                 [--max-off-heap-memory=<size>] [--quote=<char>] [--read-buffer-size=<size>]
                                 [--report-file=<path>] [--schema=<path>] [--threads=<num>] --nodes=[<label>[:
                                 <label>]...=]<files>... [--nodes=[<label>[:<label>]...=]<files>...]...
                                 [--relationships=[<type>=]<files>...]... [--multiline-fields=true|false|<path>[,
                                 <path>] [--multiline-fields-format=v1|v2]] <database>
----

=== Description

Initial import into a non-existent empty database.

=== Parameters

.`neo4j-admin database import full` parameters
[options="header", cols="5m,10a,2m"]
|===
| Parameter
| Description
| Default

|<database>
|Name of the database to import.
If the database into which you import does not exist prior to importing, you must create it subsequently using `CREATE DATABASE`.
|neo4j
|===

[WARNING]
====
Some of the options below are marked as *Advanced*.
These options should not be used for experimentation.

For more information, please contact Neo4j Professional Services.
====

=== Options

`neo4j-admin import` also supports the Parquet file format.
You can use the parameter `--input-type=csv|parquet` to explicitly specify whether to use CSV or Parquet for the importer.
If not defined, it defaults to CSV.
The xref:tools/neo4j-admin/neo4j-admin-import.adoc#import-tool-examples[examples] for CSV can also be used with Parquet.

[[full-import-options-table]]
.`neo4j-admin database import full` options
[options="header", cols="4m,6a,2m,1,2"]
|===
| Option
| Description
| Default
| CSV
| Parquet

|--additional-config=<file>footnote:[See xref:tools/neo4j-admin/index.adoc#_configuration[Tools -> Configuration] for details.]
|Configuration file with additional configuration.
|
| {check-mark}
| {check-mark}

|--array-delimiter=<char>
|Delimiter character between array elements within a value in CSV data. Also accepts `TAB` and e.g. `U+20AC` for specifying a character using Unicode.

====
* ASCII character -- e.g. `--array-delimiter=";"`.
* `\ID` -- Unicode character with ID, e.g. `--array-delimiter="\59"`.
* `U+XXXX` -- Unicode character specified with 4 HEX characters, e.g. `--array-delimiter="U+20AC"`.
* `\t` -- horizontal tabulation (HT), e.g. `--array-delimiter="\t"`.
====

For horizontal tabulation (HT), use `\t` or the Unicode character ID `\9`.

Unicode character ID can be used if prepended by `\`.
|;
| {check-mark}
| {check-mark}label:new[Available from 2025.02]

| --auto-skip-subsequent-headers[=true\|false]
|Automatically skip accidental header lines in subsequent files in file groups with more than one file.
|false
| {check-mark}
|

|--bad-tolerance=<num>
|Number of bad entries before the import is aborted. The import process is optimized for error-free data. Therefore, cleaning the data before importing it is highly recommended. If you encounter any bad entries during the import process, you can set the number of bad entries to a specific value that suits your needs. However, setting a high value may affect the performance of the tool.
|1000
| {check-mark}
| {check-mark}

|--delimiter=<char>
|Delimiter character between values in CSV data. Also accepts `TAB` and e.g. `U+20AC` for specifying a character using Unicode.

====
* ASCII character -- e.g. `--delimiter=","`.
* `\ID` -- Unicode character with ID, e.g. `--delimiter="\44"`.
* `U+XXXX` -- Unicode character specified with 4 HEX characters, e.g. `--delimiter="U+20AC"`.
* `\t` -- horizontal tabulation (HT), e.g. `--delimiter="\t"`.
====

For horizontal tabulation (HT), use `\t` or the Unicode character ID `\9`.

Unicode character ID can be used if prepended by `\`.
|,
| {check-mark}
|


|--expand-commands
|Allow command expansion in config value evaluation.
|
| {check-mark}
| {check-mark}

|--format=<format>
|Name of database format.
The imported database will be created in the specified format or use the format set in the configuration.
Valid formats are `standard`, `aligned`, `high_limit`, and `block`.
|
| {check-mark}
| {check-mark}

|-h, --help
|Show this help message and exit.
|
| {check-mark}
| {check-mark}

|--high-parallel-io=on\|off\|auto
|Ignore environment-based heuristics and indicate if the target storage subsystem can support parallel IO with high throughput or auto detect.
Typically this is `on` for SSDs, large raid arrays, and network-attached storage.
|auto
| {check-mark}
| {check-mark}

|--id-type=string\|integer\|actual
|Each node must provide a unique ID.
This is used to find the correct nodes when creating relationships.

Possible values are:

* `string` -- arbitrary strings for identifying nodes.
* `integer` -- arbitrary integer values for identifying nodes.
* `actual` -- (advanced) actual node IDs.
|string
| {check-mark}
| {check-mark}

|--ignore-empty-strings[=true\|false]
|Whether or not empty string fields, i.e. "" from input source are ignored, i.e. treated as null.
|false
| {check-mark}
| {check-mark}

|--ignore-extra-columns[=true\|false]
|If unspecified columns should be ignored during the import.
|false
| {check-mark}
|

|--input-encoding=<character-set>
|Character set that input data is encoded in.
|UTF-8
| {check-mark}
|

|--input-type=csv\|parquet
|File type to import from. Can be csv or parquet. Defaults to csv.
|
| {check-mark}
| {check-mark}

|--legacy-style-quoting[=true\|false]
|Whether or not a backslash-escaped quote e.g. \" is interpreted as an inner quote.
|false
| {check-mark}
| {check-mark}

|--max-off-heap-memory=<size>
|Maximum memory that `neo4j-admin` can use for various data structures and caching to improve performance.

Values can be plain numbers, such as `10000000`, or `20G` for 20 gigabytes.
It can also be specified as a percentage of the available memory, for example `70%`.
|90%
| {check-mark}
| {check-mark}

|--multiline-fields=true\|false\|<path>[,<path>]
|label:changed[Changed in 5.26] In v1, whether or not fields from an input source can span multiple lines, i.e. contain newline characters. Setting `--multiline-fields=true` can severely degrade the performance of the importer. Therefore, use it with care, especially with large imports. In v2, this option will specify the list of files that contain multiline fields. Files can also be specified using regular expressions.
|null
| {check-mark}
|

|--multiline-fields-format=v1\|v2
|Controls the parsing of input source that can span multiple lines, i.e. contain newline characters. When set to v1, the value for `--multiline-fields` can only be true or false. When set to v2, the value for `--multiline-fields` should be the list of files that contain multiline fields.
|null
| {check-mark}
|

|--nodes=[<label>[:<label>]...=]<files>...
|Node CSV header and data.

* Multiple files will be logically seen as one big file from the perspective of the importer.
* The first line must contain the header.
* Multiple data sources like these can be specified in one import, where each data source has its own header.
* Files can also be specified using regular expressions.

It is possible to import files from AWS S3 buckets, Google Cloud storage buckets, and Azure buckets using the appropriate URI as the path.

For an example, see <<import-tool-multiple-input-files-regex-example>>.
|
| {check-mark}
| {check-mark}

|--normalize-types[=true\|false]
|When `true`, non-array property values are converted to their equivalent Cypher types. For example, all integer values will be converted to 64-bit long integers.
|true
| {check-mark}
| {check-mark}

|--overwrite-destination[=true\|false]
|Delete any existing database files prior to the import.
|false
| {check-mark}
| {check-mark}

|--quote=<char>
|Character to treat as quotation character for values in CSV data.

Quotes can be escaped as per link:{rfc-4180}[RFC 4180] by doubling them, for example `""` would be interpreted as a literal `"`.

You cannot escape using `\`.
|"
| {check-mark}
|

|--read-buffer-size=<size>
|Size of each buffer for reading input data.

It has to be at least large enough to hold the biggest single value in the input data.
The value can be a plain number or a byte units string, e.g. `128k`, `1m`.
|4194304
| {check-mark}
| {check-mark}

|--relationships=[<type>=]<files>...
|Relationship CSV header and data.

* Multiple files will be logically seen as one big file from the perspective of the importer.
* The first line must contain the header.
* Multiple data sources like these can be specified in one import, where each data source has its own header.
* Files can also be specified using regular expressions.

It is possible to import files from AWS S3 buckets, Google Cloud storage buckets, and Azure buckets using the appropriate URI as the path.

For an example, see <<import-tool-multiple-input-files-regex-example>>.
|
| {check-mark}
| {check-mark}

|--report-file=<path>
|File in which to store the report of the csv-import.

The location of the import log file can be controlled using the `--report-file` option.
If you run large imports of CSV files that have low data quality, the import log file can grow very large.
For example, CSV files that contain duplicate node IDs, or that attempt to create relationships between non-existent nodes, could be classed as having low data quality.
In these cases, you may wish to direct the output to a location that can handle the large log file.

If you are running on a UNIX-like system and you are not interested in the output, you can get rid of it altogether by directing the report file to `/dev/null`.

If you need to debug the import, it might be useful to collect the stack trace.
This is done by using the `--verbose` option.
|import.report
| {check-mark}
| {check-mark}

|--schema=<path>
|label:enterprise[Enterprise edition] Path to the file containing the Cypher commands for creating indexes and constraints during data import.
|
| {check-mark}
| {check-mark}

|--skip-bad-entries-logging[=true\|false]
|When set to `true`, the details of bad entries are not written in the log. Disabling logging can improve performance when the data contains lots of faults. Cleaning the data before importing it is highly recommended because faults dramatically affect the tool's performance even without logging.
|false
| {check-mark}
| {check-mark}

|--skip-bad-relationships[=true\|false]
|Whether or not to skip importing relationships that refer to missing node IDs, i.e. either start or end node ID/group referring to a node that was not specified by the node input data.

Skipped relationships will be logged if they are within the limit of entities specified by `--bad-tolerance` and the `--skip-bad-entries-logging` option is disabled.
|false
| {check-mark}
| {check-mark}

|--skip-duplicate-nodes[=true\|false]
|Whether or not to skip importing nodes that have the same ID/group.

In the event of multiple nodes within the same group having the same ID, the first encountered will be imported, whereas consecutive such nodes will be skipped.

Skipped nodes will be logged if they are within the limit of entities specified by `--bad-tolerance` and the `--skip-bad-entries-logging` option is disabled.
|false
| {check-mark}
| {check-mark}

|--strict[=true\|false]
| Whether or not the lookup of nodes referred to from relationships needs to be checked strict.
If disabled, most but not all relationships referring to non-existent nodes will be detected.
If enabled all those relationships will be found but at the cost of lower performance.
|false label:changed[Changed in 5.8]
| {check-mark}
| {check-mark}

|--threads=<num>
| (advanced) Max number of worker threads used by the importer. Defaults to the number of available processors reported by the JVM. There is a certain amount of minimum threads needed so for that reason there is no lower bound for this value. For optimal
performance, this value should not be greater than the number of available processors.
|20
| {check-mark}
| {check-mark}

|--trim-strings[=true\|false]
|Whether or not strings should be trimmed for whitespaces.
|false
| {check-mark}
|

|--verbose
|Enable verbose output.
|
| {check-mark}
| {check-mark}
|===

[NOTE]
.Heap size for the import
====
You want to set the maximum heap size to a relevant value for the import.
This is done by defining the `HEAP_SIZE` environment parameter before starting the import.
For example, 2G is an appropriate value for smaller imports.

If doing imports in the order of magnitude of 100 billion entities, 20G will be an appropriate value.
====


[NOTE]
.Record format
====
If your import data results in a graph that is larger than 34 billion nodes, 34 billion relationships, or 68 billion properties, you will need to configure the importer to use the `block` format.
This is achieved by using the `format` option of the import command and setting the value to `block`:

[source, shell]
--
bin/neo4j-admin database import full --format=block
--

The `block` format is available in Enterprise Edition only.
====

[NOTE]
.Providing arguments in a file
====
All options can be provided in a file and passed to the command using the `@` prefix.
This is useful when the command line becomes too long to manage.
For example, the following command:

[source, shell]
----
bin/neo4j-admin database import full @/path/to/your/<args-filename> mydb
----
For more information, see link:https://picocli.info/#AtFiles[Picocli -> AtFiles] official documentation.
====

[NOTE]
.Using both a multi-value option and a positional parameter
====
When using both a multi-value option, such as `--nodes` and `--relationships`, and a positional parameter (for example, in `--additional-config neo4j.properties --nodes 0-nodes.csv mydatabase`), the `--nodes` option acts "greedy" and the next option, in this case, `mydatabase`,  is pulled in via the nodes convertor.

This is a limitation of the underlying library, Picocli, and is not specific to Neo4j Admin.
For more information, see link:https://picocli.info/#_variable_arity_options_and_positional_parameters[Picocli -> Variable Arity Options and Positional Parameters] official documentation.

To resolve the problem, use one of the following solutions:

* Put the positional parameters first.
For example, `mydatabase --nodes 0-nodes.csv`.
* Put the positional parameters last, after `--` and the final value of the last multi-value option.
For example, `nodes 0-nodes.csv -- mydatabase`.
====

[NOTE]
.Importing from a cloud storage
====
The `--nodes` and `--relationships` options can also import files from AWS S3 buckets, Google Cloud storage buckets, and Azure buckets.
For more information, see <<import-files-from-cloud-storage>>.
====


[[import-tool-examples]]
=== Examples

[NOTE]
====
If importing to a database that has not explicitly been created before the import, it must be created subsequently in order to be used.
====

==== Import data from CSV files

Assume that you have formatted your data as per xref:tools/neo4j-admin/neo4j-admin-import.adoc#import-tool-header-format[CSV header format] so that you have it in six different files:

. `movies_header.csv`
. `movies.csv`
. `actors_header.csv`
. `actors.csv`
. `roles_header.csv`
. `roles.csv`

The following command imports the three datasets:

[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/movies_header.csv,import/movies.csv \
--nodes import/actors_header.csv,import/actors.csv \
--relationships import/roles_header.csv,import/roles.csv
----

[[import-tool-multiple-input-files-regex-example]]
==== Import data from CSV files using regular expression

Assume that you want to include a header and then multiple files that match a pattern, e.g. containing numbers.
In this case, a regular expression can be used.
It is guaranteed that groups of digits will be sorted in numerical order, as opposed to lexicograghic order.

For example:

[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/node_header.csv,import/node_data_\d+\.csv
----

==== Import data from CSV files using a more complex regular expression

For regular expression patterns containing commas, which is also the delimiter between files in a group, the pattern can be quoted to preserve the pattern.

For example:

[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/node_header.csv,'import/node_data_\d{1,5}.csv' databasename
----

[[import-files-from-cloud-storage]]
==== Importing files from a cloud storage

In Neo4j 2025.03, new cloud integration settings are introduced to provide better support for deployment and management in cloud ecosystems.
For details, refer to xref:configuration/configuration-settings.adoc#_cloud_storage_integration_settings[Configuration settings -> Cloud storage integration settings].

The following examples show how to import data stored in a cloud storage bucket using the `--nodes` and `--relationships` options.

[.tabbed-example]
=====
[role=include-with-AWS-S3]
======

include::partial$/aws-s3-overrides.adoc[]

include::partial$/aws-s3-credentials.adoc[]

. Run the `neo4j-admin database import` command to import your data from your AWS S3 storage bucket.
The example assumes that you have data stored in the `myBucket/data` folder in your bucket.
+
[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes s3://myBucket/data/nodes.csv --relationships s3://myBucket/data/relationships.csv newdb
----
======

[role=include-with-Google-cloud-storage]
======

include::partial$/gcs-credentials.adoc[]

. Run the `neo4j-admin database import` command to import your data from your Google storage bucket.
The example assumes that you have data stored in the `myBucket/data` folder in your bucket.
+
[source,shell]
----
bin/neo4j-admin database import full --nodes gs://myBucket/data/nodes.csv --relationships gs://myBucket/data/relationships.csv newdb
----
======

[role=include-with-Azure-cloud-storage]
======

include::partial$/azb-credentials.adoc[]

. Run the `neo4j-admin database import` command to import your data from your Azure blob storage container.
The example assumes that you have data stored in the `myStorageAccount/myContainer/data` folder in your container.
+
[source,shell]
----
bin/neo4j-admin database import full --nodes azb://myStorageAccount/myContainer/data/nodes.csv --relationships azb://myStorageAccount/myContainer/data/relationships.csv newdb
----
======
=====

[role=label--enterprise-edition]
[[import-tool-incremental]]
== Incremental import

[NOTE]
====
Incremental import supports `block` format.
====

Incremental import allows you to incorporate large amounts of data in batches into the graph.
You can run this operation as part of the initial data load when it cannot be completed in a single full import.
Besides, you can update your graph by importing data incrementally, which is more performant than transactional insertion of such data.

Incremental import requires the use of `--force` and can be run on an existing database only.

You must stop your database, if you want to perform the incremental import within one command.

If you cannot afford a full downtime of your database, split the operation into several stages:

* _prepare_ stage (offline)
* _build_ stage (offline or read-only)
* _merge_ stage (offline)

The database must be stopped for the `prepare` and `merge` stages.
During the `build` stage, the database can be left online but put into read-only mode.
For a detailed example, see <<incremental-import-stages>>.

[WARNING]
====
It is highly recommended to back up your database before running the incremental import, as if the _merge_ stage fails, is aborted, or crashes, it may corrupt the database.
====

[[import-tool-incremental-syntax]]
=== Syntax

The syntax for importing a set of CSV files incrementally is:

[source, syntax, role="nocopy"]
----
neo4j-admin database import incremental [-h] [--expand-commands] --force [--update-all-matching-relationships]
                                        [--verbose] [--auto-skip-subsequent-headers[=true|false]]
                                        [--ignore-empty-strings[=true|false]] [--ignore-extra-columns[=true|false]]
                                        [--legacy-style-quoting[=true|false]] [--normalize-types[=true|false]]
                                        [--skip-bad-entries-logging[=true|false]] [--skip-bad-relationships
                                        [=true|false]] [--skip-duplicate-nodes[=true|false]] [--strict[=true|false]]
                                        [--trim-strings[=true|false]] [--additional-config=<file>]
                                        [--array-delimiter=<char>] [--bad-tolerance=<num>] [--delimiter=<char>]
                                        [--high-parallel-io=on|off|auto] [--id-type=string|integer|actual]
                                        [--input-encoding=<character-set>] [--input-type=csv|parquet]
                                        [--max-off-heap-memory=<size>] [--quote=<char>] [--read-buffer-size=<size>]
                                        [--report-file=<path>] [--schema=<path>] [--stage=all|prepare|build|merge]
                                        [--threads=<num>] --nodes=[<label>[:<label>]...=]<files>... [--nodes=[<label>[:
                                        <label>]...=]<files>...]... [--relationships=[<type>=]<files>...]...
                                        [--multiline-fields=true|false|<path>[,<path>]
                                        [--multiline-fields-format=v1|v2]] <database>
----

=== Description

Incremental import into an existing database.

=== Usage and limitations

[WARNING]
====
The importer works well on standalone servers.

In clustering environments with multiple copies of the database, the updated database must be used as a source to reseed the rest of the database copies.
You can use the procedure xref:procedures.adoc#procedure_dbms_cluster_recreateDatabase[`dbms.cluster.recreateDatabase()`].
For details, see xref:clustering/databases.adoc#recreate-databases[Recreate databases].

Starting the clustered database after an incremental import without reseeding or performing the incremental import on a single server while the database remains online on other clustered members may result in unpredictable consequences, including data inconsistency between cluster members.
====

The incremental import command can be used to add:

* New nodes with labels and properties.
+
[WARNING]
====
Note that you must have node property uniqueness constraints in place for the property key and label combinations that form the primary key, or the uniquely identifiable nodes.
Otherwise, the command will throw an error and exit.
For more information, see <<import-tool-header-format>>.
====
* New relationships between existing or new nodes.

Starting from 2025.01, the incremental import command can also be used for:

* Adding new properties to existing nodes or relationships.
* Updating or deleting properties in nodes or relationships.
* Updating or deleting labels in nodes.
* Deleting existing nodes and relationships.

This is supported only by `block` format.
See <<#_applying_changes_to_data_via_csv_files>> for more information.

=== Parameters

.`neo4j-admin database import incremental` parameters
[options="header", cols="5m,10a,2m"]
|===
| Parameter
| Description
| Default

|<database>
|Name of the database to import.
If the database into which you import does not exist prior to importing, you must create it subsequently using `CREATE DATABASE`.
|neo4j
|===

=== Options

[[incremental-import-options-table]]
.`neo4j-admin database import incremental` options
[options="header", cols="4m,6a,2m,1,2"]
|===
| Option
| Description
| Default
| CSV
| Parquet

|--additional-config=<file>footnote:[See xref:tools/neo4j-admin/index.adoc#_configuration[Tools -> Configuration] for details.]
|Configuration file with additional configuration.
|
| {check-mark}
| {check-mark}

|--array-delimiter=<char>
|Delimiter character between array elements within a value in CSV data. Also accepts `TAB` and e.g. `U+20AC` for specifying a character using Unicode.

====
* ASCII character -- e.g. `--array-delimiter=";"`.
* `\ID` -- Unicode character with ID, e.g. `--array-delimiter="\59"`.
* `U+XXXX` -- Unicode character specified with 4 HEX characters, e.g. `--array-delimiter="U+20AC"`.
* `\t` -- horizontal tabulation (HT), e.g. `--array-delimiter="\t"`.
====

For horizontal tabulation (HT), use `\t` or the Unicode character ID `\9`.

Unicode character ID can be used if prepended by `\`.
|;
| {check-mark}
| {check-mark}label:new[Available from 2025.02]

| --auto-skip-subsequent-headers[=true\|false]
|Automatically skip accidental header lines in subsequent files in file groups with more than one file.
|false
| {check-mark}
|

|--bad-tolerance=<num>
|Number of bad entries before the import is aborted. The import process is optimized for error-free data. Therefore, cleaning the data before importing it is highly recommended. If you encounter any bad entries during the import process, you can set the number of bad entries to a specific value that suits your needs. However, setting a high value may affect the performance of the tool.
|1000
| {check-mark}
| {check-mark}

|--delimiter=<char>
|Delimiter character between values in CSV data. Also accepts `TAB` and e.g. `U+20AC` for specifying a character using Unicode.

====
* ASCII character -- e.g. `--delimiter=","`.
* `\ID` -- Unicode character with ID, e.g. `--delimiter="\44"`.
* `U+XXXX` -- Unicode character specified with 4 HEX characters, e.g. `--delimiter="U+20AC"`.
* `\t` -- horizontal tabulation (HT), e.g. `--delimiter="\t"`.
====

For horizontal tabulation (HT), use `\t` or the Unicode character ID `\9`.

Unicode character ID can be used if prepended by `\`.
|,
| {check-mark}
|

|--expand-commands
|Allow command expansion in config value evaluation.
|
| {check-mark}
| {check-mark}

|--force
|Confirm incremental import by setting this flag.
|
| {check-mark}
| {check-mark}

|-h, --help
|Show this help message and exit.
|
| {check-mark}
| {check-mark}

|--high-parallel-io=on\|off\|auto
|Ignore environment-based heuristics and indicate if the target storage subsystem can support parallel IO with high throughput or auto detect.
Typically this is `on` for SSDs, large raid arrays, and network-attached storage.
|auto
| {check-mark}
| {check-mark}

|--id-type=string\|integer\|actual
|Each node must provide a unique ID.
This is used to find the correct nodes when creating relationships.

Possible values are:

* `string` -- arbitrary strings for identifying nodes.
* `integer` -- arbitrary integer values for identifying nodes.
* `actual` -- (advanced) actual node IDs.
|string
| {check-mark}
| {check-mark}

|--ignore-empty-strings[=true\|false]
|Whether or not empty string fields, i.e. "" from input source are ignored, i.e. treated as null.
|false
| {check-mark}
| {check-mark}

|--ignore-extra-columns[=true\|false]
|If unspecified columns should be ignored during the import.
|false
| {check-mark}
|

|--input-encoding=<character-set>
|Character set that input data is encoded in.
|UTF-8
| {check-mark}
|

|--input-type=csv\|parquet
|File type to import from. Can be csv or parquet. Defaults to csv.
|
| {check-mark}
| {check-mark}

|--legacy-style-quoting[=true\|false]
|Whether or not a backslash-escaped quote e.g. \" is interpreted as an inner quote.
|false
| {check-mark}
| {check-mark}

|--max-off-heap-memory=<size>
|Maximum memory that `neo4j-admin` can use for various data structures and caching to improve performance.

Values can be plain numbers, such as `10000000`, or `20G` for 20 gigabytes.
It can also be specified as a percentage of the available memory, for example `70%`.
|90%
| {check-mark}
| {check-mark}

|--multiline-fields=true\|false\|<path>[,<path>]
|label:changed[Changed in 5.26] In v1, whether or not fields from an input source can span multiple lines, i.e. contain newline characters. Setting `--multiline-fields=true` can severely degrade the performance of the importer. Therefore, use it with care, especially with large imports. In v2, this option will specify the list of files that contain multiline fields. Files can also be specified using regular expressions.
|null
| {check-mark}
|

|--multiline-fields-format=v1\|v2
|Controls the parsing of input source that can span multiple lines, i.e. contain newline characters. When set to v1, the value for `--multiline-fields` can only be true or false. When set to v2, the value for `--multiline-fields` should be the list of files that contain multiline fields.
|null
| {check-mark}
|

|--nodes=[<label>[:<label>]...=]<files>...
|Node CSV header and data.

* Multiple files will be logically seen as one big file from the perspective of the importer.
* The first line must contain the header.
* Multiple data sources like these can be specified in one import, where each data source has its own header.
* Files can also be specified using regular expressions.

It is possible to import files from AWS S3 buckets, Google Cloud storage buckets, and Azure buckets using the appropriate URI as the path.

For an example, see <<import-tool-multiple-input-files-regex-example>>.
|
| {check-mark}
| {check-mark}

|--normalize-types[=true\|false]
|When `true`, non-array property values are converted to their equivalent Cypher types. For example, all integer values will be converted to 64-bit long integers.
| true
| {check-mark}
| {check-mark}

|--quote=<char>
|Character to treat as quotation character for values in CSV data.

Quotes can be escaped as per link:{rfc-4180}[RFC 4180] by doubling them, for example `""` would be interpreted as a literal `"`.

You cannot escape using `\`.
|"
| {check-mark}
|

|--read-buffer-size=<size>
|Size of each buffer for reading input data.

It has to be at least large enough to hold the biggest single value in the input data.
The value can be a plain number or a byte units string, e.g. `128k`, `1m`.
|4194304
| {check-mark}
| {check-mark}

|--relationships=[<type>=]<files>...
|Relationship CSV header and data.

* Multiple files will be logically seen as one big file from the perspective of the importer.
* The first line must contain the header.
* Multiple data sources like these can be specified in one import, where each data source has its own header.
* Files can also be specified using regular expressions.

It is possible to import files from AWS S3 buckets, Google Cloud storage buckets, and Azure buckets using the appropriate URI as the path.

For an example, see <<import-tool-multiple-input-files-regex-example>>.
|
| {check-mark}
| {check-mark}

|--report-file=<path>
|File in which to store the report of the csv-import.

The location of the import log file can be controlled using the `--report-file` option.
If you run large imports of CSV files that have low data quality, the import log file can grow very large.
For example, CSV files that contain duplicate node IDs, or that attempt to create relationships between non-existent nodes, could be classed as having low data quality.
In these cases, you may wish to direct the output to a location that can handle the large log file.

If you are running on a UNIX-like system and you are not interested in the output, you can get rid of it altogether by directing the report file to `/dev/null`.

If you need to debug the import, it might be useful to collect the stack trace.
This is done by using the `--verbose` option.
|import.report
| {check-mark}
| {check-mark}

|--schema=<path> label:new[Available from 2025.02]
|Path to the file containing the Cypher commands for creating indexes and constraints during data import.
|
| {check-mark}
| {check-mark}

|--skip-bad-entries-logging[=true\|false]
|When set to `true`, the details of bad entries are not written in the log. Disabling logging can improve performance when the data contains lots of faults. Cleaning the data before importing it is highly recommended because faults dramatically affect the tool's performance even without logging.
|false
| {check-mark}
| {check-mark}

|--skip-bad-relationships[=true\|false]
|Whether or not to skip importing relationships that refer to missing node IDs, i.e. either start or end node ID/group referring to a node that was not specified by the node input data.

Skipped relationships will be logged if they are within the limit of entities specified by `--bad-tolerance` and the `--skip-bad-entries-logging` option is disabled.
|false
| {check-mark}
| {check-mark}

|--skip-duplicate-nodes[=true\|false]
|Whether or not to skip importing nodes that have the same ID/group.

In the event of multiple nodes within the same group having the same ID, the first encountered will be imported, whereas consecutive such nodes will be skipped.

Skipped nodes will be logged if they are within the limit of entities specified by `--bad-tolerance` and the `--skip-bad-entries-logging` option is disabled.
|false
| {check-mark}
| {check-mark}

|--stage=all\|prepare\|build\|merge
|Stage of incremental import.

For incremental import into an existing database use `all` (which requires the database to be stopped).

For semi-online incremental import run `prepare` (on a stopped database) followed by `build` (on a potentially running database) and finally `merge` (on a stopped database).
|all
| {check-mark}
| {check-mark}

|--strict[=true\|false]
| Whether or not the lookup of nodes referred to from relationships needs to be checked strict.
If disabled, most but not all relationships referring to non-existent nodes will be detected.
If enabled all those relationships will be found but at the cost of lower performance.
|false label:changed[Changed in 5.8]
| {check-mark}
| {check-mark}

|--threads=<num>
| (advanced) Max number of worker threads used by the importer. Defaults to the number of available processors reported by the JVM. There is a certain amount of minimum threads needed so for that reason there is no lower bound for this value. For optimal performance, this value should not be greater than the number of available processors.
|20
| {check-mark}
| {check-mark}

|--trim-strings[=true\|false]
|Whether or not strings should be trimmed for whitespaces.
|false
| {check-mark}
|

|--update-all-matching-relationships
|label:new[Introduced in 2025.01] Whether or not to update all existing relationships that match a relationship data entry. If disabled, the relationship data entry will be logged if it is within the limit of entities specified by `--bad-tolerance` and the `--skip-bad-entries-logging` option is disabled.
|false
| {check-mark}
| {check-mark}

|--verbose
|Enable verbose output.
|
| {check-mark}
| {check-mark}
|===

[NOTE]
.Using both a multi-value option and a positional parameter
====
When using both a multi-value option, such as `--nodes` and `--relationships`, and a positional parameter (for example, in `--additional-config neo4j.properties --nodes 0-nodes.csv mydatabase`), the `--nodes` option acts "greedy" and the next option, in this case, `mydatabase`,  is pulled in via the nodes convertor.

This is a limitation of the underlying library, Picocli, and is not specific to Neo4j Admin.
For more information, see link:https://picocli.info/#_variable_arity_options_and_positional_parameters[Picocli -> Variable Arity Options and Positional Parameters] official documentation.

To resolve the problem, use one of the following solutions:

* Put the positional parameters first.
For example, `mydatabase --nodes 0-nodes.csv`.
* Put the positional parameters last, after `--` and the final value of the last multi-value option.
For example, `nodes 0-nodes.csv -- mydatabase`.
====

[[import-tool-incremental-examples]]
=== Examples

There are two ways of importing data incrementally.

==== Incremental import in a single command

If downtime is not a concern, you can run a single command with the option `--stage=all`.
This option requires the database to be stopped.

====
[source, shell, role=noplay]
----
neo4j@system> STOP DATABASE db1 WAIT;
...
bin/neo4j-admin database import incremental --stage=all --nodes=N1=../../raw-data/incremental-import/b.csv db1
----
====

[[incremental-import-stages]]
==== Incremental import in stages

If you cannot afford a full downtime of your database, you can run the import in three stages.

====
. `prepare` stage:
+
During this stage, the import tool analyzes the CSV headers and copies the relevant data over to the new increment database path.
The import command is run with the option `--stage=prepare` and the database must be stopped.
+
.. Using the `system` database, stop the database `db1` with the `WAIT` option to ensure a checkpoint happens before you run the incremental import command.
The database must be stopped to run `--stage=prepare`.
+
[source, shell, role=noplay]
----
STOP DATABASE db1 WAIT
----
.. Run the incremental import command with the `--stage=prepare` option:
+
[source, shell, role=noplay]
----
bin/neo4j-admin database import incremental --stage=prepare --nodes=N1=../../raw-data/incremental-import/c.csv db1
----
. `build` stage:
+
During this stage, the import tool imports the data, deduplicates it, and validates it in the new increment database path.
This is the longest stage and you can put the database in read-only mode to allow read access.
The import command is run with the option `--stage=build`.
+
.. Put the database in read-only mode:
+
[source, shell, role=noplay]
----
ALTER DATABASE db1 SET ACCESS READ ONLY
----
.. Run the incremental import command with the `--stage=build` option:
+
[source, shell, role=noplay]
----
bin/neo4j-admin database import incremental --stage=build --nodes=N1=../../raw-data/incremental-import/c.csv db1
----
. `merge` stage:
+
During this stage, the import tool merges the new with the existing data in the database.
It also updates the affected indexes and upholds the affected property uniqueness constraints and property existence constraints.
The import command is run with the option `--stage=merge` and the database must be stopped.
It is not necessary to include the `--nodes` or `--relationships` options when using `--stage=merge`.
+
.. Using the `system` database, stop the database `db1` with the `WAIT` option to ensure a checkpoint happens before you run the incremental import command.
+
[source, shell, role=noplay]
----
STOP DATABASE db1 WAIT
----
.. Run the incremental import command with the `--stage=merge` option:
+
[source, shell, role=noplay]
----
bin/neo4j-admin database import incremental --stage=merge db1
----
====

[[import-tool-header-format]]
== CSV header format

The header file of each data source specifies how the data fields should be interpreted.
You must use the same delimiter for the header file and the data files.

The header contains information for each field, with the format `<name>:<field_type>`.
The `<name>` is used for properties and node IDs.
In all other cases, the `<name>` part of the field is ignored.

.Incremental import
[NOTE]
====
When using <<import-tool-incremental, incremental import>>, you must have node property uniqueness constraints in place for the property key and label combinations that form the primary key, or the uniquely identifiable nodes.
For example, importing nodes with a `Person` label that are uniquely identified with a `uuid` property key, the format of the header should be `uuid:ID{label:Person}`.

This is also true when working with multiple groups.
For example, you can use `uuid:ID(Person){label:Person}`, where the relationship CSV data can refer to different groups for its `:START_ID` and `:END_ID`, just like the full import method.

* For more information on constraints, see link:{neo4j-docs-base-uri}/cypher-manual/current/constraints[Cypher Manual -> Constraints].
* For examples of creating property uniqueness constraints, see link:{neo4j-docs-base-uri}/cypher-manual/current/constraints/managing-constraints/#create-property-uniqueness-constraints[Cypher Manual -> Create property uniqueness constraints].
====

=== Extended header support for Parquet

In addition to the header format supported by the CSV import, the Parquet import supports name-mapping header files.
Those files contain two rows of entries, where the first row represents the name (incl. optional type, id group, etc.), and the second row references the name of the original columns in the data files.

.movie_header.csv
[source, csv]
----
movieId:ID,title,year:int,:LABEL
id,movie_title,year,label
----

If a header file is provided for a set of labels or a relationship type, the importer will ignore columns not mentioned in the headers.

[[import-tool-header-format-nodes]]
== Node files

Files containing node data can have an `ID` field, a `LABEL` field, and properties.

ID::
  Each node must have a unique ID if it is to be connected by any relationships created in the import.
  Neo4j uses the IDs to find the correct nodes when creating relationships.
  Note that the ID has to be unique across all nodes within the group, regardless of their labels.
  The unique ID is persisted in a property whose name is defined by the `<name>` part of the field definition `<name>:ID`.
  If no such property `name` is defined, the unique ID will be used for the import but not be available for reference later.
  If no ID is specified, the node will be imported, but it will not be connected to other nodes during the import.
  When a property `name` is provided, that property type can be configured globally via the `--id-type` option  (as for <<import-tool-header-format-properties>>). +
 You can specify a different value ID type to be stored for a node property in its group using the option `id-type` in the header, e.g: `id:ID(MyGroup){label:MyLabel, id-type: int}`.
  This ID type overrides the global `--id-type` option.
  For example, the global `id-type` can be a string, but the nodes will have their IDs stored as `int` type in their ID properties.
  For more information, see <<import-tool-id-types-header>>. +
  A node header can also contain multiple `ID` columns, where the relationship data references the composite value of all those columns.
  This also implies using `string` as `id-type`.
  For each `ID` column, you can specify to store its values as different node properties.
  However, the composite value cannot be stored as a node property.
  For more information, see <<import-tool-multiple-ids>>.
LABEL::
  Read one or more labels from this field.
  Like array values, multiple labels are separated by `;`, or by the character specified with `--array-delimiter`.
  The max length of label names for block format is 16,383 characters.

.Define node files
====

You define the headers for movies in the _movies_header.csv_ file.
Movies have the properties `movieId`, `year`, and `title`.
You also specify a field for labels.

[source, csv]
----
movieId:ID,title,year:int,:LABEL
----

You define three movies in the _movies.csv_ file.
They contain all the properties defined in the header file.
All the movies are given the label `Movie`.
Two of them are also given the label `Sequel`.

[source, csv]
----
tt0133093,"The Matrix",1999,Movie
tt0234215,"The Matrix Reloaded",2003,Movie;Sequel
tt0242653,"The Matrix Revolutions",2003,Movie;Sequel
----

Similarly, you also define three actors in the _actors_header.csv_ and _actors.csv_ files.
They all have the properties `personId` and `name`, and the label `Actor`.

[source, csv]
----
personId:ID,name,:LABEL
----

[source, csv]
----
keanu,"Keanu Reeves",Actor
laurence,"Laurence Fishburne",Actor
carrieanne,"Carrie-Anne Moss",Actor
----
====


[[import-tool-header-format-rels]]
== Relationship files

Files containing relationship data have three mandatory fields and can also have properties.
The mandatory fields are:

TYPE::
  The relationship type to use for this relationship.
  The max length of relationship type names for block format is 16,383 characters.
START_ID::
  The ID of the start node for this relationship.
END_ID::
  The ID of the end node for this relationship.

The `START_ID` and `END_ID` refer to the unique node ID defined in one of the node data sources, as explained in the previous section.
None of these take a name, e.g. if `<name>:START_ID` or `<name>:END_ID` is defined, the `<name>` part will be ignored.
Nor do they take a `<field_type>`, e.g. if `:START_ID:int` or `:END_ID:int` is defined, the `:int` part does not have any meaning in the context of type information.


.Define relationships files
====

In this example, you assume that the two node files from the previous example are used together with the following relationships file.

You define relationships between actors and movies in the files _roles_header.csv_ and _roles.csv_.
Each row connects a start node and an end node with a relationship of relationship type `ACTED_IN`.
Notice how you use the unique identifiers `personId` and `movieId` from the nodes files above.
The name of the character that the actor is playing in this movie is stored as a `role` property on the relationship.

[source, csv]
----
:START_ID,role,:END_ID,:TYPE
----

[source, csv]
----
keanu,"Neo",tt0133093,ACTED_IN
keanu,"Neo",tt0234215,ACTED_IN
keanu,"Neo",tt0242653,ACTED_IN
laurence,"Morpheus",tt0133093,ACTED_IN
laurence,"Morpheus",tt0234215,ACTED_IN
laurence,"Morpheus",tt0242653,ACTED_IN
carrieanne,"Trinity",tt0133093,ACTED_IN
carrieanne,"Trinity",tt0234215,ACTED_IN
carrieanne,"Trinity",tt0242653,ACTED_IN
----
====


[[import-tool-header-format-properties]]
== Property data types

For properties, the `<name>` part of the field designates the property key, while the `<field_type>` part assigns a data type.
You can have properties in both node data files and relationship data files.
The max length of property keys for block format is 16,383 characters.

Use one of `int`, `long`, `float`, `double`, `boolean`, `byte`, `short`, `char`, `string`, `point`, `date`, `localtime`, `time`, `localdatetime`, `datetime`, and `duration` to designate the data type for properties.
By default, types (except arrays) are converted to Cypher types.
See link:{neo4j-docs-base-uri}/cypher-manual/current/values-and-types/property-structural-constructed/#_property_types[Cypher Manual -> Property, structural, and constructed values].

This behavior can be disabled using the option `--normalize-types=false`.
Normalizing types can require more space on disk, but avoids Cypher converting the type during queries.
If no data type is given, this defaults to `string`.

To define an array type, append `[]` to the type.
By default, array values are separated by `;`.
A different delimiter can be specified with `--array-delimiter`.
Arrays are not affected by the `--normalize-types` flag.
For example, if you want a byte array to be stored as a Cypher long array, you must explicitly declare the property as `long[]`.

Boolean values are _true_ if they match exactly the text `true`. All other values are _false_.
Values that contain the delimiter character need to be escaped by enclosing in double quotation marks, or by using a different delimiter character with the `--delimiter` option.

.Header format with data types
====

This example illustrates several different data types specified in the CSV header.

[source, csv]
----
:ID,name,joined:date,active:boolean,points:int
user01,Joe Soap,2017-05-05,true,10
user02,Jane Doe,2017-08-21,true,15
user03,Moe Know,2018-02-17,false,7
----

====

Special considerations for the `point` data type::
A point is specified using the Cypher syntax for maps.
The map allows the same keys as the input to the link:{neo4j-docs-base-uri}/cypher-manual/current/functions/spatial/[Cypher Manual -> Point function].
The point data type in the header can be amended with a map of default values used for all values of that column, e.g. `point{crs: 'WGS-84'}`.
Specifying the header this way allows you to have an incomplete map in the value position in the data file.
Optionally, a value in a data file may override default values from the header.
+
.Property format for `point` data type
====

This example illustrates various ways of using the `point` data type in the import header and the data files.

You are going to import the name and location coordinates for cities.
First, you define the header as:

[source, csv]
----
:ID,name,location:point{crs:WGS-84}
----

You then define cities in the data file.

* The first city's location is defined using `latitude` and `longitude`, as expected when using the coordinate system defined in the header.
* The second city uses `x` and `y` instead.
This would normally lead to a point using the coordinate reference system `cartesian`.
Since the header defines `crs:WGS-84`, that coordinate reference system will be used.
* The third city overrides the coordinate reference system defined in the header and sets it explicitly to `WGS-84-3D`.

[source, csv]
----
:ID,name,location:point{crs:WGS-84}
city01,"Malm","{latitude:55.6121514, longitude:12.9950357}"
city02,"London","{y:51.507222, x:-0.1275}"
city03,"San Mateo","{latitude:37.554167, longitude:-122.313056, height: 100, crs:'WGS-84-3D'}"
----

Note that all point maps are within double quotation marks `"` in order to prevent the enclosed `,` character from being interpreted as a column separator.
An alternative approach would be to use `--delimiter='\t'` and reformat the file with tab separators, in which case the `"` characters are not required.

[source, csv]
----
:ID name    location:point{crs:WGS-84}
city01  Malm   {latitude:55.6121514, longitude:12.9950357}
city02  London  {y:51.507222, x:-0.1275}
city03  San Mateo   {latitude:37.554167, longitude:-122.313056, height: 100, crs:'WGS-84-3D'}
----

====

Special considerations for temporal data types::
The format for all temporal data types must be defined as described in link:{neo4j-docs-base-uri}/cypher-manual/current/values-and-types/temporal/#cypher-temporal-instants[Cypher Manual -> Temporal instants syntax] and link:{neo4j-docs-base-uri}/cypher-manual/current/values-and-types/temporal/#cypher-temporal-durations[Cypher Manual -> Durations syntax].
Two of the temporal types, _Time_ and _DateTime_, take a time zone parameter that might be common between all or many of the values in the data file.
It is therefore possible to specify a default time zone for _Time_ and _DateTime_ values in the header, for example: `time{timezone:+02:00}` and: `datetime{timezone:Europe/Stockholm}`.
If no default time zone is specified, the default timezone is determined by the xref:/configuration/configuration-settings.adoc#config_db.temporal.timezone[`db.temporal.timezone`] configuration setting.
The default time zone can be explicitly overridden in the values in the data file.
+
.Property format for temporal data types
====

This example illustrates various ways of using the `datetime` data type in the import header and the data files.

First, you define the header with two _DateTime_ columns.
The first one defines a time zone, but the second one does not:

[source, csv]
----
:ID,date1:datetime{timezone:Europe/Stockholm},date2:datetime
----

You then define dates in the data file.

* The first row has two values that do not specify an explicit timezone.
The value for `date1` will use the `Europe/Stockholm` time zone that was specified for that field in the header.
The value for `date2` will use the configured default time zone of the database.
* In the second row, both `date1` and `date2` set the time zone explicitly to be `Europe/Berlin`.
This overrides the header definition for `date1`, as well as the configured default time zone of the database.

[source, csv]
----
1,2018-05-10T10:30,2018-05-10T12:30
2,2018-05-10T10:30[Europe/Berlin],2018-05-10T12:30[Europe/Berlin]
----

====


[[import-tool-id-spaces]]
== Using ID spaces

By default, the import tool assumes that node identifiers are unique across node files.
In many cases, the ID is unique only across each entity file, for example, when your CSV files contain data extracted from a relational database and the ID field is pulled from the primary key column in the corresponding table.
To handle this situation you define _ID spaces_.
ID spaces are defined in the `ID` field of node files using the syntax `ID(<ID space identifier>)`.
To reference an ID of an ID space in a relationship file, you use the syntax `START_ID(<ID space identifier>)` and `END_ID(<ID space identifier>)`.

.Define and use ID spaces
====

Define a `Movie-ID` ID space in the _movies_header.csv_ file.

[source, csv]
----
movieId:ID(Movie-ID),title,year:int,:LABEL
----

[source, csv]
----
1,"The Matrix",1999,Movie
2,"The Matrix Reloaded",2003,Movie;Sequel
3,"The Matrix Revolutions",2003,Movie;Sequel
----

Define an `Actor-ID` ID space in the header of the _actors_header.csv_ file.

[source, csv]
----
personId:ID(Actor-ID),name,:LABEL
----

[source, csv]
----
1,"Keanu Reeves",Actor
2,"Laurence Fishburne",Actor
3,"Carrie-Anne Moss",Actor
----

Now use the previously defined ID spaces when connecting the actors to movies.

[source, csv]
----
:START_ID(Actor-ID),role,:END_ID(Movie-ID),:TYPE
----

[source, csv]
----
1,"Neo",1,ACTED_IN
1,"Neo",2,ACTED_IN
1,"Neo",3,ACTED_IN
2,"Morpheus",1,ACTED_IN
2,"Morpheus",2,ACTED_IN
2,"Morpheus",3,ACTED_IN
3,"Trinity",1,ACTED_IN
3,"Trinity",2,ACTED_IN
3,"Trinity",3,ACTED_IN
----
====

[[import-tool-multiple-ids]]
== Using multiple node IDs

A node header can also contain multiple `ID` columns, where the relationship data references the composite value of all those columns.
This also implies using `string` as `id-type`.

For each `ID` column, you can specify to store its values as different node properties.
However, the composite value cannot be stored as a node property.

[NOTE]
====
Incremental import doesn't support the use of multiple node identifiers.
This functionality is only available with a full import.
====


.Define multiple IDs as node properties
====
You can define multiple `ID` columns in the node header.
For example, you can define a node header with two `ID` columns.

.nodes_header.csv
[source, csv]
----
:ID,:ID,name
----

.nodes.csv
[source, csv]
----
aa,11,John
bb,22,Paul
----

Now use both IDs when defining the relationship:

.relationships_header.csv
[source, csv]
----
:START_ID,:TYPE,:END_ID
----

.relationships.csv
[source, csv]
----
aa11,WORKS_WITH,bb22
----
====

[[multiple-IDs-Id-spaces]]
.Define multiple IDs stored in ID spaces
====

Define a `MyGroup` ID space in the _nodes_header.csv_ file.

.nodes_header.csv
[source, csv]
----
personId:ID(MyGroup),memberId:ID(MyGroup),name
----

.nodes.csv
[source, csv]
----
aa,11,John
bb,22,Paul
----

Now use the defined ID space when connecting John with Paul, and use both IDs in the relationship.

.relationships_header.csv
[source, csv]
----
:START_ID(MyGroup),:TYPE,:END_ID(MyGroup)
----

.relationships.csv
[source, csv]
----
aa11,WORKS_WITH,bb22
----
====

[[import-tool-id-types-header]]
== Storing a different value type for IDs in a group

You can control the ID type of the node property that will be stored by defining the `id-type` option in the header, for example, `:ID{id-type:long}`.
The `id-type` option in the header overrides the global `--id-type` value provided to the command.
This way, you can have property values of different types for different groups of nodes.
For example, the global `id-type` can be a string, but some nodes can have their IDs stored as `long` type in their ID properties.

.Import nodes with different ID value types
====
.persons_header.csv
[source, csv]
----
id:ID(GroupOne){id-type:long},name,:LABEL
----

.persons.csv
[source, csv]
----
123,P1,Person
456,P2,Person
----

.games_header.csv
[source, csv]
----
id:ID(GroupTwo),name,:LABEL
----

.games.csv
[source, csv]
----
ABC,G1,Game
DEF,G2,Game
----

.Import the nodes
[source, shell, role=noplay]
----
neo4j_home$ --nodes persons.csv --nodes games.csv --id-type string
----

The `id` property of the nodes in the `persons` group will be stored as `long` type, while the `id` property of the nodes in the `games` group will be stored as `string` type, as the global `id-type` is a string.
====

[role=label--new-2025.01.0]
== Applying changes to data via CSV files

You can use CSV files to update existing nodes, relationships, labels, or properties during incremental import.

[NOTE]
====
This feature is supported only by `block` format.
====

=== Set an explicit action for each row

You can set an explicit action for each row in the CSV file by using the `:ACTION` keyword in the header file.
If no action is specified, the import tool works as in full import mode, creating new data.

The following actions are supported:

* `empty` = `CREATE` (default)
* `C`, `CREATE` - Creates new nodes and relationships, with or without properties, as well as labels.
* `U`, `UPDATE` - Updates existing nodes, relationships, labels, and properties.
* `D`, `DELETE` - Deletes existing nodes or relationships.
Deleting a node also deletes its relationships (`DETACH DELETE`).


.Using actions in CSV files to update nodes
[source, cypher, role="nocopy"]
----
:ACTION,uid:ID(label:Person),name,:LABEL
CREATE,person1,"Keanu Reeves",Actor
UPDATE,person2,"Laurence Fishburne",Actor
DELETE,person4,,
----

Nodes are identified by their unique property value for the key/label combination that the header specifies.

.Using actions in CSV files to update relationships
[source, cypher, role="nocopy"]
----
:ACTION,:START_ID,:END_ID,:TYPE,role
CREATE,person1,movie1,ACTED_IN,"Neo"
UPDATE,person2,movie1,ACTED_IN,"Morpheus"
DELETE,person3,movie1,ACTED_IN
----

Relationships are identified non-uniquely by their start and end node IDs, and their type.

To further narrow down selection you can tag a property column as an identifier to help out in selecting relationships uniquely (or at least more uniquely).

.Using actions in CSV files to update relationships with identifier properties
[source, cypher, role="nocopy"]
----
:ACTION,:START_ID,:TYPE,:END_ID,p1{identifier:true},name,p4
U,person1,KNOWS,person2,abc,"Keanu Reeves","Hello Morpheus"
U,person2,KNOWS,person1,def,"Laurence Fishburne","Hello Neo"
----

The data in the `p1` column for these relationships helps select relationships "more uniquely" if a multiple of `1,KNOWS,2` exists.
There can also be multiple identifier properties defined in the header.
Identifier properties match the selected relationships and will not be set on the relationships that already have them.

=== Update existing labels

You can add or remove one or more labels from an existing node by prepending the clause `LABEL` in the header with a `+` (default) or `-`:

* `:+LABEL` - Add one or more labels to an existing node.
* `:-LABEL` - Remove one or more labels (if they exist) from an existing node.


For example, a file could have the following format:

[source, csv]
----
uid:ID(label:Person),:+LABEL,:-LABEL,name,age
person1,Actor,Producer,"Keanu Reeves",55
person2,Actor;Director,,"Laurence Fishburne",60
----

In this case, all labels in the second column are added and all the labels in the third column are removed (if they exist).

=== Remove existing properties

You can remove properties from existing nodes or relationships by a `:-PROPERTY` column in the header.
In the contents of this field you can add zero or more property names to remove from the entity.
For example:

.Remove nodes' properties
[source, cypher, role="nocopy"]
----
:ACTION,uid:ID(label:Person),:-PROPERTY
U,person1,age;hometown
----

Properties `age` and `hometown` are removed from the node with the `uid:ID` `person1`.

.Remove relationships' properties
[source, cypher, role="nocopy"]
----
:ACTION,:START_ID,:END_ID,:TYPE,:-PROPERTY
U,person1,movie1,ACTED_IN,role;description
----

Properties `role` and `description` are removed from the relationship with the `:START_ID` `person1`, `:END_ID` `movie1`, and `:TYPE` `ACTED_IN`.

.Using actions in CSV files to update labels and properties
[source, cypher, role="nocopy"]
----
:ACTION,uid:ID(label:Person),:LABEL,:-LABEL,:-PROPERTY,name,height:int
U,person1,Actor,Producer,age;hometown,Henry",185
----

One CSV entry can specify all types of updates to one entity at the same time.
In this example, the node `person1` is updated with:

* added `Actor` label
* removed `Producer` label
* removed `age` and `hometown` properties
* set `name="Henry"` property
* set `height=185` property

== Importing data that spans multiple lines

The `--multiline-fields` option allows fields from an input source to span multiple lines, i.e. contain newline characters.
For example:

[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/node_header.csv,import/node_data.csv --multiline-fields=true databasename
----

Where `import/node_data.csv` contains multiline fields, such as:

[source, csv, role=nocopy]
----
id,name,birthDate,birthYear,birthLocation,description
1,John,October 1st,2000,New York,This is a multiline
description
----

[NOTE]
====
Setting `--multiline-fields=true` can severely degrade the performance of the importer.
Therefore, use it with care, especially with large imports.
====

Optionally, you can specify the format of the `--multiline-fields` to control the parsing of the input source by setting the `--multiline-fields-format` option.
Possible values are:

* `v1` - the default format, which uses the current processing method for multiline fields.
* `v2` - a more efficient processing method that requires text fields to be quoted.
For `v2`, the `--multiline-fields` option must be set to a list of files (regular expressions are allowed) that contain multiline fields.

Both formats have the restriction that the entirety of every row must be able to fit into the buffer (default is 4m).
The `--multiline-fields-format` option is available in the `full` and `incremental` import modes.

For example:

[.tabbed-example]
=====
[role=include-with-multiline-fields-format-v1]
======
[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/node_header.csv,import/node_data.csv --multiline-fields=true --multiline-fields-format=v1 databasename
----

Where `import/node_data.csv` contains multiline fields, such as:

[source, csv, role=nocopy]
----
id,name,birthDate,birthYear,birthLocation,description
1,John,October 1st,2000,New York,This is a multiline
description
----
======
[role=include-with-multiline-fields-format-v2]
======

[source, shell, role=noplay]
----
bin/neo4j-admin database import full --nodes import/node_header.csv,import/node_data.csv --multiline-fields=import/node_data.csv --multiline-fields-format=v2 databasename
----

Where `import/node_data.csv` contains multiline fields, such as:

[source, csv, role=nocopy]
----
id,name,birthDate,birthYear,birthLocation,description
1,"John","October 1st",2000,"New York","This is a multiline
description"
----
======
=====

[[import-tool-header-format-skip-columns]]
== Skipping columns

IGNORE::
If there are fields in the data that you wish to ignore completely, this can be done using the `IGNORE` keyword in the header file.
`IGNORE` must be prepended with a `:`.
+
.Skip a column
====

In this example, you are not interested in the data in the third column of the nodes file and wish to skip over it.
Note that the `IGNORE` keyword is prepended by a `:`.

[source, csv]
----
personId:ID,name,:IGNORE,:LABEL
----

[source, csv]
----
keanu,"Keanu Reeves","male",Actor
laurence,"Laurence Fishburne","male",Actor
carrieanne,"Carrie-Anne Moss","female",Actor
----
====

If all your superfluous data is placed in columns located to the right of all the columns that you wish to import, you can instead use the command line option `--ignore-extra-columns`.


[[import-tool-header-format-compressed-files]]
== Importing compressed files

The import tool can handle files compressed with `zip` or `gzip`.
Each compressed file must contain a single file.

.Perform an import using compressed files
====

[source, sh]
----
neo4j_home$ ls import
actors-header.csv  actors.csv.zip  movies-header.csv  movies.csv.gz  roles-header.csv  roles.csv.gz
----

[source, sh]
----
bin/neo4j-admin database import --nodes import/movies-header.csv,import/movies.csv.gz --nodes import/actors-header.csv,import/actors.csv.zip --relationships import/roles-header.csv,import/roles.csv.gz
----
====

[role=label--enterprise-edition]
[[indexes-constraints-import]]
== Provide indexes and constraints during import

You can use the `--schema` option to create and populate indexes/constraints during the import process.
It works for the block format and both full and incremental import.
For incremental import, this functionality is available from 2025.02.

You should have a Cypher script containing only `CREATE INDEX|CONSTRAINT` commands to be parsed and executed.
This file uses ';' as the separator.

For example:

[source, cypher, role=nocopy]
----
CREATE INDEX PersonNameIndex FOR (i:Person) ON (i.name);
CREATE CONSTRAINT PersonAgeConstraint FOR (c:Person) REQUIRE c.age IS :: INTEGER
----

List of supported indexes and constraints that can be created by the import tool:

* RANGE
* LOOKUP
* POINT
* TEXT
* FULL-TEXT
* VECTOR

For example:

.Create indexes and constraints during full import
[source, shell, role=noplay]
----
bin/neo4j-admin database import full neo4j --nodes=import/movies.csv --nodes=import/actors.csv --relationships=import/roles.csv --schema=import/schema.cypher
----

.Create indexes and constraints during incremental import
[source, shell, role=noplay]
----
bin/neo4j-admin database import incremental --stage=all --nodes=import/movies.csv --nodes=import/actors.csv --relationships=import/roles.csv --schema=import/schema.cypher
----

[NOTE]
====
You must stop your database, if you want to perform the incremental import within one command.
If you cannot afford a full downtime of your database, split the operation into several stages.
For details, see <<incremental-import-stages>>.
====

[role=label--enterprise-edition]
[[import-tool-resume]]
== Resuming a stopped or canceled import

An import that is stopped or fails before completing can be resumed from a point closer to where it was stopped.
An import can be resumed from the following points:

- Linking of relationships
- Post-processing
