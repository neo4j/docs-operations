[[configuration-settings]]
= Configuration settings
:description: This page provides a complete reference to the Neo4j configuration settings.
:description: Reference for Neo4j procedures.
:page-aliases: ROOT:reference/configuration-settings.adoc
:page-styles: hide-table-captions

This page provides a complete reference to the Neo4j configuration settings, which can be set in xref::/configuration/file-locations.adoc[_neo4j.conf_].
Refer to xref::/configuration/neo4j-conf.adoc[The _neo4j.conf_ file] for details on how to use configuration settings.

Some of the settings are labeled **_Dynamic_**, which means that they can be changed at runtime, without restarting the service.
For more information on how to update dynamic configuration settings, see xref:configuration/dynamic-settings.adoc[Update dynamic settings].

[role=label--enterprise]
== Configuration setting group for the load balancing framework

When deploying a multi-data cluster in Neo4j, you can configure the load balancing framework.

In Neo4j, the load balancing system is based on a plugin architecture.
The primary built-in plugin is `server_policies`, which is set up by the following property:

[source, shell]
----
causal_clustering.load_balancing.plugin=server_policies
----

`server_policies` plugin determines which servers are eligible to serve client requests based on predefined routing policies.
If a client does not specify a routing policy, the system defaults to using all available servers.

You can define routing policies by using the following property format:

[source, shell]
----
causal_clustering.load_balancing.config.server_policies.<policy-name>=<policy-definition>
----

Where `<policy-name>` is the name of the routing policy, and `<policy-definition>` specifies the server selection logic.

See xref:clustering-advanced/multi-data-center/load-balancing.adoc#multi-dc-load-balancing-the-load-balancing-framework[Appendix -> Advanced Causal Clustering -> Multi-data center load balancing] for more details.


== Checkpoint settings

[[config_dbms.checkpoint]]
=== `dbms.checkpoint`
.dbms.checkpoint
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configures the general policy for when check-points should occur. The default policy is the 'periodic' check-point policy, as specified by the '<<config_dbms.checkpoint.interval.tx,dbms.checkpoint.interval.tx>>' and '<<config_dbms.checkpoint.interval.time,dbms.checkpoint.interval.time>>' settings. The Neo4j Enterprise Edition provides two alternative policies: The first is the 'continuous' check-point policy, which will ignore those settings and run the check-point process all the time. The second is the 'volumetric' check-point policy, which makes a best-effort at check-pointing often enough so that the database doesn't get too far behind on deleting old transaction logs in accordance with the '<<config_dbms.tx_log.rotation.retention_policy,dbms.tx_log.rotation.retention_policy>>' setting.
|Valid values
a|dbms.checkpoint, one of [PERIODIC, CONTINUOUS, VOLUME, VOLUMETRIC]
|Default value
m|+++PERIODIC+++
|===

[[config_dbms.checkpoint.interval.time]]
=== `dbms.checkpoint.interval.time`
.dbms.checkpoint.interval.time
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configures the time interval between check-points. The database will not check-point more often than this (unless check pointing is triggered by a different event), but might check-point less often than this interval, if performing a check-point takes longer time than the configured interval. A check-point is a point in the transaction logs, which recovery would start from. Longer check-point intervals typically mean that recovery will take longer to complete in case of a crash. On the other hand, a longer check-point interval can also reduce the I/O load that the database places on the system, as each check-point implies a flushing and forcing of all the store files.
|Valid values
a|dbms.checkpoint.interval.time, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++15m+++
|===

[[config_dbms.checkpoint.interval.tx]]
=== `dbms.checkpoint.interval.tx`
.dbms.checkpoint.interval.tx
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configures the transaction interval between check-points. The database will not check-point more often  than this (unless check pointing is triggered by a different event), but might check-point less often than this interval, if performing a check-point takes longer time than the configured interval. A check-point is a point in the transaction logs, which recovery would start from. Longer check-point intervals typically mean that recovery will take longer to complete in case of a crash. On the other hand, a longer check-point interval can also reduce the I/O load that the database places on the system, as each check-point implies a flushing and forcing of all the store files.  The default is '100000' for a check-point every 100000 transactions.
|Valid values
a|dbms.checkpoint.interval.tx, an integer which is minimum `1`
|Default value
m|+++100000+++
|===

[[config_dbms.checkpoint.interval.volume]]
=== `dbms.checkpoint.interval.volume`
.dbms.checkpoint.interval.volume
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configures the volume of transacton logs between check-points. The database will not check-point more often than this (unless check pointing is triggered by a different event), but might check-point less often than this interval, if performing a check-point takes longer time than the configured interval. A check-point is a point in the transaction logs, which recovery would start from. Longer check-point intervals typically mean that recovery will take longer to complete in case of a crash. On the other hand, a longer check-point interval can also reduce the I/O load that the database places on the system, as each check-point implies a flushing and forcing of all the store files.
|Valid values
a|dbms.checkpoint.interval.volume, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `1.00KiB`
|Default value
m|+++250.00MiB+++
|===

[[config_dbms.checkpoint.iops.limit]]
=== `dbms.checkpoint.iops.limit`

label:dynamic[Dynamic]

.dbms.checkpoint.iops.limit
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limit the number of IOs the background checkpoint process will consume per second. This setting is advisory, is ignored in Neo4j Community Edition, and is followed to best effort in Enterprise Edition. An IO is in this case a 8 KiB (mostly sequential) write. Limiting the write IO in this way will leave more bandwidth in the IO subsystem to service random-read IOs, which is important for the response time of queries when the database cannot fit entirely in memory. The only drawback of this setting is that longer checkpoint times may lead to slightly longer recovery times in case of a database or system crash. A lower number means lower IO pressure, and consequently longer checkpoint times. Set this to -1 to disable the IOPS limit and remove the limitation entirely; this will let the checkpointer flush data as fast as the hardware will go. Removing the setting, or commenting it out, will set the default value of 600.
|Valid values
a|dbms.checkpoint.iops.limit, an integer
|Default value
m|+++600+++
|===


== Cluster settings

[[config_causal_clustering.catch_up_client_inactivity_timeout]]
=== `causal_clustering.catch_up_client_inactivity_timeout`
.causal_clustering.catch_up_client_inactivity_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The catch up protocol times out if the given duration elapses with no network activity. Every message received by the client from the server extends the time out duration.
|Valid values
a|causal_clustering.catch_up_client_inactivity_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10m+++
|===

[[config_causal_clustering.catchup_batch_size]]
=== `causal_clustering.catchup_batch_size`
.causal_clustering.catchup_batch_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum batch size when catching up (in unit of entries)
|Valid values
a|causal_clustering.catchup_batch_size, an integer
|Default value
m|+++64+++
|===

[[config_causal_clustering.cluster_allow_reads_on_followers]]
=== `causal_clustering.cluster_allow_reads_on_followers`
.causal_clustering.cluster_allow_reads_on_followers
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure if the `dbms.routing.getRoutingTable()` procedure should include followers as read endpoints or return only read replicas. Note: if there are no read replicas in the cluster, followers are returned as read end points regardless the value of this setting. Defaults to true so that followers are available for read-only queries in a typical heterogeneous setup.
|Valid values
a|causal_clustering.cluster_allow_reads_on_followers, a boolean
|Default value
m|+++true+++
|===

[[config_causal_clustering.cluster_allow_reads_on_leader]]
=== `causal_clustering.cluster_allow_reads_on_leader`

label:dynamic[Dynamic]

.causal_clustering.cluster_allow_reads_on_leader
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure if the `dbms.routing.getRoutingTable()` procedure should include the leader as read endpoint or return only read replicas/followers. Note: leader is returned as read endpoint if no other member is present all.
|Valid values
a|causal_clustering.cluster_allow_reads_on_leader, a boolean
|Default value
m|+++false+++
|===

[[config_causal_clustering.cluster_binding_timeout]]
=== `causal_clustering.cluster_binding_timeout`
.causal_clustering.cluster_binding_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The time allowed for a database on a Neo4j server to either join a cluster or form a new cluster with the other Neo4j Core Servers provided by `<<config_causal_clustering.initial_discovery_members,causal_clustering.initial_discovery_members>>`.
|Valid values
a|causal_clustering.cluster_binding_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10m+++
|===

[[config_causal_clustering.cluster_topology_refresh]]
=== `causal_clustering.cluster_topology_refresh`
.causal_clustering.cluster_topology_refresh
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Time between scanning the cluster to refresh current server's view of topology.
|Valid values
a|causal_clustering.cluster_topology_refresh, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1s`
|Default value
m|+++5s+++
|===

[[config_causal_clustering.command_applier_parallelism]]
=== `causal_clustering.command_applier_parallelism`
.causal_clustering.command_applier_parallelism
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limits amount of global threads for applying commands.
|Valid values
a|causal_clustering.command_applier_parallelism, an integer which is minimum `1`
|Default value
m|+++8+++
|===

[[config_causal_clustering.connect_randomly_to_server_group]]
=== `causal_clustering.connect_randomly_to_server_group`

label:dynamic[Dynamic]

.causal_clustering.connect_randomly_to_server_group
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Comma separated list of groups to be used by the connect-randomly-to-server-group selection strategy. The connect-randomly-to-server-group strategy is used if the list of strategies (`<<config_causal_clustering.upstream_selection_strategy,causal_clustering.upstream_selection_strategy>>`) includes the value `connect-randomly-to-server-group`.
|Valid values
a|causal_clustering.connect_randomly_to_server_group, a ',' separated list with elements of type 'a string identifying a Server Group'.
|Default value
m|++++++
|===

[[config_causal_clustering.delete_store_before_store_copy]]
=== `causal_clustering.delete_store_before_store_copy`
.causal_clustering.delete_store_before_store_copy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Deletes the old store (on cores and replicas) before performing a store copy (instead of deleting it after).
|Valid values
a|causal_clustering.delete_store_before_store_copy, a boolean
|Default value
m|+++true+++
|===

[[config_causal_clustering.discovery_advertised_address]]
=== `causal_clustering.discovery_advertised_address`
.causal_clustering.discovery_advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised cluster member discovery management communication.
|Valid values
a|causal_clustering.discovery_advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:5000+++
|===

[[config_causal_clustering.discovery_listen_address]]
=== `causal_clustering.discovery_listen_address`
.causal_clustering.discovery_listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Host and port to bind the cluster member discovery management communication.
|Valid values
a|causal_clustering.discovery_listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:5000+++
|===

[[config_causal_clustering.discovery_type]]
=== `causal_clustering.discovery_type`
.causal_clustering.discovery_type
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the discovery type used for cluster name resolution.
|Valid values
a|causal_clustering.discovery_type, one of [DNS, LIST, SRV, K8S] which depends on dbms.mode. If dbms.mode one of `[CORE, READ_REPLICA]` then it may require different settings depending on the discovery type: `DNS requires [causal_clustering.initial_discovery_members], LIST requires [causal_clustering.initial_discovery_members], SRV requires [causal_clustering.initial_discovery_members], K8S requires [causal_clustering.kubernetes.label_selector, causal_clustering.kubernetes.service_port_name]` otherwise it depends on dbms.mode. If dbms.mode one of `[SINGLE]` then it depends on dbms.clustering.enable. If dbms.clustering.enable is `true` then it may require different settings depending on the discovery type: `DNS requires [causal_clustering.initial_discovery_members], LIST requires [causal_clustering.initial_discovery_members], SRV requires [causal_clustering.initial_discovery_members], K8S requires [causal_clustering.kubernetes.label_selector, causal_clustering.kubernetes.service_port_name]` otherwise it is unconstrained. otherwise it is unconstrained..
|Default value
m|+++LIST+++
|===

[[config_causal_clustering.election_failure_detection_window]]
=== `causal_clustering.election_failure_detection_window`
.causal_clustering.election_failure_detection_window
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The rate at which leader elections happen. Note that due to election conflicts it might take several attempts to find a leader. The window should be significantly larger than typical communication delays to make conflicts unlikely.
|Valid values
a|causal_clustering.election_failure_detection_window, a duration-range <min-max> (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++3s-6s+++
|===

[[config_causal_clustering.enable_pre_voting]]
=== `causal_clustering.enable_pre_voting`
.causal_clustering.enable_pre_voting
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable pre-voting extension to the Raft protocol (this is breaking and must match between the core cluster members)
|Valid values
a|causal_clustering.enable_pre_voting, a boolean
|Default value
m|+++true+++
|===

[[config_causal_clustering.global_session_tracker_state_size]]
=== `causal_clustering.global_session_tracker_state_size`
.causal_clustering.global_session_tracker_state_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the global session tracker state file is rotated (in unit of entries)
|Valid values
a|causal_clustering.global_session_tracker_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.handshake_timeout]]
=== `causal_clustering.handshake_timeout`
.causal_clustering.handshake_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Time out for protocol negotiation handshake. This configuration is applicable to: `Raft` (communication between `CORE` instances only), `Catchup` (communication between any instances: `CORE` -> `CORE`, `RR` -> `CORE`, `RR` -> `RR`, `CORE` -> `RR`, including `RR` -> `SINGLE` in a replica-only cluster). `Backup` (communication between any instance and a backup client that lives in the `neo4j-admin` command, such as `BackupClient` -> `SINGLE`, `BackupClient` -> `CORE`, `BackupClient` -> `RR`).
|Valid values
a|causal_clustering.handshake_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++20s+++
|===

[[config_causal_clustering.in_flight_cache.max_bytes]]
=== `causal_clustering.in_flight_cache.max_bytes`
.causal_clustering.in_flight_cache.max_bytes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of bytes in the in-flight cache. This parameter limits the amount of memory that can be consumed by cache. If the bytes limit is reached, cache size will be limited even if max_entries is not exceeded.
|Valid values
a|causal_clustering.in_flight_cache.max_bytes, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|Default value
m|+++2.00GiB+++
|===

[[config_causal_clustering.in_flight_cache.max_entries]]
=== `causal_clustering.in_flight_cache.max_entries`
.causal_clustering.in_flight_cache.max_entries
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of entries in the in-flight cache. Increasing size will require more memory but might improve performance in high load situations.
|Valid values
a|causal_clustering.in_flight_cache.max_entries, an integer
|Default value
m|+++1024+++
|===

[[config_causal_clustering.in_flight_cache.type]]
=== `causal_clustering.in_flight_cache.type`
.causal_clustering.in_flight_cache.type
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Type of in-flight cache. CONSECUTIVE should be used for production instances, NONE will disable cache which might be useful in specific support cases. UNBOUNDED is for internal use only.
|Valid values
a|causal_clustering.in_flight_cache.type, one of [NONE, CONSECUTIVE, UNBOUNDED]
|Default value
m|+++CONSECUTIVE+++
|===

[[config_causal_clustering.initial_discovery_members]]
=== `causal_clustering.initial_discovery_members`
.causal_clustering.initial_discovery_members
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A comma-separated list of other members of the cluster to join.
|Valid values
a|causal_clustering.initial_discovery_members, a ',' separated list with elements of type 'a socket address in the format 'hostname:port', 'hostname' or ':port''.
|===

[[config_causal_clustering.join_catch_up_max_lag]]
=== `causal_clustering.join_catch_up_max_lag`
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum amount of lag accepted for a new follower to join the Raft group.
|Valid values
a|causal_clustering.join_catch_up_max_lag, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_causal_clustering.join_catch_up_timeout]]
=== `causal_clustering.join_catch_up_timeout`
.causal_clustering.join_catch_up_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Time out for a new member to catch up.
|Valid values
a|causal_clustering.join_catch_up_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10m+++
|===

[[config_causal_clustering.kubernetes.address]]
=== `causal_clustering.kubernetes.address`
.causal_clustering.kubernetes.address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Address for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.address, a socket address in the format 'hostname:port', 'hostname' or ':port'
|Default value
m|+++kubernetes.default.svc:443+++
|===

[[config_causal_clustering.kubernetes.ca_crt]]
=== `causal_clustering.kubernetes.ca_crt`
.causal_clustering.kubernetes.ca_crt
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|File location of CA certificate for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.ca_crt, a path
|Default value
m|+++/var/run/secrets/kubernetes.io/serviceaccount/ca.crt+++
|===

[[config_causal_clustering.kubernetes.cluster_domain]]
=== `causal_clustering.kubernetes.cluster_domain`
.causal_clustering.kubernetes.cluster_domain
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Kubernetes cluster domain.
|Valid values
a|causal_clustering.kubernetes.cluster_domain, a string
|Default value
m|+++cluster.local+++
|===

[[config_causal_clustering.kubernetes.label_selector]]
=== `causal_clustering.kubernetes.label_selector`
.causal_clustering.kubernetes.label_selector
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|LabelSelector for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.label_selector, a string
|===

[[config_causal_clustering.kubernetes.namespace]]
=== `causal_clustering.kubernetes.namespace`
.causal_clustering.kubernetes.namespace
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|File location of namespace for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.namespace, a path
|Default value
m|+++/var/run/secrets/kubernetes.io/serviceaccount/namespace+++
|===

[[config_causal_clustering.kubernetes.service_port_name]]
=== `causal_clustering.kubernetes.service_port_name`
.causal_clustering.kubernetes.service_port_name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Service port name for discovery for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.service_port_name, a string
|===

[[config_causal_clustering.kubernetes.token]]
=== `causal_clustering.kubernetes.token`
.causal_clustering.kubernetes.token
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|File location of token for Kubernetes API.
|Valid values
a|causal_clustering.kubernetes.token, a path
|Default value
m|+++/var/run/secrets/kubernetes.io/serviceaccount/token+++
|===

[[config_causal_clustering.last_applied_state_size]]
=== `causal_clustering.last_applied_state_size`
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the storage file is rotated (in unit of entries)
|Valid values
a|causal_clustering.last_applied_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.leader_election_timeout]]
=== `causal_clustering.leader_election_timeout`

label:deprecated[Deprecated]

.causal_clustering.leader_election_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|This setting is moved and enhanced into <<config_causal_clustering.leader_failure_detection_window,causal_clustering.leader_failure_detection_window>> and <<config_causal_clustering.election_failure_detection_window,causal_clustering.election_failure_detection_window>>.
|Valid values
a|causal_clustering.leader_election_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++7s+++
|===

[[config_causal_clustering.leader_failure_detection_window]]
=== `causal_clustering.leader_failure_detection_window`

.causal_clustering.leader_failure_detection_window
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The time window within which the loss of the leader is detected and the first re-election attempt is held.The window should be significantly larger than typical communication delays to make conflicts unlikely.
|Valid values
a|causal_clustering.leader_failure_detection_window, a duration-range <min-max> (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++20s-23s+++
|===

[[config_causal_clustering.leadership_balancing]]
=== `causal_clustering.leadership_balancing`

.causal_clustering.leadership_balancing
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Which strategy to use when transferring database leaderships around a cluster. This can be one of `equal_balancing` or `no_balancing`. `equal_balancing` automatically ensures that each Core server holds the leader role for an equal number of databases.`no_balancing` prevents any automatic balancing of the leader role.Note that if a `leadership_priority_group` is specified for a given database, the value of this setting will be ignored for that database.
|Valid values
a|causal_clustering.leadership_balancing, one of [NO_BALANCING, EQUAL_BALANCING]
|Default value
m|+++EQUAL_BALANCING+++
|===

[[config_causal_clustering.leadership_priority_group]]
=== `causal_clustering.leadership_priority_group`

.causal_clustering.leadership_priority_group
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The name of a server_group whose members should be prioritized as leaders. This does not guarantee that members of this group will be leader at all times, but the cluster will attempt to transfer leadership to such a member when possible. If a database is specified using `causal_clustering.leadership_priority_group`.<database> the specified priority group will apply to that database only. If no database is specified that group will be the default and apply to all databases which have no priority group explicitly set. Using this setting will disable leadership balancing.
|Valid values
a|causal_clustering.leadership_priority_group, a string identifying a Server Group
|Default value
m|++++++
|===

[[config_causal_clustering.load_balancing.plugin]]
=== `causal_clustering.load_balancing.plugin`

.causal_clustering.load_balancing.plugin
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The load balancing plugin to use.
|Valid values
a|causal_clustering.load_balancing.plugin, a string which depends on dbms.mode. If dbms.mode one of `[CORE]` then it specified load balancer plugin exist. otherwise it is unconstrained.
|Default value
m|+++server_policies+++
|===

[[config_causal_clustering.load_balancing.shuffle]]
=== `causal_clustering.load_balancing.shuffle`

.causal_clustering.load_balancing.shuffle
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enables shuffling of the returned load balancing result.
|Valid values
a|causal_clustering.load_balancing.shuffle, a boolean
|Default value
m|+++true+++
|===

[[config_causal_clustering.log_shipping_max_lag]]
=== `causal_clustering.log_shipping_max_lag`

.causal_clustering.log_shipping_max_lag
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum lag allowed before log shipping pauses (in unit of entries)
|Valid values
a|causal_clustering.log_shipping_max_lag, an integer
|Default value
m|+++256+++
|===

[[config_causal_clustering.log_shipping_retry_timeout]]
=== `causal_clustering.log_shipping_retry_timeout`

.causal_clustering.log_shipping_retry_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Retry time for log shipping to followers after a stall.
|Valid values
a|causal_clustering.log_shipping_retry_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5s+++
|===

[[config_causal_clustering.max_raft_channels]]
=== `causal_clustering.max_raft_channels`

.causal_clustering.max_raft_channels
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of TCP channels between two nodes to operate the raft protocol.
Each database gets allocated one channel, but a single channel can be used by more than one database.
|Valid values
a|causal_clustering.max_raft_channels, an integer
|Default value
m|+++8+++
|===

[[config_causal_clustering.middleware.logging.level]]
=== `causal_clustering.middleware.logging.level`

.causal_clustering.middleware.logging.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The level of middleware logging.
|Valid values
a|causal_clustering.middleware.logging.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|Default value
m|+++WARN+++
|===

[[config_causal_clustering.minimum_core_cluster_size_at_formation]]
=== `causal_clustering.minimum_core_cluster_size_at_formation`

.causal_clustering.minimum_core_cluster_size_at_formation
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Minimum number of Core machines initially required to form a cluster. The cluster will form when at least this many Core members have discovered each other.
|Valid values
a|causal_clustering.minimum_core_cluster_size_at_formation, an integer which is minimum `2`
|Default value
m|+++3+++
|===

[[config_causal_clustering.minimum_core_cluster_size_at_runtime]]
=== `causal_clustering.minimum_core_cluster_size_at_runtime`

.causal_clustering.minimum_core_cluster_size_at_runtime
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The minimum size of the dynamically adjusted voting set (which only core members may be a part of). Adjustments to the voting set happen automatically as the availability of core members changes, due to explicit operations such as starting or stopping a member, or unintended issues such as network partitions. Note that this dynamic scaling of the voting set is generally desirable as under some circumstances it can increase the number of instance failures which may be tolerated. A majority of the voting set must be available before voting in or out members.
|Valid values
a|causal_clustering.minimum_core_cluster_size_at_runtime, an integer which is minimum `2` and depends on dbms.mode. If dbms.mode one of `[CORE]` then it must be set less than or equal to value of 'causal_clustering.minimum_core_cluster_size_at_formation' otherwise it is unconstrained.
|Default value
m|+++3+++
|===

[[config_causal_clustering.multi_dc_license]]
=== `causal_clustering.multi_dc_license`

.causal_clustering.multi_dc_license
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable multi-data center features. Requires appropriate licensing.
|Valid values
a|causal_clustering.multi_dc_license, a boolean
|Default value
m|+++false+++
|===

[[config_causal_clustering.protocol_implementations.catchup]]
=== `causal_clustering.protocol_implementations.catchup`

.causal_clustering.protocol_implementations.catchup
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Catchup protocol implementation versions that this instance will allow in negotiation as a comma-separated list. Order is not relevant: the greatest value will be preferred. An empty list will allow all supported versions. Example value: "1.1, 1.2, 2.1, 2.2"
|Valid values
a|causal_clustering.protocol_implementations.catchup, a ',' separated list with elements of type 'an application protocol version'.
|Default value
m|++++++
|===

[[config_causal_clustering.protocol_implementations.compression]]
=== `causal_clustering.protocol_implementations.compression`

.causal_clustering.protocol_implementations.compression
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Network compression algorithms that this instance will allow in negotiation as a comma-separated list. Listed in descending order of preference for incoming connections. An empty list implies no compression. For outgoing connections this merely specifies the allowed set of algorithms and the preference of the  remote peer will be used for making the decision. Allowable values: [Gzip, Snappy, Snappy_validating, LZ4, LZ4_high_compression, LZ_validating, LZ4_high_compression_validating]
|Valid values
a|causal_clustering.protocol_implementations.compression, a ',' separated list with elements of type 'a string'.
|Default value
m|++++++
|===

[[config_causal_clustering.protocol_implementations.raft]]
=== `causal_clustering.protocol_implementations.raft`

.causal_clustering.protocol_implementations.raft
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Raft protocol implementation versions that this instance will allow in negotiation as a comma-separated list. Order is not relevant: the greatest value will be preferred. An empty list will allow all supported versions. Example value: "1.0, 1.3, 2.0, 2.1"
|Valid values
a|causal_clustering.protocol_implementations.raft, a ',' separated list with elements of type 'an application protocol version'.
|Default value
m|++++++
|===

[[config_causal_clustering.pull_interval]]
=== `causal_clustering.pull_interval`

.causal_clustering.pull_interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Interval of pulling updates from cores.
|Valid values
a|causal_clustering.pull_interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1s+++
|===

[[config_causal_clustering.raft_advertised_address]]
=== `causal_clustering.raft_advertised_address`

.causal_clustering.raft_advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised hostname/IP address and port for the RAFT server.
|Valid values
a|causal_clustering.raft_advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:7000+++
|===

[[config_causal_clustering.raft_handler_parallelism]]
=== `causal_clustering.raft_handler_parallelism`

.causal_clustering.raft_handler_parallelism
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limits amount of global threads shared by raft groups for handling bathing of messages and timeout events.
|Valid values
a|causal_clustering.raft_handler_parallelism, an integer which is minimum `1`
|Default value
m|+++8+++
|===

[[config_causal_clustering.raft_in_queue_max_batch_bytes]]
=== `causal_clustering.raft_in_queue_max_batch_bytes`

.causal_clustering.raft_in_queue_max_batch_bytes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Largest batch processed by RAFT in bytes.
|Valid values
a|causal_clustering.raft_in_queue_max_batch_bytes, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|Default value
m|+++8.00MiB+++
|===

[[config_causal_clustering.raft_in_queue_max_bytes]]
=== `causal_clustering.raft_in_queue_max_bytes`

.causal_clustering.raft_in_queue_max_bytes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of bytes in the RAFT in-queue.
|Valid values
a|causal_clustering.raft_in_queue_max_bytes, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|Default value
m|+++2.00GiB+++
|===

[[config_causal_clustering.raft_listen_address]]
=== `causal_clustering.raft_listen_address`

.causal_clustering.raft_listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Network interface and port for the RAFT server to listen on.
|Valid values
a|causal_clustering.raft_listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:7000+++
|===

[[config_causal_clustering.raft_log_entry_prefetch_buffer.max_entries]]
=== `causal_clustering.raft_log_entry_prefetch_buffer.max_entries`

.causal_clustering.raft_log_entry_prefetch_buffer.max_entries
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of entries in the raft log entry prefetch buffer.
|Valid values
a|causal_clustering.raft_log_entry_prefetch_buffer.max_entries, an integer
|Default value
m|+++1024+++
|===

[[config_causal_clustering.raft_log_implementation]]
=== `causal_clustering.raft_log_implementation`

.causal_clustering.raft_log_implementation
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|RAFT log implementation.
|Valid values
a|causal_clustering.raft_log_implementation, a string
|Default value
m|+++SEGMENTED+++
|===

[[config_causal_clustering.raft_log_prune_strategy]]
=== `causal_clustering.raft_log_prune_strategy`

.causal_clustering.raft_log_prune_strategy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|RAFT log pruning strategy that determines which logs are to be pruned. Neo4j only prunes log entries up to the last applied index, which guarantees that logs are only marked for pruning once the transactions within are safely copied over to the local transaction logs and safely committed by a majority of cluster members. Possible values are a byte size or a number of transactions (e.g., 200K txs).
|Valid values
a|causal_clustering.raft_log_prune_strategy, a string
|Default value
m|+++1g size+++
|===

[[config_causal_clustering.raft_log_pruning_frequency]]
=== `causal_clustering.raft_log_pruning_frequency`

.causal_clustering.raft_log_pruning_frequency
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|RAFT log pruning frequency.
|Valid values
a|causal_clustering.raft_log_pruning_frequency, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10m+++
|===

[[config_causal_clustering.raft_log_reader_pool_size]]
=== `causal_clustering.raft_log_reader_pool_size`

.causal_clustering.raft_log_reader_pool_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|RAFT log reader pool size.
|Valid values
a|causal_clustering.raft_log_reader_pool_size, an integer
|Default value
m|+++8+++
|===

[[config_causal_clustering.raft_log_rotation_size]]
=== `causal_clustering.raft_log_rotation_size`

.causal_clustering.raft_log_rotation_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|RAFT log rotation size.
|Valid values
a|causal_clustering.raft_log_rotation_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `1.00KiB`
|Default value
m|+++250.00MiB+++
|===

[[config_causal_clustering.raft_membership_state_size]]
=== `causal_clustering.raft_membership_state_size`

.causal_clustering.raft_membership_state_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the membership state file is rotated (in unit of entries)
|Valid values
a|causal_clustering.raft_membership_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.raft_term_state_size]]
=== `causal_clustering.raft_term_state_size`

.causal_clustering.raft_term_state_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the term state file is rotated (in unit of entries)
|Valid values
a|causal_clustering.raft_term_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.raft_vote_state_size]]
=== `causal_clustering.raft_vote_state_size`

.causal_clustering.raft_vote_state_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the vote state file is rotated (in unit of entries)
|Valid values
a|causal_clustering.raft_vote_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.refuse_to_be_leader]]
=== `causal_clustering.refuse_to_be_leader`

label:deprecated[Deprecated]

.causal_clustering.refuse_to_be_leader
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Deprecated, use <<config_dbms.databases.default_to_read_only,dbms.databases.default_to_read_only>>
|Valid values
a|causal_clustering.refuse_to_be_leader, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_dbms.databases.default_to_read_only,dbms.databases.default_to_read_only>>
|===

[[config_causal_clustering.replicated_lease_state_size]]
=== `causal_clustering.replicated_lease_state_size`

.causal_clustering.replicated_lease_state_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum file size before the replicated lease state file is rotated (in unit of entries)
|Valid values
a|causal_clustering.replicated_lease_state_size, an integer
|Default value
m|+++1000+++
|===

[[config_causal_clustering.replication_leader_await_timeout]]
=== `causal_clustering.replication_leader_await_timeout`

.causal_clustering.replication_leader_await_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The duration for which the replicator will await a new leader.
|Valid values
a|causal_clustering.replication_leader_await_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_causal_clustering.replication_retry_timeout_base]]
=== `causal_clustering.replication_retry_timeout_base`

.causal_clustering.replication_retry_timeout_base
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The initial timeout until replication is retried. The timeout will increase exponentially.
|Valid values
a|causal_clustering.replication_retry_timeout_base, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_causal_clustering.replication_retry_timeout_limit]]
=== `causal_clustering.replication_retry_timeout_limit`

.causal_clustering.replication_retry_timeout_limit
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The upper limit for the exponentially incremented retry timeout.
|Valid values
a|causal_clustering.replication_retry_timeout_limit, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

[[config_causal_clustering.server_groups]]
=== `causal_clustering.server_groups`

label:dynamic[Dynamic]

.causal_clustering.server_groups
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of group names for the server used when configuring load balancing and replication policies.
|Valid values
a|causal_clustering.server_groups, a ',' separated list with elements of type 'a string identifying a Server Group'.
|Default value
m|++++++
|===

[[config_causal_clustering.state_machine_apply_max_batch_size]]
=== `causal_clustering.state_machine_apply_max_batch_size`

.causal_clustering.state_machine_apply_max_batch_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of operations to be batched during applications of operations in the state machines.
|Valid values
a|causal_clustering.state_machine_apply_max_batch_size, an integer
|Default value
m|+++16+++
|===

[[config_causal_clustering.state_machine_flush_window_size]]
=== `causal_clustering.state_machine_flush_window_size`

.causal_clustering.state_machine_flush_window_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The number of operations to be processed before the state machines flush to disk.
|Valid values
a|causal_clustering.state_machine_flush_window_size, an integer
|Default value
m|+++4096+++
|===

[[config_causal_clustering.status_throughput_window]]
=== `causal_clustering.status_throughput_window`

.causal_clustering.status_throughput_window
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sampling window for throughput estimate reported in the status endpoint.
|Valid values
a|causal_clustering.status_throughput_window, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is in the range `1s` to `5m`
|Default value
m|+++5s+++
|===

[[config_causal_clustering.store_copy_chunk_size]]
=== `causal_clustering.store_copy_chunk_size`

.causal_clustering.store_copy_chunk_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Store copy chunk size.
|Valid values
a|causal_clustering.store_copy_chunk_size, an integer which is in the range `4096` to `1048576`
|Default value
m|+++32768+++
|===

[[config_causal_clustering.store_copy_max_retry_time_per_request]]
=== `causal_clustering.store_copy_max_retry_time_per_request`

.causal_clustering.store_copy_max_retry_time_per_request
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum retry time per request during store copy. Regular store files and indexes are downloaded in separate requests during store copy. This configures the maximum time failed requests are allowed to resend.
|Valid values
a|causal_clustering.store_copy_max_retry_time_per_request, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++20m+++
|===

[[config_causal_clustering.store_copy_parallelism]]
=== `causal_clustering.store_copy_parallelism`

.causal_clustering.store_copy_parallelism
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limits amount of global threads for store copy.
|Valid values
a|causal_clustering.store_copy_parallelism, an integer which is minimum `1`
|Default value
m|+++8+++
|===

[[config_causal_clustering.transaction_advertised_address]]
=== `causal_clustering.transaction_advertised_address`

.causal_clustering.transaction_advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised hostname/IP address and port for the transaction shipping server.
|Valid values
a|causal_clustering.transaction_advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address.
|Default value
m|+++:6000+++
|===

[[config_causal_clustering.transaction_listen_address]]
=== `causal_clustering.transaction_listen_address`

.causal_clustering.transaction_listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Network interface and port for the transaction shipping server to listen on. Please note that it is also possible to run the backup client against this port so always limit access to it via the firewall and configure an ssl policy.
|Valid values
a|causal_clustering.transaction_listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:6000+++
|===

[[config_causal_clustering.unknown_address_logging_throttle]]
=== `causal_clustering.unknown_address_logging_throttle`

.causal_clustering.unknown_address_logging_throttle
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Throttle limit for logging unknown cluster member address.
|Valid values
a|causal_clustering.unknown_address_logging_throttle, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_causal_clustering.upstream_selection_strategy]]
=== `causal_clustering.upstream_selection_strategy`

.causal_clustering.upstream_selection_strategy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|An ordered list in descending preference of the strategy which read replicas use to choose the upstream server from which to pull transactional updates.
|Valid values
a|causal_clustering.upstream_selection_strategy, a ',' separated list with elements of type 'a string'.
|Default value
m|+++default+++
|===

[[config_causal_clustering.user_defined_upstream_strategy]]
=== `causal_clustering.user_defined_upstream_strategy`

.causal_clustering.user_defined_upstream_strategy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configuration of a user-defined upstream selection strategy. The user-defined strategy is used if the list of strategies (`<<config_causal_clustering.upstream_selection_strategy,causal_clustering.upstream_selection_strategy>>`) includes the value `user_defined`.
|Valid values
a|causal_clustering.user_defined_upstream_strategy, a string
|Default value
m|++++++
|===

== Fabric settings

[[config_fabric.database.name]]
=== `fabric.database.name`

.fabric.database.name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name of the Fabric database. Only one Fabric database is currently supported per Neo4j instance.
|Valid values
a|fabric.database.name, A valid database name containing only alphabetic characters, numbers, dots and dashes with a length between 3 and 63 characters, starting with an alphabetic character but not with the name 'system'
|===

[[config_fabric.driver.api]]
=== `fabric.driver.api`

.fabric.driver.api
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Determines which driver API will be used. ASYNC must be used when the remote instance is 3.5.
|Valid values
a|fabric.driver.api, one of [RX, ASYNC]
|Default value
m|+++RX+++
|===

[[config_fabric.driver.connection.connect_timeout]]
=== `fabric.driver.connection.connect_timeout`

.fabric.driver.connection.connect_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Socket connection timeout.
A timeout of zero is treated as an infinite timeout and will be bound by the timeout configured on the
operating system level.
|Valid values
a|fabric.driver.connection.connect_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5s+++
|===

[[config_fabric.driver.connection.max_lifetime]]
=== `fabric.driver.connection.max_lifetime`

.fabric.driver.connection.max_lifetime
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections older than this threshold will be closed and removed from the pool.
Setting this option to a low value will cause a high connection churn and might result in a performance hit.
It is recommended to set maximum lifetime to a slightly smaller value than the one configured in network
equipment (load balancer, proxy, firewall, etc. can also limit maximum connection lifetime).
Zero and negative values result in lifetime not being checked.
|Valid values
a|fabric.driver.connection.max_lifetime, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1h+++
|===

[[config_fabric.driver.connection.pool.acquisition_timeout]]
=== `fabric.driver.connection.pool.acquisition_timeout`

.fabric.driver.connection.pool.acquisition_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum amount of time spent attempting to acquire a connection from the connection pool.
This timeout only kicks in when all existing connections are being used and no new connections can be created because maximum connection pool size has been reached.
Error is raised when connection can't be acquired within configured time.
Negative values are allowed and result in unlimited acquisition timeout. Value of 0 is allowed and results in no timeout and immediate failure when connection is unavailable.
|Valid values
a|fabric.driver.connection.pool.acquisition_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

[[config_fabric.driver.connection.pool.idle_test]]
=== `fabric.driver.connection.pool.idle_test`

.fabric.driver.connection.pool.idle_test
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections that have been idle in the pool for longer than this timeout will be tested before they are used again, to ensure they are still alive.
If this option is set too low, an additional network call will be incurred when acquiring a connection, which causes a performance hit.
If this is set high, no longer live connections might be used which might lead to errors.
Hence, this parameter tunes a balance between the likelihood of experiencing connection problems and performance
Normally, this parameter should not need tuning.
Value 0 means connections will always be tested for validity.
|Valid values
a|fabric.driver.connection.pool.idle_test, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|No connection liveliness check is done by default.
|===

[[config_fabric.driver.connection.pool.max_size]]
=== `fabric.driver.connection.pool.max_size`

.fabric.driver.connection.pool.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum total number of connections to be managed by a connection pool.
The limit is enforced for a combination of a host and user. Negative values are allowed and result in unlimited pool.
Value of 0 is not allowed.
|Valid values
a|fabric.driver.connection.pool.max_size, an integer
|Default value
m|Unlimited
|===

[[config_fabric.driver.logging.level]]
=== `fabric.driver.logging.level`

.fabric.driver.logging.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sets level for driver internal logging.
|Valid values
a|fabric.driver.logging.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|Default value
m|Value of dbms.logs.debug.level
|===

[[config_fabric.graph.-graph-ID-.database]]
=== `fabric.graph.<graph ID>.database`

.fabric.graph.<graph ID>.database
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name of the database associated to the Fabric graph.
|Valid values
a|fabric.graph.<graph ID>.database, a string
|Default value
m|The default database on the target DBMS. Typically 'Neo4j'
|===

[[config_fabric.graph.-graph-ID-.driver.api]]
=== `fabric.graph.<graph ID>.driver.api`

.fabric.graph.<graph ID>.driver.api
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Determines which driver API will be used. ASYNC must be used when the remote instance is 3.5
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.api,fabric.driver.api>>'
|Valid values
a|fabric.graph.<graph ID>.driver.api, one of [RX, ASYNC]
|===

[[config_fabric.graph.-graph-ID-.driver.connection.connect_timeout]]
=== `fabric.graph.<graph ID>.driver.connection.connect_timeout`

.fabric.graph.<graph ID>.driver.connection.connect_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Socket connection timeout.
A timeout of zero is treated as an infinite timeout and will be bound by the timeout configured on the
operating system level.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.connection.connect_timeout,fabric.driver.connection.connect_timeout>>'
|Valid values
a|fabric.graph.<graph ID>.driver.connection.connect_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|===

[[config_fabric.graph.-graph-ID-.driver.connection.max_lifetime]]
=== `fabric.graph.<graph ID>.driver.connection.max_lifetime`

.fabric.graph.<graph ID>.driver.connection.max_lifetime
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections older than this threshold will be closed and removed from the pool.
Setting this option to a low value will cause a high connection churn and might result in a performance hit.
It is recommended to set maximum lifetime to a slightly smaller value than the one configured in network
equipment (load balancer, proxy, firewall, etc. can also limit maximum connection lifetime).
Zero and negative values result in lifetime not being checked.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.connection.max_lifetime,fabric.driver.connection.max_lifetime>>'
|Valid values
a|fabric.graph.<graph ID>.driver.connection.max_lifetime, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|===

[[config_fabric.graph.-graph-ID-.driver.connection.pool.acquisition_timeout]]
=== `fabric.graph.<graph ID>.driver.connection.pool.acquisition_timeout`

.fabric.graph.<graph ID>.driver.connection.pool.acquisition_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum amount of time spent attempting to acquire a connection from the connection pool.
This timeout only kicks in when all existing connections are being used and no new connections can be created because maximum connection pool size has been reached.
Error is raised when connection can't be acquired within configured time.
Negative values are allowed and result in unlimited acquisition timeout. Value of 0 is allowed and results in no timeout and immediate failure when connection is unavailable.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.connection.pool.acquisition_timeout,fabric.driver.connection.pool.acquisition_timeout>>'
|Valid values
a|fabric.graph.<graph ID>.driver.connection.pool.acquisition_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|===

[[config_fabric.graph.-graph-ID-.driver.connection.pool.idle_test]]
=== `fabric.graph.<graph ID>.driver.connection.pool.idle_test`

.fabric.graph.<graph ID>.driver.connection.pool.idle_test
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections that have been idle in the pool for longer than this timeout will be tested before they are used again, to ensure they are still alive.
If this option is set too low, an additional network call will be incurred when acquiring a connection, which causes a performance hit.
If this is set high, no longer live connections might be used which might lead to errors.
Hence, this parameter tunes a balance between the likelihood of experiencing connection problems and performance
Normally, this parameter should not need tuning.
Value 0 means connections will always be tested for validity.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.connection.pool.idle_test,fabric.driver.connection.pool.idle_test>>'
|Valid values
a|fabric.graph.<graph ID>.driver.connection.pool.idle_test, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|===

[[config_fabric.graph.-graph-ID-.driver.connection.pool.max_size]]
=== `fabric.graph.<graph ID>.driver.connection.pool.max_size`

.fabric.graph.<graph ID>.driver.connection.pool.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum total number of connections to be managed by a connection pool.
The limit is enforced for a combination of a host and user. Negative values are allowed and result in unlimited pool. Value of 0 is not allowed.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.connection.pool.max_size,fabric.driver.connection.pool.max_size>>'
|Valid values
a|fabric.graph.<graph ID>.driver.connection.pool.max_size, an integer
|===

[[config_fabric.graph.-graph-ID-.driver.logging.leaked_sessions]]
=== `fabric.graph.<graph ID>.driver.logging.leaked_sessions`

.fabric.graph.<graph ID>.driver.logging.leaked_sessions
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enables logging of leaked driver session.
|Valid values
a|fabric.graph.<graph ID>.driver.logging.leaked_sessions, a boolean
|===

[[config_fabric.graph.-graph-ID-.driver.logging.level]]
=== `fabric.graph.<graph ID>.driver.logging.level`

.fabric.graph.<graph ID>.driver.logging.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sets level for driver internal logging.
This setting can be used as a graph-specific override of the global setting '<<config_fabric.driver.logging.level,fabric.driver.logging.level>>'
|Valid values
a|fabric.graph.<graph ID>.driver.logging.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|===

[[config_fabric.graph.-graph-ID-.driver.ssl_enabled]]
=== `fabric.graph.<graph ID>.driver.ssl_enabled`

.fabric.graph.<graph ID>.driver.ssl_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|SSL for Fabric drivers is configured using 'fabric' SSL policy.This setting can be used to instruct the driver not to use SSL even though 'fabric' SSL policy is configured.The driver will use SSL if 'fabric' SSL policy is configured and this setting is set to 'true'
|Valid values
a|fabric.graph.<graph ID>.driver.ssl_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_fabric.graph.-graph-ID-.name]]
=== `fabric.graph.<graph ID>.name`

.fabric.graph.<graph ID>.name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name assigned to the Fabric graph. The name can be used in Fabric queries.
|Valid values
a|fabric.graph.<graph ID>.name, A valid graph name. Containing only alphabetic characters, numbers, dots and dashes, with a length between 3 and 63 characters. It should be starting with an alphabetic character. The name 'graph' is reserved.
|===

[[config_fabric.graph.-graph-ID-.uri]]
.fabric.graph.<graph ID>.uri
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|URI of the Neo4j DBMS hosting the database associated to the Fabric graph. Example: neo4j://somewhere:7687
A comma separated list of URIs is acceptable. This is useful when the Fabric graph is hosted on a cluster and more that one bootstrap address needs to be provided in order to avoid a single point of failure. The provided addresses will be considered as an initial source of a routing table. Example: neo4j://core-1:1111,neo4j://core-2:2222.
|Valid values
a|fabric.graph.<graph ID>.uri, a ',' separated list with elements of type 'a URI'.
|===

[[config_fabric.routing.servers]]
=== `fabric.routing.servers`

label:dynamic[Dynamic]

.fabric.routing.servers
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A comma-separated list of Fabric instances that form a routing group. A driver will route transactions to available routing group members.
A Fabric instance is represented by its Bolt connector address.
|Valid values
a|fabric.routing.servers, a ',' separated list with elements of type 'a socket address in the format 'hostname:port', 'hostname' or ':port''.
|===

[[config_fabric.routing.ttl]]
=== `fabric.routing.ttl`

.fabric.routing.ttl
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The time to live (TTL) of a routing table for fabric routing group.
|Valid values
a|fabric.routing.ttl, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

[[config_fabric.stream.buffer.low_watermark]]
=== `fabric.stream.buffer.low_watermark`

.fabric.stream.buffer.low_watermark
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Number of records in prefetching buffer that will trigger prefetching again. This is strongly related to <<config_fabric.stream.buffer.size,fabric.stream.buffer.size>>
|Valid values
a|fabric.stream.buffer.low_watermark, an integer which is minimum `0`
|Default value
m|+++300+++
|===

[[config_fabric.stream.buffer.size]]
=== `fabric.stream.buffer.size`

.fabric.stream.buffer.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximal size of a buffer used for pre-fetching result records of remote queries.
To compensate for latency to remote databases, the Fabric execution engine pre-fetches records needed for local executions.
This limit is enforced per fabric query. If a fabric query uses multiple remote stream at the same time, this setting represents the maximal number of pre-fetched records counted together for all such remote streams.
|Valid values
a|fabric.stream.buffer.size, an integer which is minimum `1`
|Default value
m|+++1000+++
|===

[[config_fabric.stream.concurrency]]
=== `fabric.stream.concurrency`

.fabric.stream.concurrency
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximal concurrency within Fabric queries.
Limits the number of iterations of each subquery that are executed concurrently. Higher concurrency may consume more memory and network resources simultaneously, while lower concurrency may force sequential execution, requiring more time.
|Valid values
a|fabric.stream.concurrency, an integer which is minimum `1`
|Default value
m|The number of remote graphs
|===

== Connection settings

[[config_dbms.default_advertised_address]]
=== `dbms.default_advertised_address`

.dbms.default_advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Default hostname or IP address the server uses to advertise itself.
|Valid values
a|dbms.default_advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port' which has no specified port
|Default value
m|+++localhost+++
|===

[[config_dbms.default_listen_address]]
=== `dbms.default_listen_address`

.dbms.default_listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Default network interface to listen for incoming connections. To listen for connections on all interfaces, use "0.0.0.0".
|Valid values
a|dbms.default_listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port' which has no specified port
|Default value
m|+++localhost+++
|===

[[config_dbms.http_enabled_modules]]
=== `dbms.http_enabled_modules`

.dbms.http_enabled_modules
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the set of modules loaded into the Neo4j web server. Options include TRANSACTIONAL_ENDPOINTS, BROWSER, UNMANAGED_EXTENSIONS and ENTERPRISE_MANAGEMENT_ENDPOINTS (if applicable).
|Valid values
a|dbms.http_enabled_modules, a ',' separated set with elements of type 'one of [TRANSACTIONAL_ENDPOINTS, UNMANAGED_EXTENSIONS, BROWSER, ENTERPRISE_MANAGEMENT_ENDPOINTS]'.
|Default value
m|+++TRANSACTIONAL_ENDPOINTS,UNMANAGED_EXTENSIONS,BROWSER,ENTERPRISE_MANAGEMENT_ENDPOINTS+++
|===

[[config_dbms.routing.advertised_address]]
=== `dbms.routing.advertised_address`

.dbms.routing.advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The advertised address for the intra-cluster routing connector.
|Valid values
a|dbms.routing.advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:7688+++
|===

[[config_dbms.routing.client_side.enforce_for_domains]]
=== `dbms.routing.client_side.enforce_for_domains`

label:dynamic[Dynamic]

.dbms.routing.client_side.enforce_for_domains
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Always use client side routing (regardless of the default router) for neo4j:// protocol connections to these domains. A comma-separated list of domains. Wildcards (*) are supported.
|Valid values
a|dbms.routing.client_side.enforce_for_domains, a ',' separated set with elements of type 'a string'.
|Default value
m|++++++
|===

[[config_dbms.routing.default_router]]
=== `dbms.routing.default_router`

.dbms.routing.default_router
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Routing strategy for neo4j:// protocol connections.
Default is `CLIENT`, using client-side routing, with server-side routing as a fallback (if enabled).
When set to `SERVER`, client-side routing is short-circuited, and requests will rely on server-side routing (which must be enabled for proper operation, i.e. `<<config_dbms.routing.enabled,dbms.routing.enabled>>=true`).
Can be overridden by `<<config_dbms.routing.client_side.enforce_for_domains,dbms.routing.client_side.enforce_for_domains>>`.
|Valid values
a|dbms.routing.default_router, one of [SERVER, CLIENT]
|Default value
m|+++CLIENT+++
|===

[[config_dbms.routing.driver.api]]
=== `dbms.routing.driver.api`

.dbms.routing.driver.api
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Determines which driver API will be used. `ASYNC` must be used when the remote instance is 3.5, but is only retained for backwards-compatibility reasons. `RX` should be used in all other cases.
|Valid values
a|dbms.routing.driver.api, one of [RX, ASYNC]
|Default value
m|+++RX+++
|===

[[config_dbms.routing.driver.connection.connect_timeout]]
=== `dbms.routing.driver.connection.connect_timeout`

.dbms.routing.driver.connection.connect_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Socket connection timeout.
A timeout of zero is treated as an infinite timeout and will be bound by the timeout configured on the
operating system level.
|Valid values
a|dbms.routing.driver.connection.connect_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5s+++
|===

[[config_dbms.routing.driver.connection.max_lifetime]]
=== `dbms.routing.driver.connection.max_lifetime`

.dbms.routing.driver.connection.max_lifetime
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections older than this threshold will be closed and removed from the pool.
Setting this option to a low value will cause a high connection churn and might result in a performance hit.
It is recommended to set maximum lifetime to a slightly smaller value than the one configured in network
equipment (load balancer, proxy, firewall, etc. can also limit maximum connection lifetime).
Zero and negative values result in lifetime not being checked.
|Valid values
a|dbms.routing.driver.connection.max_lifetime, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1h+++
|===

[[config_dbms.routing.driver.connection.pool.acquisition_timeout]]
=== `dbms.routing.driver.connection.pool.acquisition_timeout`

.dbms.routing.driver.connection.pool.acquisition_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum amount of time spent attempting to acquire a connection from the connection pool.
This timeout only kicks in when all existing connections are being used and no new connections can be created because maximum connection pool size has been reached.
Error is raised when connection can't be acquired within configured time.
Negative values are allowed and result in unlimited acquisition timeout. Value of 0 is allowed and results in no timeout and immediate failure when connection is unavailable.
|Valid values
a|dbms.routing.driver.connection.pool.acquisition_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

[[config_dbms.routing.driver.connection.pool.idle_test]]
=== `dbms.routing.driver.connection.pool.idle_test`

.dbms.routing.driver.connection.pool.idle_test
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Pooled connections that have been idle in the pool for longer than this timeout will be tested before they are used again, to ensure they are still alive.
If this option is set too low, an additional network call will be incurred when acquiring a connection, which causes a performance hit.
If this is set high, no longer live connections might be used which might lead to errors.
Hence, this parameter tunes a balance between the likelihood of experiencing connection problems and performance
Normally, this parameter should not need tuning.
Value 0 means connections will always be tested for validity.
|Valid values
a|dbms.routing.driver.connection.pool.idle_test, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|No connection liveliness check is done by default.
|===

[[config_dbms.routing.driver.connection.pool.max_size]]
=== `dbms.routing.driver.connection.pool.max_size`

.dbms.routing.driver.connection.pool.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum total number of connections to be managed by a connection pool.
The limit is enforced for a combination of a host and user. Negative values are allowed and result in unlimited pool. Value of 0 is not allowed.
|Valid values
a|dbms.routing.driver.connection.pool.max_size, an integer
|Default value
m|Unlimited
|===

[[config_dbms.routing.driver.logging.level]]
=== `dbms.routing.driver.logging.level`

.dbms.routing.driver.logging.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sets level for driver internal logging.
|Valid values
a|dbms.routing.driver.logging.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|Default value
m|Value of dbms.logs.debug.level
|===

[[config_dbms.routing.enabled]]
=== `dbms.routing.enabled`

.dbms.routing.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable server-side routing in clusters using an additional bolt connector.
When configured, this allows requests to be forwarded from one cluster member to another, if the requests can't be satisfied by the first member (e.g. write requests received by a non-leader).
|Valid values
a|dbms.routing.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.routing.listen_address]]
=== `dbms.routing.listen_address`

.dbms.routing.listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The address the routing connector should bind to.
|Valid values
a|dbms.routing.listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:7688+++
|===

[[config_dbms.routing_ttl]]
=== `dbms.routing_ttl`

.dbms.routing_ttl
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|How long callers should cache the response of the routing procedure `dbms.routing.getRoutingTable()`
|Valid values
a|dbms.routing_ttl, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1s`
|Default value
m|+++5m+++
|===

[[config_dbms.connector.bolt.advertised_address]]
=== `dbms.connector.bolt.advertised_address`

.dbms.connector.bolt.advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised address for this connector.
|Valid values
a|dbms.connector.bolt.advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:7687+++
|===

[[config_dbms.connector.bolt.connection_keep_alive]]
=== `dbms.connector.bolt.connection_keep_alive`

.dbms.connector.bolt.connection_keep_alive
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum time to wait before sending a NOOP on connections waiting for responses from active ongoing queries.The minimum value is 1 millisecond.
|Valid values
a|dbms.connector.bolt.connection_keep_alive, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1ms`
|Default value
m|1m
|===

[[config_dbms.connector.bolt.connection_keep_alive_for_requests]]
=== `dbms.connector.bolt.connection_keep_alive_for_requests`

.dbms.connector.bolt.connection_keep_alive_for_requests
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The type of messages to enable keep-alive messages for (ALL, STREAMING or OFF)
|Valid values
a|dbms.connector.bolt.connection_keep_alive_for_requests, one of [ALL, STREAMING, OFF]
|Default value
m|STREAMING
|===

[[config_dbms.connector.bolt.connection_keep_alive_probes]]
=== `dbms.connector.bolt.connection_keep_alive_probes`

.dbms.connector.bolt.connection_keep_alive_probes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The total amount of probes to be missed before a connection is considered stale.The minimum for this value is 1.
|Valid values
a|dbms.connector.bolt.connection_keep_alive_probes, an integer which is minimum `1`
|Default value
m|2
|===

[[config_dbms.connector.bolt.connection_keep_alive_streaming_scheduling_interval]]
=== `dbms.connector.bolt.connection_keep_alive_streaming_scheduling_interval`

.dbms.connector.bolt.connection_keep_alive_streaming_scheduling_interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The interval between every scheduled keep-alive check on all connections with active queries. Zero duration turns off keep-alive service.
|Valid values
a|dbms.connector.bolt.connection_keep_alive_streaming_scheduling_interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `0s`
|Default value
m|1m
|===

[[config_dbms.connector.bolt.enabled]]
=== `dbms.connector.bolt.enabled`

.dbms.connector.bolt.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable the bolt connector.
|Valid values
a|dbms.connector.bolt.enabled, a boolean
|Default value
m|true
|===

[[config_dbms.connector.bolt.listen_address]]
=== `dbms.connector.bolt.listen_address`

.dbms.connector.bolt.listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Address the connector should bind to.
|Valid values
a|dbms.connector.bolt.listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:7687+++
|===

[[config_dbms.connector.bolt.ocsp_stapling_enabled]]
=== `dbms.connector.bolt.ocsp_stapling_enabled`

.dbms.connector.bolt.ocsp_stapling_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable server OCSP stapling for bolt and http connectors.
|Valid values
a|dbms.connector.bolt.ocsp_stapling_enabled, a boolean
|Default value
m|false
|===

[[config_dbms.connector.bolt.thread_pool_keep_alive]]
=== `dbms.connector.bolt.thread_pool_keep_alive`

.dbms.connector.bolt.thread_pool_keep_alive
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum time an idle thread in the thread pool bound to this connector will wait for new tasks.
|Valid values
a|dbms.connector.bolt.thread_pool_keep_alive, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5m+++
|===

[[config_dbms.connector.bolt.thread_pool_max_size]]
=== `dbms.connector.bolt.thread_pool_max_size`

.dbms.connector.bolt.thread_pool_max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of threads allowed in the thread pool bound to this connector.
|Valid values
a|dbms.connector.bolt.thread_pool_max_size, an integer
|Default value
m|+++400+++
|===

[[config_dbms.connector.bolt.thread_pool_min_size]]
=== `dbms.connector.bolt.thread_pool_min_size`

.dbms.connector.bolt.thread_pool_min_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The number of threads to keep in the thread pool bound to this connector, even if they are idle.
|Valid values
a|dbms.connector.bolt.thread_pool_min_size, an integer
|Default value
m|+++5+++
|===

[[config_dbms.connector.bolt.tls_level]]
=== `dbms.connector.bolt.tls_level`

.dbms.connector.bolt.tls_level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Encryption level to require this connector to use.
|Valid values
a|dbms.connector.bolt.tls_level, one of [REQUIRED, OPTIONAL, DISABLED]
|Default value
m|+++DISABLED+++
|===

[[config_dbms.connector.bolt.unsupported_thread_pool_shutdown_wait_time]]
=== `dbms.connector.bolt.unsupported_thread_pool_shutdown_wait_time`

.dbms.connector.bolt.unsupported_thread_pool_shutdown_wait_time
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum time to wait for the thread pool to finish processing its pending jobs and shutdown.
|Valid values
a|dbms.connector.bolt.unsupported_thread_pool_shutdown_wait_time, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5s+++
|===

[[config_dbms.connector.http.advertised_address]]
=== `dbms.connector.http.advertised_address`

.dbms.connector.http.advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised address for this connector.
|Valid values
a|dbms.connector.http.advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:7474+++
|===

[[config_dbms.connector.http.enabled]]
=== `dbms.connector.http.enabled`

.dbms.connector.http.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable the http connector.
|Valid values
a|dbms.connector.http.enabled, a boolean
|Default value
m|true
|===

[[config_dbms.connector.http.listen_address]]
=== `dbms.connector.http.listen_address`

.dbms.connector.http.listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Address the connector should bind to.
|Valid values
a|dbms.connector.http.listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:7474+++
|===

[[config_dbms.connector.https.advertised_address]]
=== `dbms.connector.https.advertised_address`

.dbms.connector.https.advertised_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Advertised address for this connector.
|Valid values
a|dbms.connector.https.advertised_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_advertised_address
|Default value
m|+++:7473+++
|===

[[config_dbms.connector.https.enabled]]
=== `dbms.connector.https.enabled`

.dbms.connector.https.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable the https connector.
|Valid values
a|dbms.connector.https.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.connector.https.listen_address]]
=== `dbms.connector.https.listen_address`

.dbms.connector.https.listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Address the connector should bind to.
|Valid values
a|dbms.connector.https.listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:7473+++
|===

== Cypher settings

[[config_cypher.default_language_version]]
=== `cypher.default_language_version`

.cypher.default_language_version
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set this to specify the default parser (language version).
|Valid values
a|cypher.default_language_version, one of [default, 3.5, 4.3, 4.4]
|Default value
m|+++default+++
|===

[[config_cypher.forbid_exhaustive_shortestpath]]
=== `cypher.forbid_exhaustive_shortestpath`

.cypher.forbid_exhaustive_shortestpath
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|This setting is associated with performance optimization. Set this to `true` in situations where it is preferable to have any queries using the 'shortestPath' function terminate as soon as possible with no answer, rather than potentially running for a long time attempting to find an answer (even if there is no path to be found). For most queries, the 'shortestPath' algorithm will return the correct answer very quickly. However there are some cases where it is possible that the fast bidirectional breadth-first search algorithm will find no results even if they exist. This can happen when the predicates in the `WHERE` clause applied to 'shortestPath' cannot be applied to each step of the traversal, and can only be applied to the entire path. When the query planner detects these special cases, it will plan to perform an exhaustive depth-first search if the fast algorithm finds no paths. However, the exhaustive search may be orders of magnitude slower than the fast algorithm. If it is critical that queries terminate as soon as possible, it is recommended that this option be set to `true`, which means that Neo4j will never consider using the exhaustive search for shortestPath queries. However, please note that if no paths are found, an error will be thrown at run time, which will need to be handled by the application.
|Valid values
a|cypher.forbid_exhaustive_shortestpath, a boolean
|Default value
m|+++false+++
|===

[[config_cypher.forbid_shortestpath_common_nodes]]
=== `cypher.forbid_shortestpath_common_nodes`

.cypher.forbid_shortestpath_common_nodes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|This setting is associated with performance optimization. The shortest path algorithm does not work when the start and end nodes are the same. With this setting set to `false` no path will be returned when that happens. The default value of `true` will instead throw an exception. This can happen if you perform a shortestPath search after a cartesian product that might have the same start and end nodes for some of the rows passed to shortestPath. If it is preferable to not experience this exception, and acceptable for results to be missing for those rows, then set this to `false`. If you cannot accept missing results, and really want the shortestPath between two common nodes, then re-write the query using a standard Cypher variable length pattern expression followed by ordering by path length and limiting to one result.
|Valid values
a|cypher.forbid_shortestpath_common_nodes, a boolean
|Default value
m|+++true+++
|===

[[config_cypher.hints_error]]
=== `cypher.hints_error`

.cypher.hints_error
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set this to specify the behavior when Cypher planner or runtime hints cannot be fulfilled. If true, then non-conformance will result in an error, otherwise only a warning is generated.
|Valid values
a|cypher.hints_error, a boolean
|Default value
m|+++false+++
|===

[[config_cypher.lenient_create_relationship]]
=== `cypher.lenient_create_relationship`

.cypher.lenient_create_relationship
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set this to change the behavior for Cypher create relationship when the start or end node is missing. By default this fails the query and stops execution, but by setting this flag the create operation is simply not performed and execution continues.
|Valid values
a|cypher.lenient_create_relationship, a boolean
|Default value
m|+++false+++
|===

[[config_cypher.min_replan_interval]]
=== `cypher.min_replan_interval`

.cypher.min_replan_interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The minimum time between possible cypher query replanning events. After this time, the graph statistics will be evaluated, and if they have changed by more than the value set by <<config_cypher.statistics_divergence_threshold,cypher.statistics_divergence_threshold>>, the query will be replanned. If the statistics have not changed sufficiently, the same interval will need to pass before the statistics will be evaluated again. Each time they are evaluated, the divergence threshold will be reduced slightly until it reaches 10% after 7h, so that even moderately changing databases will see query replanning after a sufficiently long time interval.
|Valid values
a|cypher.min_replan_interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_cypher.planner]]
=== `cypher.planner`

.cypher.planner
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set this to specify the default planner for the default language version.
|Valid values
a|cypher.planner, one of [DEFAULT, COST]
|Default value
m|+++DEFAULT+++
|===

[[config_cypher.statistics_divergence_threshold]]
=== `cypher.statistics_divergence_threshold`

.cypher.statistics_divergence_threshold
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The threshold for statistics above which a plan is considered stale.

If any of the underlying statistics used to create the plan have changed more than this value, the plan will be considered stale and will be replanned. Change is calculated as `abs(a-b)/max(a,b)`.

This means that a value of `0.75` requires the database to quadruple in size before query replanning. A value of `0` means that the query will be replanned as soon as there is any change in statistics and the replan interval has elapsed.

This interval is defined by `<<config_cypher.min_replan_interval,cypher.min_replan_interval>>` and defaults to 10s. After this interval, the divergence threshold will slowly start to decline, reaching 10% after about 7h. This will ensure that long running databases will still get query replanning on even modest changes, while not replanning frequently unless the changes are very large.
|Valid values
a|cypher.statistics_divergence_threshold, a double which is in the range `0.0` to `1.0`
|Default value
m|+++0.75+++
|===

== Database settings

[[config_dbms.filewatcher.enabled]]
=== `dbms.filewatcher.enabled`

.dbms.filewatcher.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Allows the enabling or disabling of the file watcher service. This is an auxiliary service but should be left enabled in almost all cases.
|Valid values
a|dbms.filewatcher.enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.record_format]]
=== `dbms.record_format`

.dbms.record_format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Database record format. Valid values are blank(no value, default), `standard`, `aligned`, or `high_limit`. Specifying a value will force new databases to that format and existing databases to migrate if `<<config_dbms.allow_upgrade,dbms.allow_upgrade>>=true` is specified. The `aligned` format is essentially the `standard` format with some minimal padding at the end of pages such that a single record will never cross a page boundary. The `high_limit` format is available for Enterprise Edition only. It is required if you have a graph that is larger than 34 billion nodes, 34 billion relationships, or 68 billion properties. A change of the record format is irreversible. Certain operations may suffer from a performance penalty of up to 10%, which is why this format is not switched on by default. However, if you want to change the configured record format value, you must also set `<<config_dbms.allow_upgrade,dbms.allow_upgrade>>=true`, because the setting implies a one-way store format migration.
|Valid values
a|dbms.record_format, a string
|Default value
m|`aligned` for new databases. Existing databases stay on their current format.
|===

[[config_dbms.relationship_grouping_threshold]]
=== `dbms.relationship_grouping_threshold`

.dbms.relationship_grouping_threshold
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Relationship count threshold for considering a node to be dense.
|Valid values
a|dbms.relationship_grouping_threshold, an integer which is minimum `1`
|Default value
m|+++50+++
|===

[[config_dbms.store.files.preallocate]]
=== `dbms.store.files.preallocate`

.dbms.store.files.preallocate
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Specify if Neo4j should try to preallocate store files as they grow.
|Valid values
a|dbms.store.files.preallocate, a boolean
|Default value
m|+++true+++
|===

[[config_db.temporal.timezone]]
=== `db.temporal.timezone`

.db.temporal.timezone
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Database timezone for temporal functions. All Time and DateTime values that are created without an explicit timezone will use this configured default timezone.
|Valid values
a|db.temporal.timezone, a string describing a timezone, either described by offset (e.g. `+02:00`) or by name (e.g. `Europe/Stockholm`)
|Default value
m|+++Z+++
|===

[[config_dbms.track_query_cpu_time]]
=== `dbms.track_query_cpu_time`

label:enterprise-edition[Enterprise Edition] label:dynamic[Dynamic]

.dbms.track_query_cpu_time
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enables or disables tracking of how much time a query spends actively executing on the CPU. Calling `dbms.listQueries` will display the time. +
This can also be logged in the query log by using `dbms.logs.query.time_logging_enabled`. .
|Valid values
a|dbms.track_query_cpu_time, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.track_query_allocation]]
=== `dbms.track_query_allocation`

label:dynamic[Dynamic]

.dbms.track_query_allocation
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enables or disables tracking of how many bytes are allocated by the execution of a query. If enabled, calling `dbms.listQueries` will display the allocated bytes. This can also be logged in the query log by using `<<config_dbms.logs.query.allocation_logging_enabled,dbms.logs.query.allocation_logging_enabled>>`.
|Valid values
a|dbms.track_query_allocation, a boolean
|Default value
m|+++true+++
|===

== DBMS settings

[[config_dbms.backup.enabled]]
=== `dbms.backup.enabled`

.dbms.backup.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable support for running online backups.
|Valid values
a|dbms.backup.enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.backup.incremental.strategy]]
=== `dbms.backup.incremental.strategy`

label:dynamic[Dynamic]

.dbms.backup.incremental.strategy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Strategy for incremental backup. START_TIME means that this server will send transactions until the time of when the backup started has been reached. UNBOUNDED will keep sending until all committed transactions have been sent, even if they where committed after the backup job started.
|Valid values
a|dbms.backup.incremental.strategy, one of [UNBOUNDED, START_TIME]
|Default value
m|+++UNBOUNDED+++
|===

[[config_dbms.backup.listen_address]]
=== `dbms.backup.listen_address`

.dbms.backup.listen_address
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Network interface and port for the backup server to listen on.
|Valid values
a|dbms.backup.listen_address, a socket address in the format 'hostname:port', 'hostname' or ':port'
|Default value
m|+++127.0.0.1:6362+++
|===

[[config_dbms.config.strict_validation]]
=== `dbms.config.strict_validation`

.dbms.config.strict_validation
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A strict configuration validation will prevent the database from starting up if unknown configuration options are specified in the neo4j settings namespace (such as dbms., cypher., etc).
|Valid values
a|dbms.config.strict_validation, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.databases.default_to_read_only]]
=== `dbms.databases.default_to_read_only`

label:dynamic[Dynamic]

.dbms.databases.default_to_read_only
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Whether or not any database on this instance are read_only by default. If false, individual databases may be marked as read_only using dbms.database.read_only. If true, individual databases may be marked as writable using <<config_dbms.databases.writable,dbms.databases.writable>>.
|Valid values
a|dbms.databases.default_to_read_only, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.databases.read_only]]
=== `dbms.databases.read_only`

label:dynamic[Dynamic]

.dbms.databases.read_only
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|List of databases for which to prevent write queries. Databases not included in this list maybe read_only anyway depending upon the value of <<config_dbms.databases.default_to_read_only,dbms.databases.default_to_read_only>>.
|Valid values
a|dbms.databases.read_only, a ',' separated set with elements of type 'A valid database name containing only alphabetic characters, numbers, dots and dashes with a length between 3 and 63 characters, starting with an alphabetic character but not with the name 'system''. which Value 'system' can't be included in read only databases collection!
|Default value
m|++++++
|===

[[config_dbms.databases.writable]]
=== `dbms.databases.writable`

label:dynamic[Dynamic]

.dbms.databases.writable
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|List of databases for which to allow write queries. Databases not included in this list will allow write queries anyway, unless <<config_dbms.databases.default_to_read_only,dbms.databases.default_to_read_only>> is set to true.
|Valid values
a|dbms.databases.writable, a ',' separated set with elements of type 'A valid database name containing only alphabetic characters, numbers, dots and dashes with a length between 3 and 63 characters, starting with an alphabetic character but not with the name 'system''.
|Default value
m|++++++
|===

[[config_dbms.dynamic.setting.allowlist]]
=== `dbms.dynamic.setting.allowlist`

.dbms.dynamic.setting.allowlist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of setting name patterns (comma separated) that are allowed to be dynamically changed. The list may contain both full setting names, and partial names with the wildcard '*'. If this setting is left empty all dynamic settings updates will be blocked.
|Valid values
a|dbms.dynamic.setting.allowlist, a ',' separated list with elements of type 'a string'.
|Default value
m|+++*+++
|===

[[config_dbms.dynamic.setting.whitelist]]
=== `dbms.dynamic.setting.whitelist`

label:deprecated[Deprecated in 4.2]

.dbms.dynamic.setting.whitelist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of setting name patterns (comma separated) that are allowed to be dynamically changed. The list may contain both full setting names, and partial names with the wildcard '*'. If this setting is left empty all dynamic settings updates will be blocked.
|Valid values
a|dbms.dynamic.setting.whitelist, a ',' separated list with elements of type 'a string'.
|Replaced by a|<<config_dbms.dynamic.setting.allowlist,dbms.dynamic.setting.allowlist>>
|Default value
m|+++*+++
|===

[[config_dbms.jvm.additional]]
=== `dbms.jvm.additional`

.dbms.jvm.additional
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Additional JVM arguments. Argument order can be significant. To use a Java commercial feature, the argument to unlock commercial features must precede the argument to enable the specific feature in the config value string.
|Valid values
a|dbms.jvm.additional, one or more jvm arguments
|===

[[config_dbms.panic.shutdown_on_panic]]
=== `dbms.panic.shutdown_on_panic`

label:enterprise-edition[Enterprise Edition]

.dbms.panic.shutdown_on_panic
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|If there is a Database Management System Panic (an irrecoverable error) should the neo4j process shut down or continue running. Following a DbMS panic it is likely that a significant amount of functionality will be lost. Recovering full functionality will require a Neo4j restart. This feature is available in Neo4j Enterprise Edition.
|Valid values
a|dbms.panic.shutdown_on_panic, a boolean
|Default value
m|`false` except for Neo4j Enterprise Edition deployments running on Kubernetes where it is `true`.
|===

[[config_dbms.threads.worker_count]]
=== `dbms.threads.worker_count`

.dbms.threads.worker_count
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Number of Neo4j worker threads. This setting is only valid for REST, and does not influence bolt-server. It sets the amount of worker threads for the Jetty server used by neo4j-server. This option can be tuned when you plan to execute multiple, concurrent REST requests, with the aim of getting more throughput from the database. Your OS might enforce a lower limit than the maximum value specified here.
|Valid values
a|dbms.threads.worker_count, an integer which is in the range `1` to `44738`
|Default value
m|Number of available processors, or 500 for machines which have more than 500 processors.
|===

[[config_dbms.unmanaged_extension_classes]]
=== `dbms.unmanaged_extension_classes`

.dbms.unmanaged_extension_classes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Comma-separated list of <classname>=<mount point> for unmanaged extensions.
|Valid values
a|dbms.unmanaged_extension_classes, a ',' separated list with elements of type '<classname>=<mount point> string'.
|Default value
m|++++++
|===

[[config_dbms.upgrade_max_processors]]
=== `dbms.upgrade_max_processors`

label:dynamic[Dynamic]

.dbms.upgrade_max_processors
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Max number of processors used when upgrading the store. Defaults to the number of processors available to the JVM. There is a certain amount of minimum threads needed so for that reason there is no lower bound for this value. For optimal performance this value shouldn't be greater than the number of available processors.
|Valid values
a|dbms.upgrade_max_processors, an integer which is minimum `0`
|Default value
m|+++0+++
|===

[[config_dbms.windows_service_name]]
=== `dbms.windows_service_name`

.dbms.windows_service_name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name of the Windows Service managing Neo4j when installed using `neo4j install-service`. Only applicable on Windows OS. Note: This must be unique for each individual installation.
|Valid values
a|dbms.windows_service_name, a string
|Default value
m|+++neo4j+++
|===

[[config_dbms.default_database]]
=== `dbms.default_database`

.dbms.default_database
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name of the default database (aliases are not supported).
|Valid values
a|dbms.default_database, A valid database name containing only alphabetic characters, numbers, dots and dashes with a length between 3 and 63 characters, starting with an alphabetic character but not with the name 'system'
|Default value
m|+++neo4j+++
|===

[[config_dbms.db.timezone]]
=== `dbms.db.timezone`

.dbms.db.timezone
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Database timezone. Among other things, this setting influences which timezone the logs and monitoring procedures use.
|Valid values
a|dbms.db.timezone, one of [UTC, SYSTEM]
|Default value
m|+++UTC+++
|===

[[config_dbms.max_databases]]
=== `dbms.max_databases`

.dbms.max_databases
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of databases.
|Valid values
a|dbms.max_databases, a long which is minimum `2`
|Default value
m|+++100+++
|===

[[config_dbms.mode]]
=== `dbms.mode`

.dbms.mode
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the operating mode of the database -- 'SINGLE' for stand-alone operation, 'CORE' for operating as a core member of a Causal Cluster, or 'READ_REPLICA' for operating as a read replica member of a Causal Cluster. Only SINGLE mode is allowed in Community.
|Valid values
a|dbms.mode, one of [SINGLE, CORE, READ_REPLICA]
|Default value
m|+++SINGLE+++
|===

[[config_dbms.read_only]]
=== `dbms.read_only`

label:deprecated[Deprecated]

.dbms.read_only
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Only allow read operations from this Neo4j instance. This mode still requires write access to the directory for lock purposes.
|Valid values
a|dbms.read_only, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_dbms.databases.default_to_read_only,dbms.databases.default_to_read_only>>, <<config_dbms.databases.read_only,dbms.databases.read_only>>, <<config_dbms.databases.writable,dbms.databases.writable>>
|===

[[config_dbms.clustering.enable]]
=== `dbms.clustering.enable`

label:deprecated[Deprecated]

.dbms.clustering.enable
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable discovery service and a catchup server to be started on an Enterprise Standalone Instance '<<config_dbms.mode,dbms.mode>>=SINGLE', and with that allow for Read Replicas to connect and pull transaction from it. When '<<config_dbms.mode,dbms.mode>>' is clustered (CORE, READ_REPLICA) this setting is not recognized.
|Valid values
a|dbms.clustering.enable, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.allow_single_automatic_upgrade]]
.dbms.allow_single_automatic_upgrade label:dynamic[Dynamic]
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Whether to allow a system graph upgrade to happen automatically in single instance mode (<<config_dbms.mode,dbms.mode>>=SINGLE). Default is true. In clustering environments no automatic upgrade will happen (<<config_dbms.mode,dbms.mode>>=CORE or <<config_dbms.mode,dbms.mode>>=READ_REPLICA). If set to false, or when in a clustering environment, it is necessary to call the procedure `dbms.upgrade()` to complete the upgrade.
|Valid values
a|dbms.allow_single_automatic_upgrade, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.allow_upgrade]]
=== `dbms.allow_upgrade`

label:dynamic[Dynamic]

.dbms.allow_upgrade
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Whether to allow a store upgrade in case the current version of the database starts against an older version of the store.
|Valid values
a|dbms.allow_upgrade, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.reconciler.max_backoff]]
=== `dbms.reconciler.max_backoff`

.dbms.reconciler.max_backoff
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the maximum amount of time to wait before retrying after the dbms fails to reconcile a database to its desired state.
|Valid values
a|dbms.reconciler.max_backoff, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1m`
|Default value
m|+++1h+++
|===

[[config_dbms.reconciler.max_parallelism]]
=== `dbms.reconciler.max_parallelism`

.dbms.reconciler.max_parallelism
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the level of parallelism employed by the reconciler. By default the parallelism equals the number of available processors or 8 (whichever is smaller). If configured as 0, the parallelism of the reconciler will be unbounded.
|Valid values
a|dbms.reconciler.max_parallelism, an integer which is minimum `0`
|Default value
m|+++8+++
|===

[[config_dbms.reconciler.may_retry]]
=== `dbms.reconciler.may_retry`

.dbms.reconciler.may_retry
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines whether the dbms may retry reconciling a database to its desired state.
|Valid values
a|dbms.reconciler.may_retry, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.reconciler.min_backoff]]
=== `dbms.reconciler.min_backoff`

.dbms.reconciler.min_backoff
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the minimum amount of time to wait before retrying after the dbms fails to reconcile a database to its desired state.
|Valid values
a|dbms.reconciler.min_backoff, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1s`
|Default value
m|+++2s+++
|===

[[config_dbms.directories.cluster_state]]
=== `dbms.directories.cluster_state`

.dbms.directories.cluster_state
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Directory to hold cluster state including Raft log.
|Valid values
a|dbms.directories.cluster_state, a path. If relative it is resolved from dbms.directories.data
|Default value
m|+++cluster-state+++
|===

[[config_dbms.directories.data]]
=== `dbms.directories.data`

.dbms.directories.data
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path of the data directory. You must not configure more than one Neo4j installation to use the same data directory.
|Valid values
a|dbms.directories.data, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++data+++
|===

[[config_dbms.directories.dumps.root]]
=== `dbms.directories.dumps.root`

.dbms.directories.dumps.root
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Root location where Neo4j will store database dumps optionally produced when dropping said databases.
|Valid values
a|dbms.directories.dumps.root, a path. If relative it is resolved from dbms.directories.data
|Default value
m|+++dumps+++
|===

[[config_dbms.directories.import]]
=== `dbms.directories.import`

.dbms.directories.import
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sets the root directory for file URLs used with the Cypher `LOAD CSV` clause. This should be set to a directory relative to the Neo4j installation path, restricting access to only those files within that directory and its subdirectories. For example the value "import" will only enable access to files within the 'import' folder. Removing this setting will disable the security feature, allowing all files in the local system to be imported. Setting this to an empty field will allow access to all files within the Neo4j installation folder.
|Valid values
a|dbms.directories.import, a path. If relative it is resolved from dbms.directories.neo4j_home
|===

[[config_dbms.directories.lib]]
=== `dbms.directories.lib`

.dbms.directories.lib
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path of the lib directory.
|Valid values
a|dbms.directories.lib, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++lib+++
|===

[[config_dbms.directories.licenses]]
=== `dbms.directories.licenses`

.dbms.directories.licenses
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path of the licenses directory.
|Valid values
a|dbms.directories.licenses, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++licenses+++
|===

[[config_dbms.directories.logs]]
=== `dbms.directories.logs`

.dbms.directories.logs
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path of the logs directory.
|Valid values
a|dbms.directories.logs, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++logs+++
|===

[[config_dbms.directories.metrics]]
=== `dbms.directories.metrics`

.dbms.directories.metrics
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The target location of the CSV files: a path to a directory wherein a CSV file per reported field  will be written.
|Valid values
a|dbms.directories.metrics, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++metrics+++
|===

[[config_dbms.directories.neo4j_home]]
=== `dbms.directories.neo4j_home`

.dbms.directories.neo4j_home
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Root relative to which directory settings are resolved.
|Valid values
a|dbms.directories.neo4j_home, a path which is absolute
|Default value
m|Defaults to current working directory
|===

[[config_dbms.directories.plugins]]
=== `dbms.directories.plugins`

.dbms.directories.plugins
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Location of the database plugin directory. Compiled Java JAR files that contain database procedures will be loaded if they are placed in this directory.
|Valid values
a|dbms.directories.plugins, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++plugins+++
|===

[[config_dbms.directories.run]]
=== `dbms.directories.run`

.dbms.directories.run
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path of the run directory. This directory holds Neo4j's runtime state, such as a pidfile when it is running in the background. The pidfile is created when starting neo4j and removed when stopping it. It may be placed on an in-memory filesystem such as tmpfs.
|Valid values
a|dbms.directories.run, a path. If relative it is resolved from dbms.directories.neo4j_home
|Default value
m|+++run+++
|===

[[config_dbms.directories.script.root]]
=== `dbms.directories.script.root`

.dbms.directories.script.root
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Root location where Neo4j will store scripts for configured databases.
|Valid values
a|dbms.directories.script.root, a path. If relative it is resolved from dbms.directories.data
|Default value
m|+++scripts+++
|===

[[config_dbms.directories.transaction.logs.root]]
=== `dbms.directories.transaction.logs.root`

.dbms.directories.transaction.logs.root
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Root location where Neo4j will store transaction logs for configured databases.
|Valid values
a|dbms.directories.transaction.logs.root, a path. If relative it is resolved from dbms.directories.data
|Default value
m|+++transactions+++
|===

== Import settings

[[config_dbms.import.csv.buffer_size]]
=== `dbms.import.csv.buffer_size`

.dbms.import.csv.buffer_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The size of the internal buffer in bytes used by `LOAD CSV`. If the csv file contains huge fields this value may have to be increased.
|Valid values
a|dbms.import.csv.buffer_size, a long which is minimum `1`
|Default value
m|+++2097152+++
|===

[[config_dbms.import.csv.legacy_quote_escaping]]
=== `dbms.import.csv.legacy_quote_escaping`

.dbms.import.csv.legacy_quote_escaping
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Selects whether to conform to the standard https://tools.ietf.org/html/rfc4180 for interpreting escaped quotation characters in CSV files loaded using `LOAD CSV`. Setting this to `false` will use the standard, interpreting repeated quotes '""' as a single in-lined quote, while `true` will use the legacy convention originally supported in Neo4j 3.0 and 3.1, allowing a backslash to include quotes in-lined in fields.
|Valid values
a|dbms.import.csv.legacy_quote_escaping, a boolean
|Default value
m|+++true+++
|===

== Index settings

[[config_dbms.index.default_schema_provider]]
=== `dbms.index.default_schema_provider`

label:deprecated[Deprecated]

.dbms.index.default_schema_provider
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Index provider to use for newly created schema indexes. An index provider may store different value types in separate physical indexes. native-btree-1.0: All value types and arrays of all value types, even composite keys, are stored in one native index. lucene+native-3.0: Like native-btree-1.0 but single property strings are stored in Lucene. A native index has faster updates, less heap and CPU usage compared to a Lucene index. A native index has some limitations around key size and slower execution of CONTAINS and ENDS WITH string index queries, compared to a Lucene index.
Deprecated: Which index provider to use will be a fully internal concern.
|Valid values
a|dbms.index.default_schema_provider, a string
|Default value
m|+++native-btree-1.0+++
|===

[[config_dbms.index.fulltext.default_analyzer]]
=== `dbms.index.fulltext.default_analyzer`

.dbms.index.fulltext.default_analyzer
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The name of the analyzer that the fulltext indexes should use by default.
|Valid values
a|dbms.index.fulltext.default_analyzer, a string
|Default value
m|+++standard-no-stop-words+++
|===

[[config_dbms.index.fulltext.eventually_consistent]]
=== `dbms.index.fulltext.eventually_consistent`

.dbms.index.fulltext.eventually_consistent
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Whether or not fulltext indexes should be eventually consistent by default or not.
|Valid values
a|dbms.index.fulltext.eventually_consistent, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.index.fulltext.eventually_consistent_index_update_queue_max_length]]
=== `dbms.index.fulltext.eventually_consistent_index_update_queue_max_length`

.dbms.index.fulltext.eventually_consistent_index_update_queue_max_length
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The eventually_consistent mode of the fulltext indexes works by queueing up index updates to be applied later in a background thread. This newBuilder sets an upper bound on how many index updates are allowed to be in this queue at any one point in time. When it is reached, the commit process will slow down and wait for the index update applier thread to make some more room in the queue.
|Valid values
a|dbms.index.fulltext.eventually_consistent_index_update_queue_max_length, an integer which is in the range `1` to `50000000`
|Default value
m|+++10000+++
|===

[[config_dbms.index_sampling.background_enabled]]
=== `dbms.index_sampling.background_enabled`

.dbms.index_sampling.background_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable or disable background index sampling.
|Valid values
a|dbms.index_sampling.background_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.index_sampling.sample_size_limit]]
=== `dbms.index_sampling.sample_size_limit`

.dbms.index_sampling.sample_size_limit
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Index sampling chunk size limit.
|Valid values
a|dbms.index_sampling.sample_size_limit, an integer which is in the range `1048576` to `2147483647`
|Default value
m|+++8388608+++
|===

[[config_dbms.index_sampling.update_percentage]]
=== `dbms.index_sampling.update_percentage`

.dbms.index_sampling.update_percentage
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Percentage of index updates of total index size required before sampling of a given index is triggered.
|Valid values
a|dbms.index_sampling.update_percentage, an integer which is minimum `0`
|Default value
m|+++5+++
|===

[[config_dbms.index_searcher_cache_size]]
=== `dbms.index_searcher_cache_size`

label:deprecated[Deprecated]

.dbms.index_searcher_cache_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of open Lucene index searchers.
|Valid values
a|dbms.index_searcher_cache_size, an integer which is minimum `1`
|Default value
m|+++2147483647+++
|===

== Logging settings

[[config_dbms.logs.debug.format]]
=== `dbms.logs.debug.format`

.dbms.logs.debug.format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log format to use for debug log.
|Valid values
a|dbms.logs.debug.format, one of [PLAIN, JSON]. If unset the value is inherited from dbms.logs.default_format
|===

[[config_dbms.logs.debug.level]]
=== `dbms.logs.debug.level`

label:dynamic[Dynamic]

.dbms.logs.debug.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Debug log level threshold.
|Valid values
a|dbms.logs.debug.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|Default value
m|+++INFO+++
|===

[[config_dbms.logs.debug.path]]
=== `dbms.logs.debug.path`

.dbms.logs.debug.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path to the debug log file.
|Valid values
a|dbms.logs.debug.path, a path. If relative it is resolved from dbms.directories.logs
|Default value
m|+++debug.log+++
|===

[[config_dbms.logs.debug.rotation.delay]]
=== `dbms.logs.debug.rotation.delay`

label:deprecated[Deprecated]

.dbms.logs.debug.rotation.delay
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Minimum time interval after last rotation of the debug log before it may be rotated again.
|Valid values
a|dbms.logs.debug.rotation.delay, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5m+++
|===

[[config_dbms.logs.debug.rotation.keep_number]]
=== `dbms.logs.debug.rotation.keep_number`

.dbms.logs.debug.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of history files for the debug log.
|Valid values
a|dbms.logs.debug.rotation.keep_number, an integer which is minimum `1`
|Default value
m|+++7+++
|===

[[config_dbms.logs.debug.rotation.size]]
=== `dbms.logs.debug.rotation.size`

.dbms.logs.debug.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Threshold for rotation of the debug log.
|Valid values
a|dbms.logs.debug.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++20.00MiB+++
|===

[[config_dbms.logs.default_format]]
=== `dbms.logs.default_format`

.dbms.logs.default_format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Default log format. Will apply to all logs unless overridden.
|Valid values
a|dbms.logs.default_format, one of [PLAIN, JSON]
|Default value
m|+++PLAIN+++
|===

[[config_dbms.logs.gc.enabled]]
=== `dbms.logs.gc.enabled`

.dbms.logs.gc.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable GC Logging.
|Valid values
a|dbms.logs.gc.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.gc.options]]
=== `dbms.logs.gc.options`

.dbms.logs.gc.options
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|GC Logging Options.
|Valid values
a|dbms.logs.gc.options, a string
|Default value
m|+++-Xlog:gc*,safepoint,age*=trace+++
|===

[[config_dbms.logs.gc.rotation.keep_number]]
=== `dbms.logs.gc.rotation.keep_number`

.dbms.logs.gc.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Number of GC logs to keep.
|Valid values
a|dbms.logs.gc.rotation.keep_number, an integer
|Default value
m|+++5+++
|===

[[config_dbms.logs.gc.rotation.size]]
=== `dbms.logs.gc.rotation.size`

.dbms.logs.gc.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Size of each GC log that is kept.
|Valid values
a|dbms.logs.gc.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|Default value
m|+++20.00MiB+++
|===

[[config_dbms.logs.http.enabled]]
=== `dbms.logs.http.enabled`

.dbms.logs.http.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable HTTP request logging.
|Valid values
a|dbms.logs.http.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.http.format]]
=== `dbms.logs.http.format`

.dbms.logs.http.format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log format to use for http logs.
|Valid values
a|dbms.logs.http.format, one of [PLAIN, JSON]. If unset the value is inherited from dbms.logs.default_format
|===

[[config_dbms.logs.http.path]]
=== `dbms.logs.http.path`

.dbms.logs.http.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path to HTTP request log.
|Valid values
a|dbms.logs.http.path, a path. If relative it is resolved from dbms.directories.logs
|Default value
m|+++http.log+++
|===

[[config_dbms.logs.http.rotation.keep_number]]
=== `dbms.logs.http.rotation.keep_number`

.dbms.logs.http.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Number of HTTP logs to keep.
|Valid values
a|dbms.logs.http.rotation.keep_number, an integer
|Default value
m|+++5+++
|===

[[config_dbms.logs.http.rotation.size]]
=== `dbms.logs.http.rotation.size`

.dbms.logs.http.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Size of each HTTP log that is kept.
|Valid values
a|dbms.logs.http.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++20.00MiB+++
|===

[[config_dbms.logs.query.allocation_logging_enabled]]
=== `dbms.logs.query.allocation_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.allocation_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log allocated bytes for the executed queries being logged. The logged number is cumulative over the duration of the query, i.e. for memory intense or long-running queries the value may be larger than the current memory allocation. Requires `<<config_dbms.track_query_allocation,dbms.track_query_allocation>>=true`
|Valid values
a|dbms.logs.query.allocation_logging_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.logs.query.early_raw_logging_enabled]]
=== `dbms.logs.query.early_raw_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.early_raw_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log query text and parameters without obfuscating passwords. This allows queries to be logged earlier before parsing starts.
|Valid values
a|dbms.logs.query.early_raw_logging_enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.query.enabled]]
=== `dbms.logs.query.enabled`

label:dynamic[Dynamic] label:enterprise-edition[Enterprise Edition]

.dbms.logs.query.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log executed queries. Valid values are `OFF`, `INFO`, or `VERBOSE`.

`OFF`::  no logging.
`INFO`:: log queries at the end of execution, that take longer than the configured threshold, `<<config_dbms.logs.query.threshold,dbms.logs.query.threshold>>`.
`VERBOSE`:: log queries at the start and end of execution, regardless of `<<config_dbms.logs.query.threshold,dbms.logs.query.threshold>>`.

Log entries are written to the query log (<<config_dbms.logs.query.path,dbms.logs.query.path>>).

This feature is available in the Neo4j Enterprise Edition.
|Valid values
a|dbms.logs.query.enabled, one of [OFF, INFO, VERBOSE]
|Default value
m|+++VERBOSE+++
|===

[[config_dbms.logs.query.format]]
=== `dbms.logs.query.format`

.dbms.logs.query.format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log format to use for the query log.
|Valid values
a|dbms.logs.query.format, one of [PLAIN, JSON]. If unset the value is inherited from dbms.logs.default_format
|===

[[config_dbms.logs.query.max_parameter_length]]
=== `dbms.logs.query.max_parameter_length`

label:dynamic[Dynamic]

.dbms.logs.query.max_parameter_length
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Sets a maximum character length use for each parameter in the log. This only takes effect if `<<config_dbms.logs.query.parameter_logging_enabled,dbms.logs.query.parameter_logging_enabled>> = true`.
|Valid values
a|dbms.logs.query.max_parameter_length, an integer
|Default value
m|+++2147483647+++
|===

[[config_dbms.logs.query.obfuscate_literals]]
=== `dbms.logs.query.obfuscate_literals`

label:dynamic[Dynamic]

.dbms.logs.query.obfuscate_literals
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Obfuscates all literals of the query before writing to the log. Note that node labels, relationship types and map property keys are still shown. Changing the setting will not affect queries that are cached. So, if you want the switch to have immediate effect, you must also call `CALL db.clearQueryCaches()`.
|Valid values
a|dbms.logs.query.obfuscate_literals, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.query.page_logging_enabled]]
=== `dbms.logs.query.page_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.page_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log page hits and page faults for the executed queries being logged.
|Valid values
a|dbms.logs.query.page_logging_enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.query.parameter_full_entities]]
=== `dbms.logs.query.parameter_full_entities`

label:dynamic[Dynamic]

.dbms.logs.query.parameter_full_entities
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log complete parameter entities including id, labels or relationship type, and properties. If false, only the entity id will be logged. This only takes effect if `<<config_dbms.logs.query.parameter_logging_enabled,dbms.logs.query.parameter_logging_enabled>> = true`.
|Valid values
a|dbms.logs.query.parameter_full_entities, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.query.parameter_logging_enabled]]
=== `dbms.logs.query.parameter_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.parameter_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log parameters for the executed queries being logged.
|Valid values
a|dbms.logs.query.parameter_logging_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.logs.query.path]]
=== `dbms.logs.query.path`

.dbms.logs.query.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path to the query log file.
|Valid values
a|dbms.logs.query.path, a path. If relative it is resolved from dbms.directories.logs
|Default value
m|+++query.log+++
|===

[[config_dbms.logs.query.plan_description_enabled]]
=== `dbms.logs.query.plan_description_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.plan_description_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log query plan description table, useful for debugging purposes.
|Valid values
a|dbms.logs.query.plan_description_enabled, a boolean
|Default value
m|false
|===

[[config_dbms.logs.query.rotation.keep_number]]
=== `dbms.logs.query.rotation.keep_number`

label:dynamic[Dynamic]

.dbms.logs.query.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of history files for the query log.
|Valid values
a|dbms.logs.query.rotation.keep_number, an integer which is minimum `1`
|Default value
m|+++7+++
|===

[[config_dbms.logs.query.rotation.size]]
=== `dbms.logs.query.rotation.size`

label:dynamic[Dynamic]

.dbms.logs.query.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The file size in bytes at which the query log will auto-rotate. If set to zero then no rotation will occur. Accepts a binary suffix `k`, `m` or `g`.
|Valid values
a|dbms.logs.query.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++20.00MiB+++
|===

[[config_dbms.logs.query.runtime_logging_enabled]]
=== `dbms.logs.query.runtime_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.runtime_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Logs which runtime that was used to run the query.
|Valid values
a|dbms.logs.query.runtime_logging_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.logs.query.threshold]]
=== `dbms.logs.query.threshold`

label:dynamic[Dynamic]

.dbms.logs.query.threshold
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|If the execution of query takes more time than this threshold, the query is logged once completed - provided query logging is set to INFO. Defaults to 0 seconds, that is all queries are logged.
|Valid values
a|dbms.logs.query.threshold, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++0s+++
|===

[[config_dbms.logs.query.time_logging_enabled]]
=== `dbms.logs.query.time_logging_enabled`

label:dynamic[Dynamic]

.dbms.logs.query.time_logging_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log detailed time information for the executed queries being logged, such as `(planning: 92, waiting: 0)`.
|Valid values
a|dbms.logs.query.time_logging_enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.query.transaction.enabled]]
=== `dbms.logs.query.transaction.enabled`

label:dynamic[Dynamic] label:enterprise-edition[Enterprise Edition]

.dbms.logs.query.transaction.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log the start and end of a transaction. Valid values are 'OFF', 'INFO', or 'VERBOSE'.
OFF:  no logging.
INFO: log start and end of transactions that take longer than the configured threshold, <<config_dbms.logs.query.transaction.threshold,dbms.logs.query.transaction.threshold>>.
VERBOSE: log start and end of all transactions.
Log entries are written to the query log (<<config_dbms.logs.query.path,dbms.logs.query.path>>).
This feature is available in the Neo4j Enterprise Edition.
|Valid values
a|dbms.logs.query.transaction.enabled, one of [OFF, INFO, VERBOSE]
|Default value
m|+++OFF+++
|===

[[config_dbms.logs.query.transaction.threshold]]
=== `dbms.logs.query.transaction.threshold`

label:dynamic[Dynamic]

.dbms.logs.query.transaction.threshold
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|If the transaction is open for more time than this threshold, the transaction is logged once completed - provided transaction logging (<<config_dbms.logs.query.transaction.enabled,dbms.logs.query.transaction.enabled>>) is set to `INFO`. Defaults to 0 seconds (all transactions are logged).
|Valid values
a|dbms.logs.query.transaction.threshold, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++0s+++
|===

[[config_dbms.logs.query.transaction_id.enabled]]
=== `dbms.logs.query.transaction_id.enabled`

label:dynamic[Dynamic]

.dbms.logs.query.transaction_id.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log transaction ID for the executed queries.
|Valid values
a|dbms.logs.query.transaction_id.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.logs.security.format]]
=== `dbms.logs.security.format`

.dbms.logs.security.format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log format to use for security log.
|Valid values
a|dbms.logs.security.format, one of [PLAIN, JSON]. If unset the value is inherited from dbms.logs.default_format
|===

[[config_dbms.logs.security.level]]
=== `dbms.logs.security.level`

.dbms.logs.security.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Security log level threshold.
|Valid values
a|dbms.logs.security.level, one of [DEBUG, INFO, WARN, ERROR, NONE]
|Default value
m|+++INFO+++
|===

[[config_dbms.logs.security.path]]
=== `dbms.logs.security.path`

.dbms.logs.security.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path to the security log file.
|Valid values
a|dbms.logs.security.path, a path. If relative it is resolved from dbms.directories.logs
|Default value
m|+++security.log+++
|===

[[config_dbms.logs.security.rotation.delay]]
=== `dbms.logs.security.rotation.delay`

label:deprecated[Deprecated]

.dbms.logs.security.rotation.delay
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Minimum time interval after last rotation of the security log before it may be rotated again.
|Valid values
a|dbms.logs.security.rotation.delay, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5m+++
|===

[[config_dbms.logs.security.rotation.keep_number]]
=== `dbms.logs.security.rotation.keep_number`

.dbms.logs.security.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of history files for the security log.
|Valid values
a|dbms.logs.security.rotation.keep_number, an integer which is minimum `1`
|Default value
m|+++7+++
|===

[[config_dbms.logs.security.rotation.size]]
=== `dbms.logs.security.rotation.size`

.dbms.logs.security.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Threshold for rotation of the security log.
|Valid values
a|dbms.logs.security.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++20.00MiB+++
|===

[[config_dbms.logs.user.format]]
=== `dbms.logs.user.format`

.dbms.logs.user.format
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Log format to use for user log.
|Valid values
a|dbms.logs.user.format, one of [PLAIN, JSON]. If unset the value is inherited from dbms.logs.default_format
|===

[[config_dbms.logs.user.path]]
=== `dbms.logs.user.path`

.dbms.logs.user.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Path to the user log file. Note that if <<config_dbms.logs.user.stdout_enabled,dbms.logs.user.stdout_enabled>> is enabled this setting will be ignored.
|Valid values
a|dbms.logs.user.path, a path. If relative it is resolved from dbms.directories.logs
|Default value
m|+++neo4j.log+++
|===

[[config_dbms.logs.user.rotation.delay]]
=== `dbms.logs.user.rotation.delay`

label:deprecated[Deprecated]

.dbms.logs.user.rotation.delay
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Minimum time interval after last rotation of the user log (__neo4j.log__) before it may be rotated again. Note that if <<config_dbms.logs.user.stdout_enabled,dbms.logs.user.stdout_enabled>> is enabled this setting will be ignored.
|Valid values
a|dbms.logs.user.rotation.delay, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++5m+++
|===

[[config_dbms.logs.user.rotation.keep_number]]
=== `dbms.logs.user.rotation.keep_number`

.dbms.logs.user.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of history files for the user log (__neo4j.log__). Note that if <<config_dbms.logs.user.stdout_enabled,dbms.logs.user.stdout_enabled>> is enabled this setting will be ignored.
|Valid values
a|dbms.logs.user.rotation.keep_number, an integer which is minimum `1`
|Default value
m|+++7+++
|===

[[config_dbms.logs.user.rotation.size]]
=== `dbms.logs.user.rotation.size`

.dbms.logs.user.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Threshold for rotation of the user log (__neo4j.log__). If set to 0, log rotation is disabled. Note that if <<config_dbms.logs.user.stdout_enabled,dbms.logs.user.stdout_enabled>> is enabled this setting will be ignored.
|Valid values
a|dbms.logs.user.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++0B+++
|===

[[config_dbms.logs.user.stdout_enabled]]
=== `dbms.logs.user.stdout_enabled`

.dbms.logs.user.stdout_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Send user logs to the process stdout. If this is disabled then logs will instead be sent to the file __neo4j.log__ located in the logs directory.
|Valid values
a|dbms.logs.user.stdout_enabled, a boolean
|Default value
m|+++true+++
|===

== Memory settings

[[config_dbms.memory.heap.initial_size]]
=== `dbms.memory.heap.initial_size`

.dbms.memory.heap.initial_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Initial heap size. By default it is calculated based on available system resources.
|Valid values
a|dbms.memory.heap.initial_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|===

[[config_dbms.memory.heap.max_size]]
=== `dbms.memory.heap.max_size`

.dbms.memory.heap.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum heap size. By default it is calculated based on available system resources.
|Valid values
a|dbms.memory.heap.max_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`)
|===

[[config_dbms.memory.off_heap.block_cache_size]]
=== `dbms.memory.off_heap.block_cache_size`

.dbms.memory.off_heap.block_cache_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the size of the off-heap memory blocks cache. The cache will contain this number of blocks for each block size that is power of two. Thus, maximum amount of memory used by blocks cache can be calculated as 2 * <<config_dbms.memory.off_heap.max_cacheable_block_size,dbms.memory.off_heap.max_cacheable_block_size>> * `dbms.memory.off_heap.block_cache_size`
|Valid values
a|dbms.memory.off_heap.block_cache_size, an integer which is minimum `16`
|Default value
m|+++128+++
|===

[[config_dbms.memory.off_heap.max_cacheable_block_size]]
=== `dbms.memory.off_heap.max_cacheable_block_size`

.dbms.memory.off_heap.max_cacheable_block_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines the maximum size of an off-heap memory block that can be cached to speed up allocations. The value must be a power of 2.
|Valid values
a|dbms.memory.off_heap.max_cacheable_block_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `4.00KiB` and is power of 2
|Default value
m|+++512.00KiB+++
|===

[[config_dbms.memory.off_heap.max_size]]
=== `dbms.memory.off_heap.max_size`

.dbms.memory.off_heap.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum amount of off-heap memory that can be used to store transaction state data; it's a total amount of memory shared across all active transactions. Zero means 'unlimited'. Used when <<config_dbms.tx_state.memory_allocation,dbms.tx_state.memory_allocation>> is set to 'OFF_HEAP'.
|Valid values
a|dbms.memory.off_heap.max_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `0B`
|Default value
m|+++2.00GiB+++
|===

[[config_dbms.memory.pagecache.directio]]
=== `dbms.memory.pagecache.directio`

.dbms.memory.pagecache.directio
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Use direct I/O for page cache. Setting is supported only on Linux and only for a subset of record formats that use platform aligned page size.
|Valid values
a|dbms.memory.pagecache.directio, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.memory.pagecache.flush.buffer.enabled]]
=== `dbms.memory.pagecache.flush.buffer.enabled`

label:dynamic[Dynamic]

.dbms.memory.pagecache.flush.buffer.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache can be configured to use a temporal buffer for flushing purposes. It is used to combine, if possible, sequence of several cache pages into one bigger buffer to minimize the number of individual IOPS performed and better utilization of available I/O resources, especially when those are restricted.
|Valid values
a|dbms.memory.pagecache.flush.buffer.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.memory.pagecache.flush.buffer.size_in_pages]]
=== `dbms.memory.pagecache.flush.buffer.size_in_pages`

label:dynamic[Dynamic]

.dbms.memory.pagecache.flush.buffer.size_in_pages
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache can be configured to use a temporal buffer for flushing purposes. It is used to combine, if possible, sequence of several cache pages into one bigger buffer to minimize the number of individual IOPS performed and better utilization of available I/O resources, especially when those are restricted. Use this setting to configure individual file flush buffer size in pages (8KiB). To be able to utilize this buffer during page cache flushing, buffered flush should be enabled.
|Valid values
a|dbms.memory.pagecache.flush.buffer.size_in_pages, an integer which is in the range `1` to `512`
|Default value
m|+++128+++
|===

[[config_dbms.memory.pagecache.scan.prefetchers]]
=== `dbms.memory.pagecache.scan.prefetchers`

.dbms.memory.pagecache.scan.prefetchers
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of worker threads to use for pre-fetching data when doing sequential scans. Set to '0' to disable pre-fetching for scans.
|Valid values
a|dbms.memory.pagecache.scan.prefetchers, an integer which is in the range `0` to `255`
|Default value
m|+++4+++
|===

[[config_dbms.memory.pagecache.size]]
=== `dbms.memory.pagecache.size`

.dbms.memory.pagecache.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The amount of memory to use for mapping the store files, in bytes (or kilobytes with the 'k' suffix, megabytes with 'm' and gigabytes with 'g'). If Neo4j is running on a dedicated server, then it is generally recommended to leave about 2-4 gigabytes for the operating system, give the JVM enough heap to hold all your transaction state and query context, and then leave the rest for the page cache. If no page cache memory is configured, then a heuristic setting is computed based on available system resources.
|Valid values
a|dbms.memory.pagecache.size, a string
|===

[[config_dbms.memory.pagecache.swapper]]
=== `dbms.memory.pagecache.swapper`

label:deprecated[Deprecated]

.dbms.memory.pagecache.swapper
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|This setting is not used anymore.
|Valid values
a|dbms.memory.pagecache.swapper, a string
|===

[[config_dbms.memory.pagecache.warmup.enable]]
=== `dbms.memory.pagecache.warmup.enable`

label:enterprise-edition[Enterprise Edition]

.dbms.memory.pagecache.warmup.enable
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache can be configured to perform usage sampling of loaded pages that can be used to construct active load profile. According to that profile pages can be reloaded on the restart, replication, etc. This setting allows disabling that behavior.
This feature is available in Neo4j Enterprise Edition.
|Valid values
a|dbms.memory.pagecache.warmup.enable, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.memory.pagecache.warmup.preload]]
=== `dbms.memory.pagecache.warmup.preload`

.dbms.memory.pagecache.warmup.preload
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache warmup can be configured to prefetch files, preferably when cache size is bigger than store size. Files to be prefetched can be filtered by '<<config_dbms.memory.pagecache.warmup.preload.allowlist,dbms.memory.pagecache.warmup.preload.allowlist>>'. Enabling this disables warmup by profile.
|Valid values
a|dbms.memory.pagecache.warmup.preload, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.memory.pagecache.warmup.preload.allowlist]]
=== `dbms.memory.pagecache.warmup.preload.allowlist`

.dbms.memory.pagecache.warmup.preload.allowlist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache warmup prefetch file allowlist regex. By default matches all files.
|Valid values
a|dbms.memory.pagecache.warmup.preload.allowlist, a string
|Default value
m|+++.*+++
|===

[[config_dbms.memory.pagecache.warmup.preload.whitelist]]
=== `dbms.memory.pagecache.warmup.preload.whitelist`

label:deprecated[Deprecated]

.dbms.memory.pagecache.warmup.preload.whitelist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Page cache warmup prefetch file whitelist regex. By default matches all files.
|Valid values
a|dbms.memory.pagecache.warmup.preload.whitelist, a string
|Default value
m|+++.*+++
|Replaced by
a|<<config_dbms.memory.pagecache.warmup.preload.allowlist,dbms.memory.pagecache.warmup.preload.allowlist>>
|===

[[config_dbms.memory.pagecache.warmup.profile.interval]]
=== `dbms.memory.pagecache.warmup.profile.interval`

label:enterprise-edition[Enterprise Edition]

.dbms.memory.pagecache.warmup.profile.interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The profiling frequency for the page cache. Accurate profiles allow the page cache to do active warmup after a restart, reducing the mean time to performance.
This feature is available in Neo4j Enterprise Edition.
|Valid values
a|dbms.memory.pagecache.warmup.profile.interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

[[config_dbms.memory.tracking.enable]]
=== `dbms.memory.tracking.enable`

.dbms.memory.tracking.enable
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable off heap and on heap memory tracking. Should not be set to `false` for clusters.
|Valid values
a|dbms.memory.tracking.enable, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.memory.transaction.database_max_size]]
=== `dbms.memory.transaction.database_max_size`

label:dynamic[Dynamic]

.dbms.memory.transaction.database_max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limit the amount of memory that all transactions in one database can consume, in bytes (or kilobytes with the 'k' suffix, megabytes with 'm' and gigabytes with 'g'). Zero means 'unlimited'.
|Valid values
a|dbms.memory.transaction.database_max_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `10.00MiB` or is `0B`
|Default value
m|+++0B+++
|===

[[config_dbms.memory.transaction.global_max_size]]
=== `dbms.memory.transaction.global_max_size`

label:dynamic[Dynamic]

.dbms.memory.transaction.global_max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limit the amount of memory that all of the running transactions can consume, in bytes (or kilobytes with the 'k' suffix, megabytes with 'm' and gigabytes with 'g'). Zero means 'unlimited'.
|Valid values
a|dbms.memory.transaction.global_max_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `10.00MiB` or is `0B`
|Default value
m|+++0B+++
|===

[[config_dbms.memory.transaction.max_size]]
=== `dbms.memory.transaction.max_size`

label:dynamic[Dynamic]

.dbms.memory.transaction.max_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Limit the amount of memory that a single transaction can consume, in bytes (or kilobytes with the 'k' suffix, megabytes with 'm' and gigabytes with 'g'). Zero means 'largest possible value'. When `<<config_dbms.mode,dbms.mode>>=CORE` or `<<config_dbms.mode,dbms.mode>>=SINGLE` and `<<config_dbms.clustering.enable,dbms.clustering.enable>>=true` this is '2G', in other cases this is 'unlimited'.
|Valid values
a|dbms.memory.transaction.max_size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `1.00MiB` or is `0B` and depends on dbms.mode. If dbms.mode one of `[CORE]` then it is maximum `2.00GiB` otherwise it depends on dbms.mode. If dbms.mode one of `[SINGLE]` then it depends on dbms.clustering.enable. If dbms.clustering.enable is `true` then it is maximum `2.00GiB` otherwise it is unconstrained. otherwise it is unconstrained.
|Default value
m|+++0B+++
|===

[[config_dbms.tx_state.memory_allocation]]
=== `dbms.tx_state.memory_allocation`

.dbms.tx_state.memory_allocation
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines whether memory for transaction state should be allocated on- or off-heap. Note that for small transactions you can gain up to 25% write speed by setting it to `ON_HEAP`.
|Valid values
a|dbms.tx_state.memory_allocation, one of [ON_HEAP, OFF_HEAP]
|Default value
m|+++OFF_HEAP+++
|===

[[config_dbms.query_cache_size]]
=== `dbms.query_cache_size`

.dbms.query_cache_size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The number of cached Cypher query execution plans per database. The max number of query plans that can be kept in cache is the `number of databases` * ``dbms.query_cache_size``. With 10 databases and ``dbms.query_cache_size``=1000, the caches can keep 10000 plans in total on the instance, assuming that each DB receives queries that fill up its cache.
|Valid values
a|dbms.query_cache_size, an integer which is minimum `0`
|Default value
m|+++1000+++
|===

== Metrics settings

[[config_metrics.bolt.messages.enabled]]
=== `metrics.bolt.messages.enabled`

label:deprecated[Deprecated]

.metrics.bolt.messages.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about Bolt Protocol message processing.
|Valid values
a|metrics.bolt.messages.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.csv.enabled]]
=== `metrics.csv.enabled`

.metrics.csv.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set to `true` to enable exporting metrics to CSV files.
|Valid values
a|metrics.csv.enabled, a boolean
|Default value
m|+++true+++
|===

[[config_metrics.csv.interval]]
=== `metrics.csv.interval`

.metrics.csv.interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The reporting interval for the CSV files. That is, how often new rows with numbers are appended to the CSV files.
|Valid values
a|metrics.csv.interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1ms`
|Default value
m|+++30s+++
|===

[[config_metrics.csv.rotation.compression]]
=== `metrics.csv.rotation.compression`

.metrics.csv.rotation.compression
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Decides what compression to use for the csv history files.
|Valid values
a|metrics.csv.rotation.compression, one of [NONE, ZIP, GZ]
|Default value
m|+++NONE+++
|===

[[config_metrics.csv.rotation.keep_number]]
=== `metrics.csv.rotation.keep_number`

.metrics.csv.rotation.keep_number
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Maximum number of history files for the csv files.
|Valid values
a|metrics.csv.rotation.keep_number, an integer which is minimum `1`
|Default value
m|+++7+++
|===

[[config_metrics.csv.rotation.size]]
=== `metrics.csv.rotation.size`

.metrics.csv.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The file size in bytes at which the csv files will auto-rotate. If set to zero then no rotation will occur. Accepts a binary suffix `k`, `m` or `g`.
|Valid values
a|metrics.csv.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is in the range `0B` to `8388608.00TiB`
|Default value
m|+++10.00MiB+++
|===

[[config_metrics.cypher.replanning.enabled]]
=== `metrics.cypher.replanning.enabled`

label:deprecated[Deprecated]

.metrics.cypher.replanning.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about number of occurred replanning events. instead.
|Valid values
a|metrics.cypher.replanning.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.enabled]]
=== `metrics.enabled`

.metrics.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable metrics. Setting this to `false` will to turn off all metrics.
|Valid values
a|metrics.enabled, a boolean
|Default value
m|+++true+++
|===

[[config_metrics.filter]]
=== `metrics.filter`

.metrics.filter
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Specifies which metrics should be enabled by using a comma separated list of globbing patterns. Only the metrics matching the filter will be enabled. For example `\*check_point*,neo4j.page_cache.evictions` will enable any checkpoint metrics and the pagecache eviction metric.
|Valid values
a|metrics.filter, a ',' separated list with elements of type 'A simple globbing pattern that can use `*` and `?`.'.
|Default value
m|+++*bolt.connections*,*bolt.messages_received*,*bolt.messages_started*,*dbms.pool.bolt.free,*dbms.pool.bolt.total_size,*dbms.pool.bolt.total_used,*dbms.pool.bolt.used_heap,*causal_clustering.core.is_leader,*causal_clustering.core.last_leader_message,*causal_clustering.core.replication_attempt,*causal_clustering.core.replication_fail,*check_point.duration,*check_point.total_time,*cypher.replan_events,*ids_in_use*,*pool.transaction.*.total_used,*pool.transaction.*.used_heap,*pool.transaction.*.used_native,*store.size*,*transaction.active_read,*transaction.active_write,*transaction.committed*,*transaction.last_committed_tx_id,*transaction.peak_concurrent,*transaction.rollbacks*,*page_cache.hit*,*page_cache.page_faults,*page_cache.usage_ratio,*vm.file.descriptors.count,*vm.gc.time.*,*vm.heap.used,*vm.memory.buffer.direct.used,*vm.memory.pool.g1_eden_space,*vm.memory.pool.g1_old_gen,*vm.pause_time,*vm.thread*,*db.query.execution*+++
|===

[[config_metrics.graphite.enabled]]
=== `metrics.graphite.enabled`

.metrics.graphite.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set to `true` to enable exporting metrics to Graphite.
|Valid values
a|metrics.graphite.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_metrics.graphite.interval]]
=== `metrics.graphite.interval`

.metrics.graphite.interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The reporting interval for Graphite. That is, how often to send updated metrics to Graphite.
|Valid values
a|metrics.graphite.interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++30s+++
|===

[[config_metrics.graphite.server]]
=== `metrics.graphite.server`

.metrics.graphite.server
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The hostname or IP address of the Graphite server.
|Valid values
a|metrics.graphite.server, a socket address in the format 'hostname:port', 'hostname' or ':port'. If missing port or hostname it is acquired from dbms.default_listen_address
|Default value
m|+++:2003+++
|===

[[config_metrics.jmx.enabled]]
=== `metrics.jmx.enabled`

.metrics.jmx.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set to `true` to enable the JMX metrics endpoint.
|Valid values
a|metrics.jmx.enabled, a boolean
|Default value
m|+++true+++
|===

[[config_metrics.jvm.buffers.enabled]]
=== `metrics.jvm.buffers.enabled`

label:deprecated[Deprecated]

.metrics.jvm.buffers.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the buffer pools.
|Valid values
a|metrics.jvm.buffers.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.file.descriptors.enabled]]
=== `metrics.jvm.file.descriptors.enabled`

label:deprecated[Deprecated]

.metrics.jvm.file.descriptors.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the number of open file descriptors.
|Valid values
a|metrics.jvm.file.descriptors.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.gc.enabled]]
=== `metrics.jvm.gc.enabled`

label:deprecated[Deprecated]

.metrics.jvm.gc.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the duration of garbage collections.
|Valid values
a|metrics.jvm.gc.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.heap.enabled]]
=== `metrics.jvm.heap.enabled`

label:deprecated[Deprecated]

.metrics.jvm.heap.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the heap memory usage.
|Valid values
a|metrics.jvm.heap.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.memory.enabled]]
=== `metrics.jvm.memory.enabled`

label:deprecated[Deprecated]

.metrics.jvm.memory.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the memory usage.
|Valid values
a|metrics.jvm.memory.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.pause_time.enabled]]
=== `metrics.jvm.pause_time.enabled`

label:deprecated[Deprecated]

.metrics.jvm.pause_time.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the VM pause time.
|Valid values
a|metrics.jvm.pause_time.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.jvm.threads.enabled]]
=== `metrics.jvm.threads.enabled`

label:deprecated[Deprecated]

.metrics.jvm.threads.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the current number of threads running.
|Valid values
a|metrics.jvm.threads.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.namespaces.enabled]]
=== `metrics.namespaces.enabled`

.metrics.namespaces.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable metrics namespaces that separates the global and database specific metrics. If enabled all database specific metrics will have field names starting with <metrics_prefix>.database.<database_name> and all global metrics will start with <metrics_prefix>.dbms. For example, neo4j.page_cache.hits will become neo4j.dbms.page_cache.hits and neo4j.system.log.rotation_events will become neo4j.database.system.log.rotation_events.
|Valid values
a|metrics.namespaces.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_metrics.neo4j.causal_clustering.enabled]]
=== `metrics.neo4j.causal_clustering.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.causal_clustering.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about Causal Clustering mode.
|Valid values
a|metrics.neo4j.causal_clustering.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.checkpointing.enabled]]
=== `metrics.neo4j.checkpointing.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.checkpointing.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about Neo4j check pointing; when it occurs and how much time it takes to complete.
|Valid values
a|metrics.neo4j.checkpointing.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.counts.enabled]]
=== `metrics.neo4j.counts.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.counts.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about approximately how many entities are in the database; nodes, relationships, properties, etc.
|Valid values
a|metrics.neo4j.counts.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.data.counts.enabled]]
=== `metrics.neo4j.data.counts.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.data.counts.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about number of entities in the database.
|Valid values
a|metrics.neo4j.data.counts.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.database_operation_count.enabled]]
=== `metrics.neo4j.database_operation_count.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.database_operation_count.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics for Neo4j dbms operations; how many times databases have been created, started, stopped or dropped, and how many attempted operations have failed and recovered later.
|Valid values
a|metrics.neo4j.database_operation_count.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.logs.enabled]]
=== `metrics.neo4j.logs.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.logs.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the Neo4j transaction logs.
|Valid values
a|metrics.neo4j.logs.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.pagecache.enabled]]
=== `metrics.neo4j.pagecache.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.pagecache.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the Neo4j page cache; page faults, evictions, flushes, exceptions, etc.
|Valid values
a|metrics.neo4j.pagecache.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.pools.enabled]]
=== `metrics.neo4j.pools.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.pools.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about Neo4j memory pools.
|Valid values
a|metrics.neo4j.pools.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.server.enabled]]
=== `metrics.neo4j.server.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.server.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about Server threading info.
|Valid values
a|metrics.neo4j.server.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.size.enabled]]
=== `metrics.neo4j.size.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.size.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about the store size of each database.
|Valid values
a|metrics.neo4j.size.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.neo4j.tx.enabled]]
=== `metrics.neo4j.tx.enabled`

label:deprecated[Deprecated]

.metrics.neo4j.tx.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable reporting metrics about transactions; number of transactions started, committed, etc.
|Valid values
a|metrics.neo4j.tx.enabled, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_metrics.filter,metrics.filter>>
|===

[[config_metrics.prefix]]
=== `metrics.prefix`

.metrics.prefix
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A common prefix for the reported metrics field names.
|Valid values
a|metrics.prefix, a string
|Default value
m|+++neo4j+++
|===

[[config_metrics.prometheus.enabled]]
=== `metrics.prometheus.enabled`

.metrics.prometheus.enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set to `true` to enable the Prometheus endpoint.
|Valid values
a|metrics.prometheus.enabled, a boolean
|Default value
m|+++false+++
|===

[[config_metrics.prometheus.endpoint]]
=== `metrics.prometheus.endpoint`

.metrics.prometheus.endpoint
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The hostname and port to use as Prometheus endpoint.
|Valid values
a|metrics.prometheus.endpoint, a socket address in the format `hostname:port`, `hostname`, or `:port`. If missing, port and hostname are acquired from `dbms.default_listen_address`.
|Default value
m|+++localhost:2004+++
|===

== Neo4j Browser and client settings

[[config_browser.allow_outgoing_connections]]
=== `browser.allow_outgoing_connections`

.browser.allow_outgoing_connections
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the policy for outgoing Neo4j Browser connections.
|Valid values
a|browser.allow_outgoing_connections, a boolean
|Default value
m|+++true+++
|===

[[config_browser.credential_timeout]]
=== `browser.credential_timeout`

.browser.credential_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the Neo4j Browser to time out logged in users after this idle period. Setting this to 0 indicates no limit.
|Valid values
a|browser.credential_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++0s+++
|===

[[config_browser.post_connect_cmd]]
=== `browser.post_connect_cmd`

.browser.post_connect_cmd
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Commands to be run when Neo4j Browser successfully connects to this server. Separate multiple commands with semi-colon.
|Valid values
a|browser.post_connect_cmd, a string
|Default value
m|++++++
|===

[[config_browser.remote_content_hostname_whitelist]]
=== `browser.remote_content_hostname_whitelist`

.browser.remote_content_hostname_whitelist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Whitelist of hosts for the Neo4j Browser to be allowed to fetch content from.
|Valid values
a|browser.remote_content_hostname_whitelist, a string
|Default value
m|+++guides.neo4j.com,localhost+++
|===

[[config_browser.retain_connection_credentials]]
=== `browser.retain_connection_credentials`

.browser.retain_connection_credentials
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the Neo4j Browser to store or not store user credentials.
|Valid values
a|browser.retain_connection_credentials, a boolean
|Default value
m|+++true+++
|===

[[config_browser.retain_editor_history]]
=== `browser.retain_editor_history`

.browser.retain_editor_history
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure the Neo4j Browser to store or not store user editor history.
|Valid values
a|browser.retain_editor_history, a boolean
|Default value
m|+++true+++
|===

[[config_clients.allow_telemetry]]
=== `clients.allow_telemetry`

.clients.allow_telemetry
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configure client applications such as Browser and Bloom to send Product Analytics data.
|Valid values
a|clients.allow_telemetry, a boolean
|Default value
m|+++true+++
|===

== Security settings

[[config_dbms.security.allow_csv_import_from_file_urls]]
=== `dbms.security.allow_csv_import_from_file_urls`

.dbms.security.allow_csv_import_from_file_urls
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Determines if Cypher will allow using file URLs when loading data using `LOAD CSV`. Setting this value to `false` will cause Neo4j to fail `LOAD CSV` clauses that load data from the file system.
|Valid values
a|dbms.security.allow_csv_import_from_file_urls, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.security.auth_cache_max_capacity]]
=== `dbms.security.auth_cache_max_capacity`

.dbms.security.auth_cache_max_capacity
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum capacity for authentication and authorization caches (respectively).
|Valid values
a|dbms.security.auth_cache_max_capacity, an integer
|Default value
m|+++10000+++
|===

[[config_dbms.security.auth_cache_ttl]]
=== `dbms.security.auth_cache_ttl`

.dbms.security.auth_cache_ttl
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The time to live (TTL) for cached authentication and authorization info when using external auth providers (OIDC, LDAP or plugin). Setting the TTL to 0 will disable auth caching. Disabling caching while using the LDAP auth provider requires the use of an LDAP system account for resolving authorization information.
|Valid values
a|dbms.security.auth_cache_ttl, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10m+++
|===

[[config_dbms.security.auth_cache_use_ttl]]
=== `dbms.security.auth_cache_use_ttl`

.dbms.security.auth_cache_use_ttl
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable time-based eviction of the authentication and authorization info cache for external auth providers (OIDC, LDAP or plugin). Disabling this setting will make the cache live forever and only be evicted when `<<config_dbms.security.auth_cache_max_capacity,dbms.security.auth_cache_max_capacity>>` is exceeded.
|Valid values
a|dbms.security.auth_cache_use_ttl, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.security.auth_enabled]]
=== `dbms.security.auth_enabled`

.dbms.security.auth_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Enable auth requirement to access Neo4j.
|Valid values
a|dbms.security.auth_enabled, a boolean
|Default value
m|true
|===

[[config_dbms.security.auth_lock_time]]
=== `dbms.security.auth_lock_time`

.dbms.security.auth_lock_time
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The amount of time user account should be locked after a configured number of unsuccessful authentication attempts. The locked out user will not be able to log in until the lock period expires, even if correct credentials are provided. Setting this configuration option to a low value is not recommended because it might make it easier for an attacker to brute force the password.
|Valid values
a|dbms.security.auth_lock_time, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `0s`
|Default value
m|+++5s+++
|===

[[config_dbms.security.auth_max_failed_attempts]]
=== `dbms.security.auth_max_failed_attempts`

.dbms.security.auth_max_failed_attempts
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of unsuccessful authentication attempts before imposing a user lock for  the configured amount of time, as defined by `<<config_dbms.security.auth_lock_time,dbms.security.auth_lock_time>>`.The locked out user will not be able to log in until the lock period expires, even if correct  credentials are provided. Setting this configuration option to values less than 3 is not recommended because it might make  it easier for an attacker to brute force the password.
|Valid values
a|dbms.security.auth_max_failed_attempts, an integer which is minimum `0`
|Default value
m|+++3+++
|===

[[config_dbms.security.authentication_providers]]
=== `dbms.security.authentication_providers`

.dbms.security.authentication_providers
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of security authentication providers containing the users and roles. This can be any of the built-in `native` or `ldap` providers, or it can be an externally provided plugin, with a custom name prefixed by `plugin-`, i.e. `plugin-<AUTH_PROVIDER_NAME>`. They will be queried in the given order when login is attempted.
|Valid values
a|dbms.security.authentication_providers, a ',' separated list with elements of type 'a string'.
|Default value
m|+++native+++
|===

[[config_dbms.security.authorization_providers]]
=== `dbms.security.authorization_providers`

.dbms.security.authorization_providers
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of security authorization providers containing the users and roles. This can be any of the built-in `native` or `ldap` providers, or it can be an externally provided plugin, with a custom name prefixed by `plugin-`, i.e. `plugin-<AUTH_PROVIDER_NAME>`. They will be queried in the given order when login is attempted.
|Valid values
a|dbms.security.authorization_providers, a ',' separated list with elements of type 'a string'.
|Default value
m|+++native+++
|===

[[config_dbms.security.causal_clustering_status_auth_enabled]]
=== `dbms.security.causal_clustering_status_auth_enabled`

.dbms.security.causal_clustering_status_auth_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Require authorization for access to the Causal Clustering status endpoints.
|Valid values
a|dbms.security.causal_clustering_status_auth_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.security.http_access_control_allow_origin]]
=== `dbms.security.http_access_control_allow_origin`

.dbms.security.http_access_control_allow_origin
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Value of the Access-Control-Allow-Origin header sent over any HTTP or HTTPS connector. This defaults to '*', which allows broadest compatibility. Note that any URI provided here limits HTTP/HTTPS access to that URI only.
|Valid values
a|dbms.security.http_access_control_allow_origin, a string
|Default value
m|+++*+++
|===

[[config_dbms.security.http_auth_allowlist]]
=== `dbms.security.http_auth_allowlist`

.dbms.security.http_auth_allowlist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines an allowlist of http paths where Neo4j authentication is not required.
|Valid values
a|dbms.security.http_auth_allowlist, a ',' separated list with elements of type 'a string'.
|Default value
m|+++/,/browser.*+++
|===

[[config_dbms.security.http_auth_whitelist]]
=== `dbms.security.http_auth_whitelist`

label:deprecated[Deprecated]

.dbms.security.http_auth_whitelist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Defines a whitelist of http paths where Neo4j authentication is not required.
|Valid values
a|dbms.security.http_auth_whitelist, a ',' separated list with elements of type 'a string'.
|Default value
m|+++/,/browser.*+++
|Replaced by
a|<<config_dbms.security.http_auth_allowlist,dbms.security.http_auth_allowlist>>
|===

[[config_dbms.security.http_strict_transport_security]]
=== `dbms.security.http_strict_transport_security`

.dbms.security.http_strict_transport_security
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Value of the HTTP Strict-Transport-Security (HSTS) response header. This header tells browsers that a webpage should only be accessed using HTTPS instead of HTTP. It is attached to every HTTPS response. Setting is not set by default so 'Strict-Transport-Security' header is not sent. Value is expected to contain directives like 'max-age', 'includeSubDomains' and 'preload'.
|Valid values
a|dbms.security.http_strict_transport_security, a string
|===

[[config_dbms.security.ldap.authentication.attribute]]
=== `dbms.security.ldap.authentication.attribute`

label:dynamic[Dynamic]

.dbms.security.ldap.authentication.attribute
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The attribute to use when looking up users.
Using this setting requires `<<config_dbms.security.ldap.authentication.search_for_attribute,dbms.security.ldap.authentication.search_for_attribute>>` to be true and thus `<<config_dbms.security.ldap.authorization.system_username,dbms.security.ldap.authorization.system_username>>` and `<<config_dbms.security.ldap.authorization.system_password,dbms.security.ldap.authorization.system_password>>` to be configured.
|Valid values
a|dbms.security.ldap.authentication.attribute, a string which matches the pattern `[A-Za-z0-9-]*` (has to be a valid LDAP attribute name, only containing letters [A-Za-z], digits [0-9] and hyphens [-].)
|Default value
m|+++samaccountname+++
|===

[[config_dbms.security.ldap.authentication.cache_enabled]]
=== `dbms.security.ldap.authentication.cache_enabled`

.dbms.security.ldap.authentication.cache_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Determines if the result of authentication via the LDAP server should be cached or not. Caching is used to limit the number of LDAP requests that have to be made over the network for users that have already been authenticated successfully. A user can be authenticated against an existing cache entry (instead of via an LDAP server) as long as it is alive (see `<<config_dbms.security.auth_cache_ttl,dbms.security.auth_cache_ttl>>`).
An important consequence of setting this to `true` is that Neo4j then needs to cache a hashed version of the credentials in order to perform credentials matching. This hashing is done using a cryptographic hash function together with a random salt. Preferably a conscious decision should be made if this method is considered acceptable by the security standards of the organization in which this Neo4j instance is deployed.
|Valid values
a|dbms.security.ldap.authentication.cache_enabled, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.security.ldap.authentication.mechanism]]
=== `dbms.security.ldap.authentication.mechanism`

.dbms.security.ldap.authentication.mechanism
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|LDAP authentication mechanism. This is one of `simple` or a SASL mechanism supported by JNDI, for example `DIGEST-MD5`. `simple` is basic username and password authentication and SASL is used for more advanced mechanisms. See RFC 2251 LDAPv3 documentation for more details.
|Valid values
a|dbms.security.ldap.authentication.mechanism, a string
|Default value
m|+++simple+++
|===

[[config_dbms.security.ldap.authentication.search_for_attribute]]
=== `dbms.security.ldap.authentication.search_for_attribute`

.dbms.security.ldap.authentication.search_for_attribute
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Perform authentication by searching for an unique attribute of a user.
Using this setting requires `<<config_dbms.security.ldap.authorization.system_username,dbms.security.ldap.authorization.system_username>>` and `<<config_dbms.security.ldap.authorization.system_password,dbms.security.ldap.authorization.system_password>>` to be configured.
|Valid values
a|dbms.security.ldap.authentication.search_for_attribute, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.ldap.authentication.use_samaccountname]]
=== `dbms.security.ldap.authentication.use_samaccountname`

label:deprecated[Deprecated]

.dbms.security.ldap.authentication.use_samaccountname
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Perform authentication by searching for an unique attribute of a user.
|Valid values
a|dbms.security.ldap.authentication.use_samaccountname, a boolean
|Default value
m|+++false+++
|Replaced by
a|<<config_dbms.security.ldap.authentication.search_for_attribute,dbms.security.ldap.authentication.search_for_attribute>>
|===

[[config_dbms.security.ldap.authentication.user_dn_template]]
=== `dbms.security.ldap.authentication.user_dn_template`

label:dynamic[Dynamic]

.dbms.security.ldap.authentication.user_dn_template
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|LDAP user DN template. An LDAP object is referenced by its distinguished name (DN), and a user DN is an LDAP fully-qualified unique user identifier. This setting is used to generate an LDAP DN that conforms with the LDAP directory's schema from the user principal that is submitted with the authentication token when logging in. The special token {0} is a placeholder where the user principal will be substituted into the DN string.
|Valid values
a|dbms.security.ldap.authentication.user_dn_template, a string which Must be a string containing '{0}' to understand where to insert the runtime authentication principal.
|Default value
m|+++uid={0},ou=users,dc=example,dc=com+++
|===

[[config_dbms.security.ldap.authorization.access_permitted_group]]
=== `dbms.security.ldap.authorization.access_permitted_group`

label:dynamic[Dynamic]

.dbms.security.ldap.authorization.access_permitted_group
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The LDAP group to which a user must belong to get any access to the system.Set this to restrict access to a subset of LDAP users belonging to a particular group. If this is not set, any user to successfully authenticate via LDAP will have access to the PUBLIC role and any other roles assigned to them via <<config_dbms.security.ldap.authorization.group_to_role_mapping,dbms.security.ldap.authorization.group_to_role_mapping>>.
|Valid values
a|dbms.security.ldap.authorization.access_permitted_group, a string
|Default value
m|++++++
|===

[[config_dbms.security.ldap.authorization.group_membership_attributes]]
=== `dbms.security.ldap.authorization.group_membership_attributes`

label:dynamic[Dynamic]

.dbms.security.ldap.authorization.group_membership_attributes
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of attribute names on a user object that contains groups to be used for mapping to roles when LDAP authorization is enabled.
|Valid values
a|dbms.security.ldap.authorization.group_membership_attributes, a ',' separated list with elements of type 'a string'. which Can not be empty
|Default value
m|+++memberOf+++
|===

[[config_dbms.security.ldap.authorization.group_to_role_mapping]]
=== `dbms.security.ldap.authorization.group_to_role_mapping`

label:dynamic[Dynamic]

.dbms.security.ldap.authorization.group_to_role_mapping
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|An authorization mapping from LDAP group names to Neo4j role names. The map should be formatted as a semicolon separated list of key-value pairs, where the key is the LDAP group name and the value is a comma separated list of corresponding role names. For example: group1=role1;group2=role2;group3=role3,role4,role5
You could also use whitespaces and quotes around group names to make this mapping more readable, for example:
----
dbms.security.ldap.authorization.group_to_role_mapping=\
         "cn=Neo4j Read Only,cn=users,dc=example,dc=com"      = reader;    \
         "cn=Neo4j Read-Write,cn=users,dc=example,dc=com"     = publisher; \
         "cn=Neo4j Schema Manager,cn=users,dc=example,dc=com" = architect; \
         "cn=Neo4j Administrator,cn=users,dc=example,dc=com"  = admin
----
|Valid values
a|dbms.security.ldap.authorization.group_to_role_mapping, a string which must be semicolon separated list of key-value pairs or empty
|Default value
m|++++++
|===

[[config_dbms.security.ldap.authorization.system_password]]
=== `dbms.security.ldap.authorization.system_password`

.dbms.security.ldap.authorization.system_password
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|An LDAP system account password to use for authorization searches when `<<config_dbms.security.ldap.authorization.use_system_account,dbms.security.ldap.authorization.use_system_account>>` is `true`.
|Valid values
a|dbms.security.ldap.authorization.system_password, a secure string
|===

[[config_dbms.security.ldap.authorization.system_username]]
=== `dbms.security.ldap.authorization.system_username`

.dbms.security.ldap.authorization.system_username
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|An LDAP system account username to use for authorization searches when `<<config_dbms.security.ldap.authorization.use_system_account,dbms.security.ldap.authorization.use_system_account>>` is `true`. Note that the `<<config_dbms.security.ldap.authentication.user_dn_template,dbms.security.ldap.authentication.user_dn_template>>` will not be applied to this username, so you may have to specify a full DN.
|Valid values
a|dbms.security.ldap.authorization.system_username, a string
|===

[[config_dbms.security.ldap.authorization.use_system_account]]
=== `dbms.security.ldap.authorization.use_system_account`

.dbms.security.ldap.authorization.use_system_account
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Perform LDAP search for authorization info using a system account instead of the user's own account.
If this is set to `false` (default), the search for group membership will be performed directly after authentication using the LDAP context bound with the user's own account. The mapped roles will be cached for the duration of `<<config_dbms.security.auth_cache_ttl,dbms.security.auth_cache_ttl>>`, and then expire, requiring re-authentication. To avoid frequently having to re-authenticate sessions you may want to set a relatively long auth cache expiration time together with this option. NOTE: This option will only work if the users are permitted to search for their own group membership attributes in the directory.
If this is set to `true`, the search will be performed using a special system account user with read access to all the users in the directory. You need to specify the username and password using the settings `<<config_dbms.security.ldap.authorization.system_username,dbms.security.ldap.authorization.system_username>>` and `<<config_dbms.security.ldap.authorization.system_password,dbms.security.ldap.authorization.system_password>>` with this option. Note that this account only needs read access to the relevant parts of the LDAP directory and does not need to have access rights to Neo4j, or any other systems.
|Valid values
a|dbms.security.ldap.authorization.use_system_account, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.ldap.authorization.user_search_base]]
=== `dbms.security.ldap.authorization.user_search_base`

label:dynamic[Dynamic]

.dbms.security.ldap.authorization.user_search_base
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The name of the base object or named context to search for user objects when LDAP authorization is enabled. A common case is that this matches the last part of `<<config_dbms.security.ldap.authentication.user_dn_template,dbms.security.ldap.authentication.user_dn_template>>`.
|Valid values
a|dbms.security.ldap.authorization.user_search_base, a string which Can not be empty
|Default value
m|+++ou=users,dc=example,dc=com+++
|===

[[config_dbms.security.ldap.authorization.user_search_filter]]
=== `dbms.security.ldap.authorization.user_search_filter`

label:dynamic[Dynamic]

.dbms.security.ldap.authorization.user_search_filter
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The LDAP search filter to search for a user principal when LDAP authorization is enabled. The filter should contain the placeholder token {0} which will be substituted for the user principal.
|Valid values
a|dbms.security.ldap.authorization.user_search_filter, a string
|Default value
m|+++(&(objectClass=*)(uid={0}))+++
|===

[[config_dbms.security.ldap.connection_timeout]]
=== `dbms.security.ldap.connection_timeout`

.dbms.security.ldap.connection_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The timeout for establishing an LDAP connection. If a connection with the LDAP server cannot be established within the given time the attempt is aborted. A value of 0 means to use the network protocol's (i.e., TCP's) timeout value.
|Valid values
a|dbms.security.ldap.connection_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++30s+++
|===

[[config_dbms.security.ldap.host]]
=== `dbms.security.ldap.host`

.dbms.security.ldap.host
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|URL of LDAP server to use for authentication and authorization. The format of the setting is `<protocol>://<hostname>:<port>`, where hostname is the only required field. The supported values for protocol are `ldap` (default) and `ldaps`. The default port for `ldap` is 389 and for `ldaps` 636. For example: `ldaps://ldap.example.com:10389`.
You may want to consider using STARTTLS (`<<config_dbms.security.ldap.use_starttls,dbms.security.ldap.use_starttls>>`) instead of LDAPS for secure connections, in which case the correct protocol is `ldap`.
|Valid values
a|dbms.security.ldap.host, a string
|Default value
m|+++localhost+++
|===

[[config_dbms.security.ldap.read_timeout]]
=== `dbms.security.ldap.read_timeout`

.dbms.security.ldap.read_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The timeout for an LDAP read request (i.e. search). If the LDAP server does not respond within the given time the request will be aborted. A value of 0 means wait for a response indefinitely.
|Valid values
a|dbms.security.ldap.read_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++30s+++
|===

[[config_dbms.security.ldap.referral]]
=== `dbms.security.ldap.referral`

.dbms.security.ldap.referral
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The LDAP referral behavior when creating a connection. This is one of `follow`, `ignore` or `throw`.
* `follow` automatically follows any referrals
* `ignore` ignores any referrals
* `throw` throws an exception, which will lead to authentication failure.
|Valid values
a|dbms.security.ldap.referral, a string
|Default value
m|+++follow+++
|===

[[config_dbms.security.ldap.use_starttls]]
=== `dbms.security.ldap.use_starttls`

.dbms.security.ldap.use_starttls
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Use secure communication with the LDAP server using opportunistic TLS. First an initial insecure connection will be made with the LDAP server, and a STARTTLS command will be issued to negotiate an upgrade of the connection to TLS before initiating authentication.
|Valid values
a|dbms.security.ldap.use_starttls, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.log_successful_authentication]]
=== `dbms.security.log_successful_authentication`

.dbms.security.log_successful_authentication
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Set to log successful authentication events to the security log. If this is set to `false` only failed authentication events will be logged, which could be useful if you find that the successful events spam the logs too much, and you do not require full auditing capability.
|Valid values
a|dbms.security.log_successful_authentication, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.security.oidc.-provider-.audience]]
=== `dbms.security.oidc.<provider>.audience`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.audience
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Expected values of the Audience (aud) claim in the id token.
|Valid values
a|dbms.security.oidc.<provider>.audience, a ',' separated list with elements of type 'a string'. which Can not be empty
|===

[[config_dbms.security.oidc.-provider-.auth_endpoint]]
=== `dbms.security.oidc.<provider>.auth_endpoint`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.auth_endpoint
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The OIDC authorization endpoint. If this is not supplied Neo4j will attempt to discover it from the well_known_discovery_uri.
|Valid values
a|dbms.security.oidc.<provider>.auth_endpoint, a URI
|===

[[config_dbms.security.oidc.-provider-.auth_flow]]
=== `dbms.security.oidc.<provider>.auth_flow`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.auth_flow
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The OIDC flow to use. This is exposed to clients via the discovery endpoint.
|Valid values
a|dbms.security.oidc.<provider>.auth_flow, one of [PKCE, IMPLICIT]
|Default value
m|+++PKCE+++
|===

[[config_dbms.security.oidc.-provider-.auth_params]]
=== `dbms.security.oidc.<provider>.auth_params`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.auth_params
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Optional additional configuration options used for the authentication request. The map is a semicolon separated list of key-value pairs. For example: `k1=v1;k2=v2`.
|Valid values
a|dbms.security.oidc.<provider>.auth_params, A simple key value map pattern  k1=v1;k2=v2
|Default value
m|+++{}+++
|===

[[config_dbms.security.oidc.-provider-.authorization.group_to_role_mapping]]
=== `dbms.security.oidc.<provider>.authorization.group_to_role_mapping`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.authorization.group_to_role_mapping
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|An authorization mapping from IdP group names to Neo4j role names. The map should be formatted as a semicolon separated list of key-value pairs, where the key is the IdP group name and the value is a comma separated list of corresponding role names. For example: group1=role1;group2=role2;group3=role3,role4,role5
You could also use whitespaces and quotes around group names to make this mapping more readable, for example:
----
dbms.security.oidc.<provider>.authorization.group_to_role_mapping=\
         "Neo4j Read Only"      = reader;    \
         "Neo4j Read-Write"     = publisher; \
         "Neo4j Schema Manager" = architect; \
         "Neo4j Administrator"  = admin
----
|Valid values
a|dbms.security.oidc.<provider>.authorization.group_to_role_mapping, a string which must be semicolon separated list of key-value pairs or empty
|===

[[config_dbms.security.oidc.-provider-.claims.groups]]
=== `dbms.security.oidc.<provider>.claims.groups`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.claims.groups
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The claim to use as the list of groups in Neo4j. These could be Neo4J roles directly, or can be mapped using dbms.security.oidc.<provider>.authorization.group_to_role_mapping.
|Valid values
a|dbms.security.oidc.<provider>.claims.groups, a string
|===

[[config_dbms.security.oidc.-provider-.claims.username]]
=== `dbms.security.oidc.<provider>.claims.username`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.claims.username
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The claim to use as the username in Neo4j. This would typically be sub, but in some situations it may be be desirable to use something else such as email.
|Valid values
a|dbms.security.oidc.<provider>.claims.username, a string
|Default value
m|+++sub+++
|===

[[config_dbms.security.oidc.-provider-.client_id]]
=== `dbms.security.oidc.<provider>.client_id`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.client_id
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Client id needed if token contains multiple Audience (aud) claims.
|Valid values
a|dbms.security.oidc.<provider>.client_id, a string
|===

[[config_dbms.security.oidc.-provider-.config]]
=== `dbms.security.oidc.<provider>.config`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.config
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Additional configuration options that the clients may require to authenticate. The map is a semicolon separated list of key-value pairs. For example: `k1=v1;k2=v2`.
|Valid values
a|dbms.security.oidc.<provider>.config, A simple key value map pattern  k1=v1;k2=v2
|Default value
m|+++{}+++
|===

[[config_dbms.security.logs.oidc.jwt_claims_at_debug_level_enabled]]
=== `dbms.security.logs.oidc.jwt_claims_at_debug_level_enabled`

.dbms.security.logs.oidc.jwt_claims_at_debug_level_enabled
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|When set to `true`, it logs the claims from the JWT. This will only take effect when the security log level is set to `DEBUG`. +
WARNING: It is strongly advised that this is set to `false` when running in a production environment in order to prevent logging of sensitive information. Also note that the contents of the JWT claims set can change over time because they are dependent entirely upon the ID provider.
|Valid values
a|dbms.security.logs.oidc.jwt_claims_at_debug_level_enabled, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.oidc.-provider-.display_name]]
=== `dbms.security.oidc.<provider>.display_name`

.dbms.security.oidc.<provider>.display_name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The user-facing name of the provider as provided by the discovery endpoint to clients (Bloom, Browser etc.).
|Valid values
a|dbms.security.oidc.<provider>.display_name, a string
|===

[[config_dbms.security.oidc.-provider-.get_groups_from_user_info]]
=== `dbms.security.oidc.<provider>.get_groups_from_user_info`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.get_groups_from_user_info
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|When turned on, Neo4j gets the groups from the provider user info endpoint.
|Valid values
a|dbms.security.oidc.<provider>.get_groups_from_user_info, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.oidc.-provider-.get_username_from_user_info]]
=== `dbms.security.oidc.<provider>.get_username_from_user_info`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.get_username_from_user_info
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|When turned on, Neo4j gets the username from the provider user info endpoint.
|Valid values
a|dbms.security.oidc.<provider>.get_username_from_user_info, a boolean
|Default value
m|+++false+++
|===

[[config_dbms.security.oidc.-provider-.issuer]]
=== `dbms.security.oidc.<provider>.issuer`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.issuer
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The expected value of the iss claim in the id token.
|Valid values
a|dbms.security.oidc.<provider>.issuer, a string
|===

[[config_dbms.security.oidc.-provider-.jwks_uri]]
=== `dbms.security.oidc.<provider>.jwks_uri`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.jwks_uri
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The location of the JWK public key set for the identity provider. If this is not supplied Neo4j will attempt to discover it from the well_known_discovery_uri.
|Valid values
a|dbms.security.oidc.<provider>.jwks_uri, a URI
|===

[[config_dbms.security.oidc.-provider-.params]]
=== `dbms.security.oidc.<provider>.params`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.params
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Generic parameters that the clients may require to authenticate. The map is a semicolon separated list of key-value pairs. For example: `k1=v1;k2=v2`.
|Valid values
a|dbms.security.oidc.<provider>.params, A simple key value map pattern  k1=v1;k2=v2
|Default value
m|+++{}+++
|===

[[config_dbms.security.oidc.-provider-.redirect_uri]]
=== `dbms.security.oidc.<provider>.redirect_uri`

label:dynamic[Dynamic] label:deprecated[Deprecated]

.dbms.security.oidc.<provider>.redirect_uri
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The redirect URI the IdP should return the user to when authenticated.
|Valid values
a|dbms.security.oidc.<provider>.redirect_uri, a URI
|===

[[config_dbms.security.oidc.-provider-.token_endpoint]]
=== `dbms.security.oidc.<provider>.token_endpoint`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.token_endpoint
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The OIDC token endpoint. If this is not supplied Neo4j will attempt to discover it from the well_known_discovery_uri.
|Valid values
a|dbms.security.oidc.<provider>.token_endpoint, a URI
|===

[[config_dbms.security.oidc.-provider-.token_params]]
=== `dbms.security.oidc.<provider>.token_params`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.token_params
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Optional additional configuration options used for the token request. The map is a semicolon separated list of key-value pairs. For example: `k1=v1;k2=v2`.
|Valid values
a|dbms.security.oidc.<provider>.token_params, A simple key value map pattern  k1=v1;k2=v2
|Default value
m|+++{}+++
|===

[[config_dbms.security.oidc.-provider-.user_info_uri]]
=== `dbms.security.oidc.<provider>.user_info_uri`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.user_info_uri
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The identity providers user info uri.
|Valid values
a|dbms.security.oidc.<provider>.user_info_uri, a URI
|===

[[config_dbms.security.oidc.-provider-.well_known_discovery_uri]]
=== `dbms.security.oidc.<provider>.well_known_discovery_uri`

label:dynamic[Dynamic]

.dbms.security.oidc.<provider>.well_known_discovery_uri
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The 'well known' OpenID Connect Discovery endpoint used to fetch identity provider settings.
|Valid values
a|dbms.security.oidc.<provider>.well_known_discovery_uri, a URI
|===

[[config_dbms.security.procedures.allowlist]]
=== `dbms.security.procedures.allowlist`

.dbms.security.procedures.allowlist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of procedures (comma separated) that are to be loaded. The list may contain both fully-qualified procedure names, and partial names with the wildcard '*'. If this setting is left empty no procedures will be loaded.
|Valid values
a|dbms.security.procedures.allowlist, a ',' separated list with elements of type 'a string'.
|Default value
m|+++*+++
|===

[[config_dbms.security.procedures.default_allowed]]
=== `dbms.security.procedures.default_allowed`

label:deprecated[Deprecated]

.dbms.security.procedures.default_allowed
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The default role that can execute all procedures and user-defined functions that are not covered by the `<<config_dbms.security.procedures.roles,dbms.security.procedures.roles>>` setting. This setting (if not empty string) will be translated to 'GRANT EXECUTE BOOSTED PROCEDURE *' and 'GRANT EXECUTE BOOSTED FUNCTION *' for that role. If `<<config_dbms.security.procedures.roles,dbms.security.procedures.roles>>`is not empty, any procedure or function that this role is not mapped to will result in a 'DENY EXECUTE BOOSTED PROCEDURE name' and 'DENY EXECUTE BOOSTED FUNCTION name' for this role. Any privilege mapped in this way cannot be revoked, instead the config must be changed and will take effect after a restart.
|Valid values
a|dbms.security.procedures.default_allowed, a string
|Default value
m|++++++
|Replaced by
a|`EXECUTE PROCEDURE`, `EXECUTE BOOSTED PROCEDURE`, `EXECUTE FUNCTION` and `EXECUTE BOOSTED FUNCTION` privileges
|===

[[config_dbms.security.procedures.roles]]
=== `dbms.security.procedures.roles`

label:deprecated[Deprecated]

.dbms.security.procedures.roles
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|This provides a finer level of control over which roles can execute procedures than the `<<config_dbms.security.procedures.default_allowed,dbms.security.procedures.default_allowed>>` setting. For example: `+dbms.security.procedures.roles=apoc.convert.*:reader;apoc.load.json*:writer;apoc.trigger.add:TriggerHappy+` will allow the role `reader` to execute all procedures in the `apoc.convert` namespace, the role `writer` to execute all procedures in the `apoc.load` namespace that starts with `json` and the role `TriggerHappy` to execute the specific procedure `apoc.trigger.add`. Procedures not matching any of these patterns will be subject to the `<<config_dbms.security.procedures.default_allowed,dbms.security.procedures.default_allowed>>` setting. This setting (if not empty string) will be translated to 'GRANT EXECUTE BOOSTED PROCEDURE name' and 'GRANT EXECUTE BOOSTED FUNCTION name' privileges for the mapped roles. Any privilege mapped in this way cannot be revoked, instead the config must be changed and will take effect after a restart.
|Valid values
a|dbms.security.procedures.roles, a string
|Default value
m|++++++
|Replaced by
a|`EXECUTE PROCEDURE`, `EXECUTE BOOSTED PROCEDURE`, `EXECUTE FUNCTION` and `EXECUTE BOOSTED FUNCTION` privileges
|===

[[config_dbms.security.procedures.unrestricted]]
=== `dbms.security.procedures.unrestricted`

.dbms.security.procedures.unrestricted
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of procedures and user defined functions (comma separated) that are allowed full access to the database. The list may contain both fully-qualified procedure names, and partial names with the wildcard '*'. Note that this enables these procedures to bypass security. Use with caution.
|Valid values
a|dbms.security.procedures.unrestricted, a ',' separated list with elements of type 'a string'.
|Default value
m|++++++
|===

[[config_dbms.security.procedures.whitelist]]
=== `dbms.security.procedures.whitelist`

label:deprecated[Deprecated]

.dbms.security.procedures.whitelist
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|A list of procedures (comma separated) that are to be loaded. The list may contain both fully-qualified procedure names, and partial names with the wildcard '*'. If this setting is left empty no procedures will be loaded.
|Valid values
a|dbms.security.procedures.whitelist, a ',' separated list with elements of type 'a string'.
|Default value
m|+++*+++
|Replaced by
a|<<config_dbms.security.procedures.allowlist,dbms.security.procedures.allowlist>>
|===

[[config_dbms.netty.ssl.provider]]
=== `dbms.netty.ssl.provider`

.dbms.netty.ssl.provider
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Netty SSL provider.
|Valid values
a|dbms.netty.ssl.provider, one of [JDK, OPENSSL, OPENSSL_REFCNT]
|Default value
m|+++JDK+++
|===

[[config_systemdb.secrets.key.name]]
=== `systemdb.secrets.key.name`

label:dynamic[Dynamic]

.systemdb.secrets.key.name
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Name of the 256 length AES encryption key, which is used for the symmetric encryption.
|Valid values
a|systemdb.secrets.key.name, a string
|Default value
m|+++aesKey+++
|===

[[config_systemdb.secrets.keystore.password]]
=== `systemdb.secrets.keystore.password`

label:dynamic[Dynamic]

.systemdb.secrets.keystore.password
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Password for accessing the keystore holding a 256 length AES encryption key, which is used for the symmetric encryption.
|Valid values
a|systemdb.secrets.keystore.password, a secure string
|===

[[config_systemdb.secrets.keystore.path]]
=== `systemdb.secrets.keystore.path`

label:dynamic[Dynamic]

.systemdb.secrets.keystore.path
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Location of the keystore holding a 256 length AES encryption key, which is used for the symmetric encryption of secrets held in system database.
|Valid values
a|systemdb.secrets.keystore.path, a path
|===

== Transaction settings

[[config_dbms.lock.acquisition.timeout]]
=== `dbms.lock.acquisition.timeout`

label:dynamic[Dynamic]

.dbms.lock.acquisition.timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum time interval within which lock should be acquired. Zero (default) means timeout is disabled.
|Valid values
a|dbms.lock.acquisition.timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++0s+++
|===

[[config_dbms.shutdown_transaction_end_timeout]]
=== `dbms.shutdown_transaction_end_timeout`

.dbms.shutdown_transaction_end_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum amount of time to wait for running transactions to complete before allowing initiated database shutdown to continue.
|Valid values
a|dbms.shutdown_transaction_end_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++10s+++
|===

[[config_dbms.transaction.bookmark_ready_timeout]]
=== `dbms.transaction.bookmark_ready_timeout`

label:dynamic[Dynamic]

.dbms.transaction.bookmark_ready_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum amount of time to wait for the database state represented by the bookmark.
|Valid values
a|dbms.transaction.bookmark_ready_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`) which is minimum `1s`
|Default value
m|+++30s+++
|===

[[config_dbms.transaction.concurrent.maximum]]
=== `dbms.transaction.concurrent.maximum`

label:dynamic[Dynamic]

.dbms.transaction.concurrent.maximum
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum number of concurrently running transactions. If set to 0, limit is disabled.
|Valid values
a|dbms.transaction.concurrent.maximum, an integer
|Default value
m|+++1000+++
|===

[[config_dbms.transaction.monitor.check.interval]]
=== `dbms.transaction.monitor.check.interval`

.dbms.transaction.monitor.check.interval
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Configures the time interval between transaction monitor checks. Determines how often monitor thread will check transaction for timeout.
|Valid values
a|dbms.transaction.monitor.check.interval, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++2s+++
|===

[[config_dbms.transaction.sampling.percentage]]
=== `dbms.transaction.sampling.percentage`

label:dynamic[Dynamic]

.dbms.transaction.sampling.percentage
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Transaction sampling percentage.
|Valid values
a|dbms.transaction.sampling.percentage, an integer which is in the range `1` to `100`
|Default value
m|+++5+++
|===

[[config_dbms.transaction.timeout]]
=== `dbms.transaction.timeout`

label:dynamic[Dynamic]

.dbms.transaction.timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|The maximum time interval of a transaction within which it should be completed.
|Valid values
a|dbms.transaction.timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++0s+++
|===

[[config_dbms.transaction.tracing.level]]
=== `dbms.transaction.tracing.level`

label:dynamic[Dynamic]

.dbms.transaction.tracing.level
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Transaction creation tracing level.
|Valid values
a|dbms.transaction.tracing.level, one of [DISABLED, SAMPLE, ALL]
|Default value
m|+++DISABLED+++
|===

[[config_dbms.rest.transaction.idle_timeout]]
=== `dbms.rest.transaction.idle_timeout`

.dbms.rest.transaction.idle_timeout
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Timeout for idle transactions in the REST endpoint.
|Valid values
a|dbms.rest.transaction.idle_timeout, a duration (Valid units are: `ns`, `μs`, `ms`, `s`, `m`, `h` and `d`; default unit is `s`)
|Default value
m|+++1m+++
|===

== Transaction log settings

[[config_dbms.recovery.fail_on_missing_files]]
=== `dbms.recovery.fail_on_missing_files`

.dbms.recovery.fail_on_missing_files
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|If `true`, Neo4j will abort recovery if transaction log files are missing. Setting this to `false` will allow Neo4j to create new empty missing files for the already existing  database, but the integrity of the database might be compromised.
|Valid values
a|dbms.recovery.fail_on_missing_files, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.tx_log.buffer.size]]
=== `dbms.tx_log.buffer.size`

.dbms.tx_log.buffer.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|On serialization of transaction logs, they will be temporary stored in the byte buffer that will be flushed at the end of the transaction or at any moment when buffer will be full.
|Valid values
a|dbms.tx_log.buffer.size, a long which is minimum `131072`
|Default value
m|By default the size of byte buffer is based on number of available cpu's with minimal buffer size of 512KB. Every another 4 cpu's will add another 512KB into the buffer size. Maximal buffer size in this default scheme is 4MB taking into account that we can have one transaction log writer per database in multi-database env.For example, runtime with 4 cpus will have buffer size of 1MB; runtime with 8 cpus will have buffer size of 1MB 512KB; runtime with 12 cpus will have buffer size of 2MB.
|===

[[config_dbms.tx_log.preallocate]]
=== `dbms.tx_log.preallocate`

label:dynamic[Dynamic]

.dbms.tx_log.preallocate
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Specify if Neo4j should try to preallocate the logical log file in advance.
It optimizes the filesystem by ensuring there is room to accommodate newly generated files and avoid file-level fragmentation.
|Valid values
a|dbms.tx_log.preallocate, a boolean
|Default value
m|+++true+++
|===

[[config_dbms.tx_log.rotation.retention_policy]]
=== `dbms.tx_log.rotation.retention_policy`

label:dynamic[Dynamic]

.dbms.tx_log.rotation.retention_policy
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Tell Neo4j how long logical transaction logs should be kept to backup the database.For example, "10 days" will prune logical logs that only contain transactions older than 10 days.Alternatively, "100k txs" will keep the 100k latest transactions from each database and prune any older transactions.
|Valid values
a|dbms.tx_log.rotation.retention_policy, a string which matches the pattern `^(true{vbar}keep_all{vbar}false{vbar}keep_none{vbar}(\d+[KkMmGg]?( (files{vbar}size{vbar}txs{vbar}entries{vbar}hours{vbar}days))))$` (Must be `true` or `keep_all`, `false` or `keep_none`, or of format `<number><optional unit> <type>`. Valid units are `K`, `M` and `G`. Valid types are `files`, `size`, `txs`, `entries`, `hours` and `days`. For example, `100M size` will limit logical log space on disk to 100MB per database,and `200K txs` will limit the number of transactions kept to 200 000 per database.)
|Default value
m|+++7 days+++
|===

[[config_dbms.tx_log.rotation.size]]
=== `dbms.tx_log.rotation.size`

label:dynamic[Dynamic]

.dbms.tx_log.rotation.size
[frame="topbot", stripes=odd, grid="cols", cols="<1s,<4"]
|===
|Description
a|Specifies at which file size the logical log will auto-rotate. Minimum accepted value is 128 KiB.
|Valid values
a|dbms.tx_log.rotation.size, a byte size (valid multipliers are `B`, `KiB`, `KB`, `K`, `kB`, `kb`, `k`, `MiB`, `MB`, `M`, `mB`, `mb`, `m`, `GiB`, `GB`, `G`, `gB`, `gb`, `g`, `TiB`, `TB`, `PiB`, `PB`, `EiB`, `EB`) which is minimum `128.00KiB`
|Default value
m|+++250.00MiB+++
|===

