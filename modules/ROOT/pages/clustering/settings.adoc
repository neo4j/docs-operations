:description: This section lists the important settings related to running a Neo4j cluster.
[role=enterprise-edition]
[[clustering-settings]]
= Settings reference

[options="header",width="100%",cols="2,3a"]
|===
| Parameter
| Explanation

//was dbms.mode
| xref:configuration/configuration-settings.adoc#config_initial.server.mode_constraint[`initial.server.mode_constraint`]
| This setting constrains the operating mode of the database to be used only in primary or secondary mode.
Default setting is `NONE`, ie. no constraint.
As an initial setting, the value set here is used when a server is first enabled.
Once enabled, a server's mode constraint can only be changed with `ALTER SERVER 'name' SET OPTIONS {modeConstraint:'PRIMARY'}`.

**Example**: a server configured with `initial.server.mode_constraint=SECONDARY` is only allocated databases whose topologies contain 1 or more secondary.
This server always only hosts those databases in `SECONDARY` mode.

| xref:configuration/configuration-settings.adoc#config_server.cluster.system_database_mode[`server.cluster.system_database_mode`]
| Every cluster member hosts the `system` database.
This config controls what mode a given instance hosts the `system` database in: `PRIMARY` or `SECONDARY`.

**Example:** `server.cluster.system_database_mode=SECONDARY` means that this instance holds only a secondary copy of the `system` database.

[NOTE]
====
There should be a relatively high number (5-7) of `system` primaries, spread across availability zones.
However, if enabling more than 10 servers, it is recommended to start making the later ones secondaries.
====
| xref:configuration/configuration-settings.adoc#config_dbms.cluster.minimum_initial_system_primaries_count[`dbms.cluster.minimum_initial_system_primaries_count`]
| Minimum number of servers configured with `server.cluster.system_database_mode=PRIMARY` required to form a cluster.

**Example:** `dbms.cluster.minimum_initial_system_primaries_count=3` specifies that the cluster is considered bootstrapped and the DBMS online when at least 3 `system` database primaries have discovered one another.


| xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.resolver_type[`dbms.cluster.discovery.resolver_type`]
| This setting specifies the strategy that the instance uses to determine the addresses for other instances in the cluster to contact for _bootstrapping_.
Possible values are:

[.compact]
`LIST`::
Treats `dbms.cluster.discovery.endpoints` as a list of addresses of servers to contact for discovery.
`DNS`::
Treats `dbms.cluster.discovery.endpoints` as a domain name to resolve via DNS.
Expect DNS resolution to provide A records with hostnames or IP addresses of servers to contact for discovery, on the port specified by `dbms.cluster.discovery.endpoints`.
`SRV`::
Treats `dbms.cluster.discovery.endpoints` as a domain name to resolve via DNS.
Expect DNS resolution to provide SRV records with hostnames or IP addresses and ports, of servers to contact for discovery.
`K8S`::
Accesses the Kubernetes list service API to derive addresses of servers to contact for discovery.
Requires `dbms.kubernetes.label_selector` to be a Kubernetes label selector for Kubernetes services running a server each and `dbms.kubernetes.service_port_name` to be a service port name identifying the discovery port of cluster servers services.
The value of `dbms.cluster.discovery.endpoints` is ignored for this option.

The value of this setting determines how `dbms.cluster.discovery.endpoints` is interpreted.
Detailed information about discovery and discovery configuration options is given in xref:clustering/setup/discovery.adoc#clustering-discovery-dns[Discovery using DNS with multiple records].

**Example:** `clustering-discovery-dns=DNS` combined with `dbms.cluster.discovery.endpoints=cluster01.example.com:5000` fetch all DNS A records for _cluster01.example.com_ and attempt to reach Neo4j instances listening on port `5000` for each A record's IP address.

| xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.endpoints[`dbms.cluster.discovery.endpoints`]
| One or more network addresses used to discover other servers in the cluster.
The exact method by which endpoints are resolved to other cluster members is determined by the value of `dbms.cluster.discovery.resolver_type`.
In the default case, the initial discovery members are given as a comma-separated list of address/port pairs, and the default port for the discovery service is `:5000`.

It is good practice to set this parameter to the same value on all servers in the cluster.

The behavior of this setting can be modified by configuring the setting `dbms.cluster.discovery.resolver_type`.
This is described in detail in xref:clustering/setup/discovery.adoc#clustering-discovery-dns[Discovery using DNS with multiple records].

**Example:** `dbms.cluster.discovery.resolver_type=LIST` combined with `server01.example.com:5000,server02.example.com:5000,server03.example.com:5000` attempt to reach Neo4j instances listening on _server01.example.com_, _server02.example.com_ and _server03.example.com_; all on port `5000`.

| xref:configuration/configuration-settings.adoc#config_server.discovery.advertised_address[`server.discovery.advertised_address`]
| The address/port setting that specifies where the instance advertises that it listens for discovery protocol messages from other members of the cluster.
If this server is included in the `discovery.endpoints` of other cluster members, the value there must **exactly** match this advertised address.

**Example:** `server.discovery.advertised_address=192.168.33.21:5001` indicates that other cluster members can communicate with this server using the discovery protocol at host `192.168.33.20` and port `5001`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.raft.advertised_address[`server.cluster.raft.advertised_address`]
| The address/port setting that specifies where the Neo4j server advertises to other members of the cluster that it listens for Raft messages within the cluster.

**Example:** `server.cluster.raft.advertised_address=192.168.33.20:7000` listens for cluster communication in the network interface bound to `192.168.33.20` on port `7000`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.advertised_address[`server.cluster.advertised_address`]
| The address/port setting that specifies where the instance advertises it listens for requests for transactions in the transaction-shipping catchup protocol.

**Example:** `causal_clustering.transaction_advertised_address=192.168.33.20:6001` listens for transactions from cluster members on the network interface bound to `192.168.33.20` on port `6001`.

| xref:configuration/configuration-settings.adoc#config_server.discovery.listen_address[`server.discovery.listen_address`]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for the cluster discovery protocol.

**Example:** `server.discovery.listen_address=0.0.0.0:5001` listens for cluster membership communication on any network interface at port `5001`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.raft.listen_address[`server.cluster.raft.listen_address`]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for cluster communication.
This setting must be set in coordination with the address this instance advertises it listens at in the setting `server.cluster.raft.advertised_address`.

**Example:** `server.cluster.raft.listen_address=0.0.0.0:7000` listens for cluster communication on any network interface at port `7000`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.listen_address[`server.cluster.listen_address`]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for cluster communication.
This setting must be set in coordination with the address this instance advertises it listens at in the setting `server.cluster.advertised_address`.

**Example:** `server.cluster.listen_address=0.0.0.0:6001` listens for cluster communication on any network interface at port `6001`.

|===


// [[clustering-settings-multi-dc]]
// == Multi-data center settings
//
// [options="header",width="100%",cols="1,3"]
// |===
// | Parameter
// | Explanation
//
//
// | <<config_server.groups,`server.groups`>>
// | A list of group names for the server used when configuring load balancing and replication policies.
//
// *Example:* `server.groups=us,us-east` adds the current instance to the groups `us` and `us-east`.
//
// | <<config_db.cluster.raft.leader_transfer.priority_group,`+db.cluster.raft.leader_transfer.priority_group.<database>+`>>
// |The group of servers which should be preferred when selecting leaders for the specified database.
// If the instance currently acting as leader for this database is not a member of the configured server group, then the cluster attempts to transfer leadership to an instance that _is_ a member.
// It is not guaranteed that leadership is always held by a server in the desired group.
// For example, if no member of the desired group is available or has up-to-date store contents.
// The cluster seeks to preserve availability over respecting the `leadership_priority_group` setting.
//
// //To set a default `leadership_priority_group` for all databases that do not have an explicitly set `leadership_priority_group`, the `<database>` can be omitted.
// //See <<config_causal_clustering.leadership_priority_group,`causal_clustering.leadership_priority_group`>>.
//
// *Example:* `db.cluster.raft.leader_transfer.priority_group.foo=us` ensures that if the leader for `foo` is not held by a server configured with `server.groups=us`, the cluster attempts to transfer leadership to a server that is.
//
// | <<config_server.cluster.catchup.upstream_strategy,`server.cluster.catchup.upstream_strategy`>>
// | An ordered list in descending preference of the strategy which secondaries use to choose upstream database server to pull transactional updates from.
//
// *Example:* `server.cluster.catchup.upstream_strategy=connect-randomly-within-server-group,typically-connect-to-random-secondary` configures the behavior so that the secondary first tries to connect to any other instance in the group(s) specified in `server.groups`.
// If it fails to find any live instances in those groups, then it connects to a random secondary.
// A value of `user-defined` enables custom strategy definitions using the setting `server.cluster.catchup.user_defined_upstream_strategy`.
//
// | <<config_server.cluster.catchup.user_defined_upstream_strategy,`server.cluster.catchup.user_defined_upstream_strategy`>>
// | Defines the configuration of upstream dependencies.
// Can only be used if `server.cluster.catchup.upstream_strategy` is set to `user-defined`.
//
// *Example:* `server.cluster.catchup.user_defined_upstream_strategy=groups(north2); groups(north); halt()` looks for servers in the `north2`.
// If none are available it looks in the `north` server group.
// Finally, if it cannot resolve any servers in any of the previous groups, then rule chain is stopped via `halt()`.
//
// | <<config_dbms.routing.load_balancing.plugin,`dbms.routing.load_balancing.plugin`>>
// | The load balancing plugin to use.
// One pre-defined plugin named `server_policies` is available by default.
//
// *Example:* `dbms.routing.load_balancing.plugin=server_policies` enables custom policy definitions.
//
// | `+causal_clustering.load_balancing.config.server_policies.<policy-name>+`
// | Defines a custom policy under the name `<policy-name>`.
// Note that load balancing policies are cluster-global configurations and should be defined the exact same way on all core machines.
//
// *Example:* `causal_clustering.load_balancing.config.server_policies.north1_only=groups(north1)->min(2); halt();` defines a load balancing policy named `north1_only`. +
// Queries are sent only to servers in the `north1` server group, provided there are two of them available.
// If there are less than two servers in `north1`, the chain is halted.
//
// By default, the load balancer sends read requests only to replicas/followers, which means these two servers must be of that kind.
// To allow reads on the leader, set to <<config_causal_clustering.cluster_allow_reads_on_leader, `causal_clustering.cluster_allow_reads_on_leader`>> to `true`.
// |===
