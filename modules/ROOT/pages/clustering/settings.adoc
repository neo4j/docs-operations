:description: This section lists the important settings related to running a Neo4j cluster.
[role=enterprise-edition]
[[clustering-settings]]
= Settings reference

[options="header",width="100%",cols="2,3a"]
|===
| Parameter
| Explanation

//was dbms.mode
| xref:configuration/configuration-settings.adoc#config_initial.server.mode_constraint[`initial.server.mode_constraint`]
| This setting constrains the operating mode of the database to be used only in primary or secondary mode.
Default setting is `NONE`, ie. no constraint.
As an initial setting, the value set here is used when a server is first enabled.
Once enabled, a server's mode constraint can only be changed with `ALTER SERVER 'name' SET OPTIONS {modeConstraint:'PRIMARY'}`.

**Example**: a server configured with `initial.server.mode_constraint=SECONDARY` is only allocated databases whose topologies contain 1 or more secondary.
This server always only hosts those databases in `SECONDARY` mode.

| xref:configuration/configuration-settings.adoc#config_initial.dbms.automatically_enable_free_servers[`initial.dbms.automatically_enable_free_servers`]
| This setting allows for auto-enable of servers in the `FREE` state.
After startup, it can be changed with the xref:procedures.adoc#procedure_dbms_cluster_setAutomaticallyEnableFreeServers[`dbms.cluster.setAutomaticallyEnableFreeServers`] procedure.

| xref:configuration/configuration-settings.adoc#config_server.cluster.system_database_mode[`server.cluster.system_database_mode`]
| Every cluster member hosts the `system` database.
This config controls what mode a given instance hosts the `system` database in: `PRIMARY` or `SECONDARY`.

**Example:** `server.cluster.system_database_mode=SECONDARY` means that this instance holds only a secondary copy of the `system` database.

[NOTE]
====
There should be a relatively high number (5-7) of `system` primaries, spread across availability zones.
However, if enabling more than 10 servers, it is recommended to start making the later ones secondaries.
====
| xref:configuration/configuration-settings.adoc#config_dbms.cluster.minimum_initial_system_primaries_count[`dbms.cluster.minimum_initial_system_primaries_count`]
| Minimum number of servers configured with `server.cluster.system_database_mode=PRIMARY` required to form a cluster.

**Example:** `dbms.cluster.minimum_initial_system_primaries_count=3` specifies that the cluster is considered bootstrapped and the DBMS online when at least 3 `system` database primaries have discovered one another.


| xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.resolver_type[`dbms.cluster.discovery.resolver_type`]
| This setting specifies the strategy that the instance uses to determine the addresses for other instances in the cluster to contact for _bootstrapping_.
Possible values are:

[.compact]
`LIST`::
Treats `dbms.cluster.discovery.endpoints` (or `dbms.cluster.discovery.v2.endpoints`, if you use discovery service v2 available as of Neo4j 5.23) as a list of addresses of servers to contact for discovery.
`DNS`::
Treats `dbms.cluster.discovery.endpoints` (or `dbms.cluster.discovery.v2.endpoints`, if you use discovery service v2 available as of Neo4j 5.23) as a domain name to resolve via DNS.
Expect DNS resolution to provide A records with hostnames or IP addresses of servers to contact for discovery, on the port specified by `dbms.cluster.discovery.endpoints`.
`SRV`::
Treats `dbms.cluster.discovery.endpoints` (or `dbms.cluster.discovery.v2.endpoints`, if you use discovery service v2 available as of Neo4j 5.23) as a domain name to resolve via DNS.
Expect DNS resolution to provide SRV records with hostnames or IP addresses and ports, of servers to contact for discovery.
`K8S`::
Accesses the Kubernetes list service API to derive addresses of servers to contact for discovery.
Requires `dbms.kubernetes.label_selector` to be a Kubernetes label selector for Kubernetes services running a server each and `dbms.kubernetes.service_port_name` to be a service port name identifying the discovery port of cluster servers services.
The value of `dbms.cluster.discovery.endpoints` (or `dbms.cluster.discovery.v2.endpoints`, if you use discovery service v2 available as of Neo4j 5.23) is ignored for this option.
For more details, see xref:clustering/setup/discovery.adoc#clustering-discovery-k8s[Discovery in Kubernetes].

From Neo4j 5.23, depending on which version of the discovery service you are using, you need to set either `dbms.cluster.discovery.endpoints` or `dbms.cluster.discovery.v2.endpoints` in the _neo4j.conf_ file.
Detailed information about discovery and discovery configuration options is given in xref:clustering/setup/discovery.adoc#clustering-discovery-methods[Methods for server discovery].

| xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.endpoints[`dbms.cluster.discovery.endpoints`] label:deprecated[Deprecated in 5.23]
| One or more network addresses used to discover other servers in the cluster.
The exact method by which endpoints are resolved to other cluster members is determined by the value of `dbms.cluster.discovery.resolver_type`.
In the default case, the initial discovery members are given as a comma-separated list of address/port pairs, and the default port for the discovery service is `:5000`.

It is good practice to set this parameter to the same value on all servers in the cluster.

**Example:** `dbms.cluster.discovery.resolver_type=LIST` combined with `server01.example.com:5000,server02.example.com:5000,server03.example.com:5000` attempt to reach Neo4j instances listening on _server01.example.com_, _server02.example.com_ and _server03.example.com_; all on port `5000`.

|xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.v2.endpoints[`dbms.cluster.discovery.v2.endpoints`] label:new[Introduced in 5.22]
|A comma-separated list of endpoints that a server should contact in order to discover other cluster members.
Typically, all cluster members, including the current server, must be specified in this list.
The setting configures the endpoints for discovery service v2.

**Example:** `dbms.cluster.discovery.resolver_type=LIST` combined with `server01.example.com:6000,server02.example.com:6000,server03.example.com:6000` attempt to reach Neo4j instances listening on _server01.example.com_, _server02.example.com_ and _server03.example.com_; all on port `6000`.

|xref:configuration/configuration-settings.adoc#config_dbms.cluster.discovery.version[`dbms.cluster.discovery.version`] label:new[Introduced in 5.22]
|This setting allows you to select which discovery service should be started.
Possible values are:

* V1_ONLY -— it runs only discovery service v1.

* V1_OVER_V2 -— it runs both discovery service v1 and discovery service v2, where v1 is the main service and v2 runs in the background.

* V2_OVER_V1 -— it runs both discovery service v1 and discovery service v2, where v2 is the main service and v1 runs in the background.

* V2_ONLY -- it runs only discovery service v2.

The default value is `V1_ONLY`.

Discovery services v1 and v2 are designed to run in parallel.
They are completely independent of each other, thus allowing you to keep the cluster functioning while switching over from v1 to v2.
For details on how to move from discovery service v1 to v2, see xref:clustering/setup/discovery.adoc#clustering-discovery-v1-to-v2[Moving from discovery service v1 to v2].

| xref:configuration/configuration-settings.adoc#config_server.discovery.advertised_address[`server.discovery.advertised_address`] label:deprecated[Deprecated in 5.23]
| The address/port setting that specifies where the instance advertises that it listens for discovery protocol messages from other members of the cluster.
If this server is included in the `discovery.endpoints` of other cluster members, the value there must **exactly** match this advertised address.

**Example:** `server.discovery.advertised_address=192.168.33.21:5001` indicates that other cluster members can communicate with this server using the discovery protocol at host `192.168.33.20` and port `5001`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.raft.advertised_address[`server.cluster.raft.advertised_address`]
| The address/port setting that specifies where the Neo4j server advertises to other members of the cluster that it listens for Raft messages within the cluster.

**Example:** `server.cluster.raft.advertised_address=192.168.33.20:7000` listens for cluster communication in the network interface bound to `192.168.33.20` on port `7000`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.advertised_address[`server.cluster.advertised_address`]
| The address/port setting that specifies where the instance advertises it listens for requests for transactions in the transaction-shipping catch-up protocol.

**Example:** `causal_clustering.transaction_advertised_address=192.168.33.20:6001` listens for transactions from cluster members on the network interface bound to `192.168.33.20` on port `6001`.

| xref:configuration/configuration-settings.adoc#config_server.discovery.listen_address[`server.discovery.listen_address`] label:deprecated[Deprecated in 5.23]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for the cluster discovery protocol.

**Example:** `server.discovery.listen_address=0.0.0.0:5001` listens for cluster membership communication on any network interface at port `5001`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.raft.listen_address[`server.cluster.raft.listen_address`]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for cluster communication.
This setting must be set in coordination with the address this instance advertises it listens at in the setting `server.cluster.raft.advertised_address`.

**Example:** `server.cluster.raft.listen_address=0.0.0.0:7000` listens for cluster communication on any network interface at port `7000`.

| xref:configuration/configuration-settings.adoc#config_server.cluster.listen_address[`server.cluster.listen_address`]
| The address/port setting that specifies which network interface and port the Neo4j instance binds to for cluster communication.
This setting must be set in coordination with the address this instance advertises it listens at in the setting `server.cluster.advertised_address`.

**Example:** `server.cluster.listen_address=0.0.0.0:6001` listens for cluster communication on any network interface at port `6001`.

|===


// [[clustering-settings-multi-dc]]
// == Multi-data center settings
//
// [options="header",width="100%",cols="1,3"]
// |===
// | Parameter
// | Explanation
//
//
// | <<config_server.groups,`server.groups`>>
// | A list of group names for the server used when configuring load balancing and replication policies.
//
// *Example:* `server.groups=us,us-east` adds the current instance to the groups `us` and `us-east`.
//
// | <<config_db.cluster.raft.leader_transfer.priority_group,`+db.cluster.raft.leader_transfer.priority_group.<database>+`>>
// |The group of servers which should be preferred when selecting leaders for the specified database.
// If the instance currently acting as leader for this database is not a member of the configured server group, then the cluster attempts to transfer leadership to an instance that _is_ a member.
// It is not guaranteed that leadership is always held by a server in the desired group.
// For example, if no member of the desired group is available or has up-to-date store contents.
// The cluster seeks to preserve availability over respecting the `leadership_priority_group` setting.
//
// //To set a default `leadership_priority_group` for all databases that do not have an explicitly set `leadership_priority_group`, the `<database>` can be omitted.
// //See <<config_causal_clustering.leadership_priority_group,`causal_clustering.leadership_priority_group`>>.
//
// *Example:* `db.cluster.raft.leader_transfer.priority_group.foo=us` ensures that if the leader for `foo` is not held by a server configured with `server.groups=us`, the cluster attempts to transfer leadership to a server that is.
//
// | <<config_server.cluster.catchup.upstream_strategy,`server.cluster.catchup.upstream_strategy`>>
// | An ordered list in descending preference of the strategy which secondaries use to choose upstream database server to pull transactional updates from.
//
// *Example:* `server.cluster.catchup.upstream_strategy=connect-randomly-within-server-group,typically-connect-to-random-secondary` configures the behavior so that the secondary first tries to connect to any other instance in the group(s) specified in `server.groups`.
// If it fails to find any live instances in those groups, then it connects to a random secondary.
// A value of `user-defined` enables custom strategy definitions using the setting `server.cluster.catchup.user_defined_upstream_strategy`.
//
// | <<config_server.cluster.catchup.user_defined_upstream_strategy,`server.cluster.catchup.user_defined_upstream_strategy`>>
// | Defines the configuration of upstream dependencies.
// Can only be used if `server.cluster.catchup.upstream_strategy` is set to `user-defined`.
//
// *Example:* `server.cluster.catchup.user_defined_upstream_strategy=groups(north2); groups(north); halt()` looks for servers in the `north2`.
// If none are available it looks in the `north` server group.
// Finally, if it cannot resolve any servers in any of the previous groups, then rule chain is stopped via `halt()`.
//
// | <<config_dbms.routing.load_balancing.plugin,`dbms.routing.load_balancing.plugin`>>
// | The load balancing plugin to use.
// One pre-defined plugin named `server_policies` is available by default.
//
// *Example:* `dbms.routing.load_balancing.plugin=server_policies` enables custom policy definitions.
//
// | `+causal_clustering.load_balancing.config.server_policies.<policy-name>+`
// | Defines a custom policy under the name `<policy-name>`.
// Note that load balancing policies are cluster-global configurations and should be defined the exact same way on all core machines.
//
// *Example:* `causal_clustering.load_balancing.config.server_policies.north1_only=groups(north1)->min(2); halt();` defines a load balancing policy named `north1_only`. +
// Queries are sent only to servers in the `north1` server group, provided there are two of them available.
// If there are less than two servers in `north1`, the chain is halted.
//
// By default, the load balancer sends read requests only to replicas/followers, which means these two servers must be of that kind.
// To allow reads on the leader, set to <<config_causal_clustering.cluster_allow_reads_on_leader, `causal_clustering.cluster_allow_reads_on_leader`>> to `true`.
// |===
