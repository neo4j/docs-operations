[role=enterprise-edition]
[[causal-clustering-internals]]
= Internals of clustering
:description: This section details a few selected internals of a Neo4j Causal Cluster. Understanding the internals is not vital but can be helpful in diagnosing and resolving operational issues. 


[[causal-clustering-elections-and-leadership]]
== Elections and leadership

The Core Servers in a Causal Cluster use the Raft protocol to ensure consistency and safety.
An implementation detail of Raft is that it uses a _Leader_ role to impose an ordering on an underlying log with other instances acting as _Followers_ which replicate the leader's state.
In Neo4j terms this means writes to the database are ordered by the Core instance currently playing the leader role.

If a follower has not heard from the leader for a while, then it can initiate an election and attempt to become the new leader.
The follower makes itself a _Candidate_ and asks other Cores to vote for it.
If it can get a majority of the votes, then it assumes the leader role.
Cores will not vote for a candidate which is less up-to-date than itself.
There can only be one leader at any time, and that leader is guaranteed to have the most up-to-date log.

It is expected for elections to occur during the normal running of a cluster and they do not pose an issue in and of itself.
If you are experiencing frequent re-elections and they are disturbing the operation of the cluster then you should try to figure out what is causing them.
Some common causes are environmental issues (e.g. a flaky networking) and work overload conditions (e.g. more concurrent queries and transactions than the hardware can handle).


[[causal-clustering-multi-database-and-the-reconciler]]
== Multi-database and the reconciler

Databases operate as independent entities in a Neo4j DBMS, both in standalone and in a cluster.
Since a cluster consists of multiple independent server instances, the effects of administrative operations like creating a new database happen asynchronously and independently for each server.
However, the immediate effect of an administrative operation is to safely commit the desired state in the system database.

The desired state committed in the system database gets replicated and is picked up by an internal component called the reconciler.
It runs on every instance and takes the appropriate actions required locally on that instance for reaching the desired state; creating, starting, stopping and dropping databases.

Every database runs in an independent Raft group and since there are two databases in a fresh cluster, `system` and `neo4j`, this means that it also has two Raft groups.
Every Raft group also has an independent leader and thus a particular Core server could be the leader for one database and a follower for another.


[[causal-clustering-store-copy]]
== Store copy

Store copies are initiated when an instance does not have an up-to-date copy of the database.
For example, this will be the case when a new instance is joining a cluster (without a seed).
It can also happen as a consequence of falling behind the rest of the cluster, for reasons such as connectivity issues or having been shutdown.
Upon re-establishing connection with the cluster, an instance will recognize that it is too far behind and fetch a new copy from the rest of the cluster.

A store copy is a major operation which may disrupt the availability of instances in the cluster.
Store copies should not be a frequent occurrence in a well-functioning cluster, but rather be an exceptional operation that happens due to specific causes, e.g. network outages or planned maintenance outages.
If store copies happen during regular operation, then the configuration of the cluster, or the workload directed at it, might have to be reviewed so that all instances can keep up, and that there is enough of a buffer of Raft logs and transaction logs to handle smaller transient issues.

The protocol used for store copies is robust and configurable.
The network requests will be directed at an upstream member according to configuration and they will be retried despite transient failures.
The maximum amount of time to retry every request can be configured with xref:reference/configuration-settings.adoc#config_causal_clustering.store_copy_max_retry_time_per_request[`causal_clustering.store_copy_max_retry_time_per_request`].
If a request fails and the maximum retry time has elapsed then it will stop retrying and the store copy will fail.

Use xref:reference/configuration-settings.adoc#config_causal_clustering.catch_up_client_inactivity_timeout[`causal_clustering.catch_up_client_inactivity_timeout`] to configure the inactivity timeout for any particular request.

[NOTE]
====
The xref:reference/configuration-settings.adoc#config_causal_clustering.catch_up_client_inactivity_timeout[`causal_clustering.catch_up_client_inactivity_timeout`] configuration is for all requests from the catchup client, including the pulling of transactions.
====

The default upstream strategy differs for Cores and Read Replicas.
Cores will always send the initial request to the leader to get the most up-to-date information about the store.
The strategy for the file and index requests for Cores is to vary every other request to a random Read Replica and every other to a random Core member.

Read Replicas use the same strategy for store copies as it uses for pulling transactions.
The default is to pull from a random Core member.

If you are running a multi-data center cluster, then upstream strategies for both Cores and Read Replicas can be configured.
Remember that for Read Replicas this also affects from where transactions are pulled.
See more in xref:clustering-advanced/multi-data-center/configuration.adoc[Configure for multi-data center operations].

=== Using the Replica instance in case of failure

In case of failure (e.g. a partial failure of a cluster due to the loss of an instance, but not of the majority), you may transform a Read Replica instance into a Core instance as a way to restore the cluster's core availability.
However, keep in mind that this is not advised as it could cause data loss and complications in the Raft group.

To avoid that, the _read_replica_ instance must not be initialized as a *single* instance, nor be introduced in a different or new cluster. 
This action would cause an override of the raft state, thus preventing the replica from successfully joining the targeted cluster.

After performing that change, follow these instructions to unbind the Replica instance and update the discovery configurations amongst cluster members: 

. Ensure that the converted _read_replica_ currently belongs to the same cluster that it will be re-introduced back to, as a _core_. 
This can be done by performing `CALL dbms.cluster.overview()` and verifying the instance's address and cluster mode.

. Stop and unbind the _read_replica_ instance.

. Update the cluster mode configuration in _neo4j.conf_, from `dbms.mode=READ_REPLICA` to `dbms.mode=CORE`.

. Stop Neo4j on the removed core instances that are not intended to serve as core members. 

. Unbind those instances from the cluster by performing `neo4j-admin unbind` while they are stopped. 
This action will prevent such instances from subsequently attempting to rejoin the running cluster.

At this point, the previous _read_replica_ (now _core_) instance may be introduced into the running cluster. 
To persist this change in the cluster's architecture, the following configuration updates are advised:

- On the previous _read_replica_ (now _core_) instance, set `causal_clustering.discovery_advertised_address` and `causal_clustering.discovery_listen_address` as appropriate.

- Update the `causal_clustering.initial_discovery_members` configuration with the currently valid list of discovery addresses for each member of the cluster.
This should replace the addresses of any removed _core(s)_ with the discovery addresses of the previous _read_replica_ (now _core_) instance.
+
[NOTE]
====
In cases where `causal_clustering.discovery_type` is other than `LIST`, make sure to update the corresponding address resolution addresses records. 
For example, DNS A records for discovery types DNS and SRV, and any Kubernetes service address alternate to reflect the inclusion of the _read_replica_ discovery address.
====

[[causal-clustering-on-disk-state]]
== On-disk state

The on-disk state of cluster instances is different to that of standalone instances.
The biggest difference being the existence of additional cluster state.
Most of the files there are relatively small, but the Raft logs can become quite large depending on the configuration and workload.

It is important to understand that once a database has been extracted from a cluster and used in a standalone deployment, it must not be put back into an operational cluster.
This is because the cluster and the standalone deployment now have separate databases, with different and irreconcilable writes applied to them.

[WARNING]
====
If you try to reinsert a modified database back into the cluster, then the logs and stores will mismatch.
Operators should not try to merge standalone databases into the cluster in the optimistic hope that their data will become replicated.
That will not happen and will likely lead to unpredictable cluster behavior.
====
